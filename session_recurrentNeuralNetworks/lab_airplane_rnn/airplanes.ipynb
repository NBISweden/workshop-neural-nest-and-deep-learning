{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cf5527",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df190a28",
   "metadata": {},
   "source": [
    "Execute the following code blocks to configure the session and import relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5507149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461c95f",
   "metadata": {},
   "source": [
    "There is also a utility module [rnnutils.py](https://raw.githubusercontent.com/NBISweden/workshop-neural-nets-and-deep-learning/master/session_recurrentNeuralNetworks/lecture_RNN/rnnutils.py) that you can use if you want to save time coding. Whenever it is used, it will be commented out to leave you the choice whether you want to use the functions or write up your own solution. In any case, make sure the file is located in the current directory to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb48b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rnnutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c347ae",
   "metadata": {},
   "source": [
    "# Lab session: predicting airline passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2d6ab",
   "metadata": {},
   "source": [
    "## Aims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647fcb5",
   "metadata": {},
   "source": [
    "In this lab the idea is to try out different RNN models on the Box & Jenkins monthly airline passengers dataset. The dataset is a monthly time series of airline passengers recorded in the 50'ies and 60'ies. Your task is to build a model to make a future prediction of the number of passengers given a number of observation.\n",
    "\n",
    "You will download data and prepare it for later analyses. More specifically, you will partition the data into a training and test set. In order to create input / label pairs (X/Y), the data is split into time slices, where a slice corresponds to the input (X) and the consecutive time point the (known) output (Y).\n",
    "\n",
    "\n",
    "To help you along the way, some of the steps have been prepared in advance, but in most cases, your task is to complete missing code. Don't hesitate to change parameter settings and experiment with the model architectures. Also, make sure to examine the contents of variables by printing them. Things to try:\n",
    "\n",
    "- change the number of time steps\n",
    "- change the number of epoch\n",
    "- experiment with the network topology (e.g. number of units in the hidden layer)\n",
    "\n",
    "See if you can improve on the model presented in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b56c2",
   "metadata": {},
   "source": [
    "# Session 1: Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b49cd",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27fd57",
   "metadata": {},
   "source": [
    "Start by downloading the data and loading it into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f40b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79a708",
   "metadata": {},
   "source": [
    "We modify the data somewhat for easier processing. `df.head()` simply shows you the first entries of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('airline-passengers.csv')\n",
    "df = df.rename(columns={'Month': 'time','Passengers': 'passengers'})\n",
    "df['time'] = pd.to_datetime(df['time'], format='%Y-%m')\n",
    "df['year'] = pd.DatetimeIndex(df['time']).year\n",
    "df['month'] = pd.DatetimeIndex(df['time']).month\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c43994",
   "metadata": {},
   "source": [
    "Plot the data for overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7cabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.time, df.passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c8c87",
   "metadata": {},
   "source": [
    "## Create training and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1077c56",
   "metadata": {},
   "source": [
    "Next, we partition the data into training and test data sets. We first define the size of the training set and the index at which to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 2/3\n",
    "split_index = int(df.shape[0] * train_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadd12f",
   "metadata": {},
   "source": [
    "The [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) function rescales (normalizes) the data to values in the range (0, 1) with unit variance, the motivation being that the network training tends to perform better in this case. Here, it is important to remember that scaling should be done first on the training data, and then that the same transformation is applied to the test data. In practice, we first rescale the training data independently. Then, the resulting scaler instance is used to rescale both the test data and the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2625e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for MinMaxScaler\n",
    "data = np.array(df['passengers'].values.astype('float32')).reshape(-1, 1)\n",
    "# Instantiate scaler object\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Scale training data\n",
    "train = data[range(split_index)]\n",
    "train_scaled = scaler.fit_transform(train).flatten()\n",
    "# Apply *same* transformation to test data\n",
    "test = data[split_index:]\n",
    "test_scaled = scaler.transform(test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e98a7e",
   "metadata": {},
   "source": [
    "## Transform data to input - output pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a19b90",
   "metadata": {},
   "source": [
    "Now that we have train and test data sets we need to convert the data to input - output (X/Y) pairs. The general idea is to take time slices (e.g. 12 data points) as input vectors and use the subsequent value as the known output. Since the time unit is months and there likely is a recurrent yearly seasonality in the data it makes sense to use 12 time steps, but this is a parameter you could modify to see what effect it has on the end results. In particular, if you increase this number, you would look further back in the past when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d638f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 12\n",
    "trainX, trainY, trainX_indices, trainY_indices = rnnutils.make_xy(train_scaled, time_steps)\n",
    "testX, testY, testX_indices, testY_indices = rnnutils.make_xy(test_scaled, time_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa52fde",
   "metadata": {},
   "source": [
    "## Define the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6668e5",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "Complete the model below to include a [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/) layer and a [Dense](https://keras.io/api/layers/core_layers/dense/) output layer. If you look at the SimpleRNN documentation, you will find that inputs is a 3D tensor (`[batch, timesteps, feature]`). Since we are using univariate data (i.e. one feature per time step), `features=1`. Recall also that for the `input_shape` parameter you don't specify `batch`. You can click the '+' button to the top left to reveal a working example, but first try to complete the model without peeking! Also, consult the lecture notes for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722a10e",
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=4, input_shape=(time_steps, 1), activation=\"tanh\"))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# Add layers here\n",
    "# model.add()\n",
    "#\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d680442",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "Once you are happy with the configuration, fit the model and evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b540d80",
   "metadata": {
    "scrolled": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2, validation_split=0.1)\n",
    "Ytrainpred = model.predict(trainX)\n",
    "Ytestpred = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f786e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# history = model.fit(trainX, trainY, ...)\n",
    "# Ytrainpred = model.predict(trainX)\n",
    "# Ytestpred = model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c714919",
   "metadata": {},
   "source": [
    "You can use the utility plotting functions in `rnnutils` to plot training history and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32368401",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnutils.plot_history(history, figsize=(14, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9ec3d",
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "data = {'train': (Ytrainpred, train_scaled, trainY_indices),\n",
    "       'test': (Ytestpred, test_scaled, testY_indices)}\n",
    "rnnutils.plot_pred(data, scaler=scaler, figsize=(14, 8), plotmarkers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc66ead",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Session 2: LSTM (and optionally GRU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25ce07",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "Building on session 1, analyse the data set using LSTM layers. Here is a tentative model setup to get you started. Here you could try using multiple layers, in which case you need to return the sequences for all but the last layer (cf [Stacked Long Short-Term Memory Networks](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)). If you have time, you can also try out the GRU layers for comparison. Do you notice any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194ac58",
   "metadata": {
    "scrolled": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=4, return_sequences=False, input_shape=(time_steps, 1)))\n",
    "# Remember: if you stack LSTMs, remember to set return_sequences=True for all but the last layer\n",
    "# model.add(LSTM(units=2, return_sequences=False))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78590338",
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2, validation_split=0.1)\n",
    "Ytrainpred = model.predict(trainX)\n",
    "Ytestpred = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd958ead",
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "rnnutils.plot_history(history, figsize=(14, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8b050",
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "data = {'train': (Ytrainpred, train_scaled, trainY_indices),\n",
    "       'test': (Ytestpred, test_scaled, testY_indices)}\n",
    "rnnutils.plot_pred(data, scaler=scaler, figsize=(14, 8), plotmarkers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885e745",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "Recall that the default setting for time steps is 12. If you increase this number, you would look further back in time, which in principle should play to the strength of LSTMs. Note that this need not necessarily lead to better predictions though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(..., return_sequences=True, input_shape=(..., ...)))\n",
    "# model.add(LSTM(..., return_sequences=False))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "482049/KKZZ9PC6": {
     "URL": "https://www.molecularecologist.com/2016/05/opening-pandoras-box-psmc-and-population-structure/",
     "abstract": "Essentially, all models are wrong, but some are useful. — George Box Publication of the Li and Durbin’s 2011 paper titled “Inference of human population history from individual whole-genome sequenc…",
     "accessed": {
      "date-parts": [
       [
        2019,
        5,
        29
       ]
      ]
     },
     "author": [
      {
       "family": "Pečnerová",
       "given": "Patrícia Chrzanová"
      }
     ],
     "issued": {
      "date-parts": [
       [
        2016,
        5,
        18
       ]
      ]
     },
     "language": "en-US",
     "shortTitle": "Opening Pandora’s box",
     "title": "Opening Pandora’s box: PSMC and population structure",
     "type": "post-weblog"
    },
    "482049/NF2CQDJL": {
     "ISBN": "978-0-8162-1104-3",
     "abstract": "Table of Contents Preface 1 Introduction 1 2 Autocorrelation Function and Spectrum of Stationary Processes 21 3 Linear Stationary Models 46 4 Linear Nonstationary Models 89 5 Forecasting 131 6 Model Identification 183 7 Model Estimation 224 8 Model Diagnostic Checking 308 9 Seasonal Models 327 10 Transfer Function Models 373 11 Identification, Fitting, and Checking of Transfer Function Models 407 12 Intervention Analysis Models and Outlier Detection 462 13 Aspects of Process Control 483 Collection of Tables and Charts 533 Collection of Time Series Used for Examples in the Text and in Exercises 540 References 556 Exercises and Problems 569 Index 589.",
     "author": [
      {
       "family": "Box",
       "given": "George E. P."
      },
      {
       "family": "Jenkins",
       "given": "Gwilym M."
      }
     ],
     "issued": {
      "date-parts": [
       [
        1976
       ]
      ]
     },
     "language": "en",
     "note": "Google-Books-ID: 1WVHAAAAMAAJ",
     "number-of-pages": "616",
     "publisher": "Holden-Day",
     "shortTitle": "Time Series Analysis",
     "title": "Time Series Analysis: Forecasting and Control",
     "type": "book"
    },
    "undefined": {
     "ISBN": "978-0-8162-1104-3",
     "abstract": "Table of Contents Preface 1 Introduction 1 2 Autocorrelation Function and Spectrum of Stationary Processes 21 3 Linear Stationary Models 46 4 Linear Nonstationary Models 89 5 Forecasting 131 6 Model Identification 183 7 Model Estimation 224 8 Model Diagnostic Checking 308 9 Seasonal Models 327 10 Transfer Function Models 373 11 Identification, Fitting, and Checking of Transfer Function Models 407 12 Intervention Analysis Models and Outlier Detection 462 13 Aspects of Process Control 483 Collection of Tables and Charts 533 Collection of Time Series Used for Examples in the Text and in Exercises 540 References 556 Exercises and Problems 569 Index 589.",
     "author": [
      {
       "family": "Box",
       "given": "George E. P."
      },
      {
       "family": "Jenkins",
       "given": "Gwilym M."
      }
     ],
     "issued": {
      "date-parts": [
       [
        1976
       ]
      ]
     },
     "language": "en",
     "note": "Google-Books-ID: 1WVHAAAAMAAJ",
     "number-of-pages": "616",
     "publisher": "Holden-Day",
     "shortTitle": "Time Series Analysis",
     "title": "Time Series Analysis: Forecasting and Control",
     "type": "book"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python (nn_dl_python)",
   "language": "python",
   "name": "nn_dl_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
