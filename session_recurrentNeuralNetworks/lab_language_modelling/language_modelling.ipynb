{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad537c",
   "metadata": {},
   "source": [
    "# Preparations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68eb89",
   "metadata": {},
   "source": [
    "Execute the following code blocks to configure the session and import relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6938f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d82f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "#from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "# Alternatively:\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b5aa4",
   "metadata": {},
   "source": [
    "## Running on colab\n",
    "You can use this [link](https://colab.research.google.com/github/NBISweden/workshop-neural-nets-and-deep-learning/blob/master/session_recurrentNeuralNetworks/lab_language_modelling/language_modelling.ipynb) to run the notebook on Google Colab. If you do so, it's advised that you first make a copy to your own Google Drive before starting you work on the notebbok. Otherwise changes you make to the notebook will not be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae410d8",
   "metadata": {},
   "source": [
    "## Download data\n",
    "You can download and extract the data to your local machine by executing the cell below. This is useful if you haven't extracted the archive in the repository, or are running the notebook outside of the repository (e.g. on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "93515a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you don't have the data and you are running the notebook on colab. It'll download it from github and exctract it to the current directory.\n",
    "data_directory = Path('data')\n",
    "data_url = \"https://github.com/NBISweden/workshop-neural-nets-and-deep-learning/raw/master/session_recurrentNeuralNetworks/lab_language_modelling/data/jane_austen.zip\"\n",
    "\n",
    "if not (data_directory / 'jane_austen').exists():\n",
    "    if not data_directory.exists():\n",
    "        data_directory.mkdir(parents=True)\n",
    "    if not (data_directory / 'jane_austen.zip').exists():\n",
    "        from urllib.request import urlretrieve\n",
    "        urlretrieve(data_url, 'data/jane_austen.zip')\n",
    "    if (data_directory / 'jane_austen.zip').exists():\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(data_directory / 'jane_austen.zip') as zf:\n",
    "            zf.extractall(data_directory)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee2654",
   "metadata": {},
   "source": [
    "# Lab session: Language Modelling\n",
    "\n",
    "A long standing application of RNNs is to modell natural language. In the field of computational linguistics, a \"Language Model\" is a model which assign probaility to strings of a language. In this lab we will train an RNN to perform this task.\n",
    "\n",
    "We desire a model which can give us the probability of observing a string in a language, e.g. $P(\\text{the}, \\text{cat}, \\text{sat}, \\text{on}, \\text{the}, \\text{mat})$. Directly modelling this joint probability distribution is problematic, in particular because we will pretty much only see single examples of most strings in our data. The way we model this instead is by using the \"chain rule of probability\". A joint probability distribution can be broken down into products of conditional distributions according to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x_1, \\dots, x_n) = P(x_1) \\prod_{i=2}^{n} P(x_i | x_{1}, \\dots, x_{i-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So with the example of $P(\\text{the}, \\text{cat}, \\text{sat}, \\text{on}, \\text{the}, \\text{mat})$ we can actually model this as\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\text{the}, \\text{cat}, \\text{sat}, \\text{on}, \\text{the}, \\text{mat}) = P(\\text{the})P(\\text{cat} | \\text{the})P(\\text{sat} | \\text{the}, \\text{cat})P(\\text{on} | \\text{the}, \\text{cat}, \\text{sat})P(\\text{the}| \\text{the}, \\text{cat}, \\text{sat}, \\text{on})P(\\text{mat}| \\text{the}, \\text{cat}, \\text{sat}, \\text{on}, \\text{the})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "<img src=\"images/language_modelling.gif\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "As you can see, this is a kind of auto regressive structure, and we'll model it using a recurrent neural network. Below illustrates how we model this for the fourth word in the example sentence:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\text{on} | \\text{the}, \\text{cat}, \\text{sat}) = f(\\text{the}, \\text{cat}, \\text{sat}; \\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $f(\\bullet ; \\theta)$ is our learnable model, our RNN. We then train this model as we would train a classifier; for the example input we try to make it assign all probability to the word \"$\\text{on}$\". We can then use this to then the probability of whole sequences by taking the product of the probabilities. In practice, when we learn the maximum log likelihood, the loss decomposes into a sum of the conditional log probabilites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19145c1",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "In this lab we'll work with discrete input data. We'll start with the most simple way of just _tokenizing_ the string at each byte. Tokenizing refers to the process of taking a raw string of characters and convert that into a sequence of tokens. In language modelling, the tokens are typically larger units, e.g. words or word-pieces but we'll start out simply. As an example we'll use the collected works of Jane Austen (under `data/jane_austen`). The text we're going to use are in markdown (`.md` files) so we search for all such files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c58171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_directory = Path('data/jane_austen')\n",
    "data_files = sorted(data_directory.glob('*.txt'))\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304865d8",
   "metadata": {},
   "source": [
    "The data is small, so we can actually ingest the whole dataset. We will create a list of strings which can later be used to train our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d13102",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = []\n",
    "for data_file in data_files:\n",
    "    with open(data_file) as fp:\n",
    "        loaded_data.append(fp.read())\n",
    "loaded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a216b6",
   "metadata": {},
   "source": [
    "### The tokenizer\n",
    "\n",
    "In this example we'll use a very simple tokenizer, but in general you can think of this as important preprocessing step when creating a language model. \n",
    "\n",
    "It's important to realize that the model will see each token as it's own discrete, atomic symbol. It will not be able to _see_ beforehand that that tokens might have similarities. For example, if we tokenize by splitting the text on whitespace and punctuation we might get the tokens \"book\" and \"bookseller\". From the models point of view these will be as similar as the words \"book\" and \"profiting\". In the field of computational linguistics, tokenization was long an important consideration to make the model learn to exploit similarites of words.\n",
    "\n",
    "In our example, we tokenize by simply breaking the text down into the most basic unit, the characters of the text. This makes the problem a lot harder for the model, it will need to spend quite a lot of time trying to learn what sequences of characters actually describe words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1610ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tokenizer(text):\n",
    "    \"\"\"returns a sequence of tokens based on the given text. Just tokenizes the text based on the character in the text\"\"\"\n",
    "    return list(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ac67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [list(character_tokenizer(text)) for text in loaded_data]\n",
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44f6ed",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "In a neural network, everything needs to be represented as a vector of real numbers. If the data we want to operate on is _categorical_ we need to first change the representation into dense vectors. We will do this by creating a look-up table, where each discrete token in our data (e.g. the characters of the text when using character tokenization) is represented by its _own_ vector of real values. We will create these vectors using random values and the vector associated with a particular discrete token is called its _embedding_.\n",
    "\n",
    "To efficiently make this lookup, we will create an intermediary representation, where we assign each token an integer value in the range of $[1, n]$, where $n$ is the number of tokens we have. We will reserve the integer code $0$ for invalid tokens, and if it's part of the input it will be replaced by the zero vector.\n",
    "\n",
    "The embedding vectors will then be collected in a matrix, with one vector per row. The embedding process can then be efficiently implemented by just indexing into this matrix with the token integer code.\n",
    "To create these encodings, we will create a map from all tokens we expect in our input to a set of integers using a python dictionary. We will also create a special _unknown_ `<UNK>` token, which is used whenever we see an input token we didn't expect. This is often used in practice if we need to limit the number of tokens (for example, we might only  choose to use the most common 50000 words in our data, even though there might be millions. Any word less common than these 50000 will be replaced by the `<UNK>` token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "vocabulary_counter = Counter()\n",
    "for tokenized_text in tokenized_data:\n",
    "    vocabulary_counter.update(tokenized_text)\n",
    "vocabulary_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now take the vocabular_counter and create a dictionary from the the token to an integer code\n",
    "# We reserve the first two code positions for the empty token (used for padding sequences) and \n",
    "# the unknown token\n",
    "token_encoding_map = dict()\n",
    "token_encoding_map['<EMPTY>'] = 0\n",
    "token_encoding_map['<UNK>'] = 1\n",
    "i = 2\n",
    "for token, count in vocabulary_counter.most_common():\n",
    "    token_encoding_map[token] = i\n",
    "    i += 1\n",
    "token_encoding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also create the inverse dictionary so that we can go the other way around\n",
    "inverse_token_encoding_map = {i: token for token, i in token_encoding_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokenized_text(tokenized_text):\n",
    "    unk_code = token_encoding_map['<UNK>']\n",
    "    # By using .get() on the dictionary instead of subscript (token_encoding_map[c]) \n",
    "    # we can supply a default value to use if that token isn't in the encoding map. \n",
    "    # This allows us to handle out of vocabulary tokens by simply replacing them with the <UNK> token (its encoding actually)\n",
    "    encoded_text = [token_encoding_map.get(c, unk_code) for c in tokenized_text]\n",
    "    return encoded_text\n",
    "\n",
    "def decode_encoded_text(encoded_text):\n",
    "    decoded_text = [inverse_token_encoding_map.get(x, '<UNK>') for x in encoded_text]\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d93bb2",
   "metadata": {},
   "source": [
    "We now have what we need to be able to create or datasets. We will encode them all into integer sequences, and then go the other way around to make sure the process worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by printing the first 100 characters from the first text\n",
    "print(\"Tokenized data:\", tokenized_data[0][:100])\n",
    "\n",
    "encoded_data = []\n",
    "for tokenized_text in tokenized_data:\n",
    "    encoded_text = encode_tokenized_text(tokenized_text)\n",
    "    encoded_data.append(encoded_text)\n",
    "\n",
    "print(\"Encoded data:  \", encoded_data[0][:100])\n",
    "test_decode = decode_encoded_text(encoded_data[0])\n",
    "print(\"Decoded data:  \", test_decode[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345d541",
   "metadata": {},
   "source": [
    "### What about one-hot vectors?\n",
    "\n",
    "As a general rule of thumb, you should never represent categorical variables as one-hot vectors when using them as inputs to neural networks. It's wasteful on computation and makes optimization with momentum optimizers messier. Instead, use an `Embedding` layer and encode the input as integers. If you like, you can think of this as a sparse representation of one-hot vectors, where the integer is essentially the index of the \"hot\" bit in the vector. One case where you might want to use one-hot vectors is for the targets and the `CategoricalCrossEntropy` loss, but here it's also better to stick with integer encoded categorical variables and use `SparseCategoricalCrossEntropy` unless your targets have their probability mass spread over more than one class value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2d307",
   "metadata": {},
   "source": [
    "## A data sampler - `keras.utils.Sequence`\n",
    "\n",
    "For convenience we will create a data sampler. Its job will be to supply the training with mini-batches of text sequences from all our Jane Austen books. In practice it's a good idea to create such a wrapper for our dataset since it allows us to control the batches delivered to the training loop while still allowing the data loading to be done in sequence. \n",
    "\n",
    "In this case we will take the simplest possible approach to just randomly sample subsequences from all of our texts.\n",
    "\n",
    "We will incorporate the above preprocessing steps into this class, so that it encapsulate all the necessary book keeping info we need to encode and decode data,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, pad_sequences\n",
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "import numpy \n",
    "\n",
    "class RandomTextDataset(Sequence):\n",
    "    def __init__(self, text_files, context_length, batch_size, tokenizer=character_tokenizer, unk_string = '<UNK>', empty_string='<EMPTY>', rng=None, max_vocab=None) -> None:\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = numpy.random.default_rng()\n",
    "        self.rng = rng\n",
    "                \n",
    "        self.context_length = context_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.unk_string = unk_string\n",
    "        self.empty_string = empty_string\n",
    "        self.max_vocab = max_vocab\n",
    "        \n",
    "        self.tokenzier = tokenizer\n",
    "        \n",
    "        loaded_data = []\n",
    "        for data_file in data_files:\n",
    "            with open(data_file) as fp:\n",
    "                loaded_data.append(fp.read())\n",
    "        \n",
    "        tokenized_data = [list(tokenizer(text)) for text in loaded_data]\n",
    "        \n",
    "        vocabulary_counter = Counter()\n",
    "        for tokenized_text in tokenized_data:\n",
    "            vocabulary_counter.update(tokenized_text)\n",
    "            \n",
    "        # We now take the vocabular_counter and create a dictionary from the the token to an integer code\n",
    "        # We reserve the first two code positions for the empty token (used for padding sequences) and \n",
    "        # the unknown token\n",
    "        self.token_encoding_map = dict()\n",
    "        self.token_encoding_map[self.empty_string] = 0\n",
    "        self.token_encoding_map[self.unk_string] = 1\n",
    "        i = 2\n",
    "        for token, count in vocabulary_counter.most_common(self.max_vocab):\n",
    "            self.token_encoding_map[token] = i\n",
    "            i += 1\n",
    "\n",
    "        # We also create the inverse dictionary so that we can go the other way around\n",
    "        self.inverse_token_encoding_map = {i: token for token, i in self.token_encoding_map.items()}\n",
    "        self.encoded_texts = [self.encode_tokenized_text(text) for text in tokenized_data]\n",
    "        \n",
    "        # The +1 is because we create a input and target sequence by taking one which is context_length+1, \n",
    "        # and then shift the target one step to the left while dropping the last token for the input\n",
    "        self.n = sum(len(encoded_text)//((self.context_length+1)*self.batch_size) for encoded_text in self.encoded_texts)  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # We don't actually sample the text round-robin, instead we just take any text. \n",
    "        input_sequences, target_sequences = zip(*[self.sample_sequence() for i in range(self.batch_size)])\n",
    "        pad_size_input = max(len(s) for s in input_sequences)\n",
    "        input_sequences_padded = pad_sequences(input_sequences, pad_size_input, padding=\"post\", value=0)\n",
    "        target_sequences_padded = pad_sequences(target_sequences, pad_size_input, padding=\"post\", value=0)\n",
    "        return input_sequences_padded, target_sequences_padded\n",
    "        \n",
    "        \n",
    "    def sample_sequence(self):\n",
    "        text_to_sample = self.rng.integers(len(self.encoded_texts))\n",
    "        sampled_text = self.encoded_texts[text_to_sample]\n",
    "        start_index = self.rng.integers(len(sampled_text)- self.context_length - 1)\n",
    "        sampled_sequence = sampled_text[start_index: start_index+self.context_length+1]\n",
    "        input_sequence = sampled_sequence[:-1]\n",
    "        target_sequence = sampled_sequence[1:]\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        tokenized_text = self.tokenize_text(text)\n",
    "        encoded_text = self.encode_tokenized_text(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        return self.tokenzier(text)\n",
    "    \n",
    "    def encode_tokenized_text(self, tokenized_text):\n",
    "        unk_code = self.token_encoding_map[self.unk_string]\n",
    "        # By using .get() on the dictionary instead of subscript (token_encoding_map[c]) \n",
    "        # we can supply a default value to use if that token isn't in the encoding map. \n",
    "        # This allows us to handle out of vocabulary tokens by simply replacing them with the <UNK> token (its encoding actually)\n",
    "        encoded_text = [self.token_encoding_map.get(c, unk_code) for c in tokenized_text]\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_encoded_text(self, encoded_text):\n",
    "        decoded_text = [self.inverse_token_encoding_map.get(x, self.unk_string) for x in encoded_text]\n",
    "        return decoded_text\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_encoding_map)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define some data parameters\n",
    "CONTEXT_LENGTH = 256  # The length of the sequences we will train on\n",
    "BATCH_SIZE = 64  # How many examples we'll process per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomTextDataset(data_files, CONTEXT_LENGTH, BATCH_SIZE, tokenizer=character_tokenizer, max_vocab=None)  # If you set max_vocab to an integer n, only the most frequent n tokens will be used. The remainder will be replaced by <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123bf6e",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We're now ready to start training a model, but there are some things we need to decide on first. We will train our RNNs (LSTMs actually) using _truncated_ Back-Progation Through Time. The truncation comes from the fact that we can't train the model on whole books; we simply don't have enough memory to do so. Instead we train the model by showing it _truncated_ parts of the whole books. Ideally, we wan't to show it as long sequences as we can fit into the memory of our GPUs, so this variable will depend on that. We also wan't to make use of the parallel nature of the GPU, so we want to train on multiple sequences in parallel in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "\n",
    "\n",
    "epochsVal = 100\n",
    "learnRateVal = 0.01\n",
    "batchSizeVal = 10\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "\n",
    "embedding_dimension = 32\n",
    "rnn_dimension = 128\n",
    "output_projection_dimension = 128\n",
    "\n",
    "num_embeddings = dataset.get_vocab_size()\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_embeddings, embedding_dimension, mask_zero=True))\n",
    "# Add LSTM layers; X.shape[1] refers to the number of columns in X which is the number of time steps, or window size\n",
    "model.add(LSTM(units=rnn_dimension, return_sequences=True, activation=\"tanh\", unit_forget_bias=True, recurrent_dropout=0, dropout=0.2, use_bias=True))\n",
    "# Add dense layer with activation for categorical output\n",
    "model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "model.add(Dense(num_embeddings))\n",
    "# Compile model using loss function for categorical data\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # We use the sparse loss since we've integer encoded the targets. We also set from_logits=True, since we're not applying the softmax explicityly\n",
    "model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d829c56",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We're now ready to train the model. Do this by running the `fit()` mehtod of the model object. You will se a steady drop in loss and accuracy. For this problem we're not looking at the performance of a development (validation set), so it's hard to track if the model overfits. Likely it will not have capacity to do so, and for the purpose of this lab we can allow the model to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be166316",
   "metadata": {},
   "source": [
    "## Generating text - ChatLSTM\n",
    "\n",
    "Now that we've trained our model we can try using to to generate some text. First, you can choose a text to prompt it with by setting the `prompt` variable below. You can change how long the text you wish to generate should be by setting `generation_length` to some desired value. There's a third parameter `temperature` which allows you to control how random the sampling of the next character should be. It should be a float value strictly larger than $0$. If it's set to $1$, the learnt probabilty distribution of the model will be used to sample the next character, as the temperature gets closer to 0, it gets closer to the most probable prediction, and as the temperature goes above $1$, the distribution approches the uniform distribution over the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"It is a truth universally acknowledged, that \"\n",
    "generation_length = 100\n",
    "temperature = 0.3\n",
    "random_seed = 1729  # Set to None to use a different random seed each time this cell is executed\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "def softmax(logits, temperature):\n",
    "    temperature_scaled_logits = logits/temperature\n",
    "    exponentiated_logits = np.exp(temperature_scaled_logits)\n",
    "    return exponentiated_logits / np.sum(exponentiated_logits)\n",
    "\n",
    "\n",
    "encoded_prompt = np.array(dataset.encode_text(prompt), dtype=np.int32)\n",
    "for i in range(generation_length):\n",
    "    next_token_logits = model.predict(encoded_prompt[None, ...], verbose=0)[0, -1]\n",
    "    p =  softmax(next_token_logits, temperature)\n",
    "    sampled_token = rng.choice(len(p), p=p)\n",
    "    encoded_prompt = np.concatenate([encoded_prompt, [sampled_token]])\n",
    "    \n",
    "generated_text = \"\".join(dataset.decode_encoded_text(encoded_prompt))\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399f54e",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now that we've tested the language model, let's try to modify it and see what effect it might have.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "We've trained the model by just taking the text in as is. Often it's a good idea to preprocess the text to make the learning easier. \n",
    " \n",
    " - Create a new tokenizer that before return the character, converts it to lower case. \n",
    " - Do this by implementing a new function `lower_case_character_tokenizer()`. \n",
    " - Look at the [`str class`](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) in the python documentation for suitable methods to convert  to lower case.\n",
    " - When you create the dataset, give this function as the tokenizer input instead. \n",
    " - Now train a model with this new dataset. Do you notice any performance difference after you've trained for the same amount of epochs?\n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Training the neural network to predict the next character requires a lot of capacity. \n",
    "- Try adding two more `LSTM` layers to the model and train it for the same number of epochs as the previous model.\n",
    "- Can you see any difference in performance?\n",
    "- Try increasing the dimensionality of the `LSTM` layers. What effect does it have on the model?\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Instead of increasing the model capacity to solve the problem, we can try to change the data.\n",
    "- Implement a _new tokenizer_ that instead of splitting the text by each character splits it using white space and punctuations.\n",
    "- You can use a regular expression and the  [`re module`](https://docs.python.org/3/library/re.html) to split the string. Below is a useful snipped:\n",
    "  ```python\n",
    "  import re\n",
    "  tokenized_text = re.split(r\"[;.!:, \\-_'\\n\\t#]+\", loaded_data[0])\n",
    "  ```\n",
    "  The parts inside the brackets of the regular expression lists all the characters we want to remove. The list might not be exhaustive.\n",
    "\n",
    "- The tokenizer should remove any empty strings from the result, these would behave weirdly in the model. You can use something like:\n",
    "  ```python\n",
    "  tokenized_text = [token for token in tokenized_text if token]\n",
    "  ```\n",
    "\n",
    "- Use this nerw  white space tokenzier as the tokenizer input to the dataset class. You should probably also set the `max_vocab` parameter to something apropriate like 10000. This limits the number of words the dataset effectively uses to this number.\n",
    "\n",
    "- Now train the model using this new tokenizer. What can you say about loss? After training it for some epochs and trying it, how does the text generation change?\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Instead of using the collected works of Jane Austen, try to train a language model on your own dataset or on the Shakespeare dataset also provided in the `data` directory.\n",
    "\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Two other variables are important for training RNNs, the context length (the length of sequences we actually train the model on) and batch size.\n",
    "- Double the batch size (set in the cell where you create the dataset) and train the model for 5 epochs. Does anything happen with training time?\n",
    "- Reset batch size (halve it again) Double the sequence length and train the model for 5 epochs. Does the change have a similar effect as when you doubled the batch size? \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
