{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad537c",
   "metadata": {},
   "source": [
    "# Preparations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d82f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac127aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ed72b",
   "metadata": {},
   "source": [
    "# Promoter region classification\n",
    "\n",
    "Promoter region prediction is an important part to understand the transcription process. In this lab we'll take a look at a simple way of training a recurrent neural network to solve this task. We'll take the approach from the [DeePromoter](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2019.00286/full) model using data from the implementation [here](https://github.com/egochao/DeePromoter).\n",
    "\n",
    "The goal here is to see how we can easily use recurrent neural networks to classify sequences.\n",
    "\n",
    "\n",
    "## The dataset\n",
    "The dataset is made up known promoter regions from two different species; human and mouse. The regions are 300 base pairs long, and were extracted from $-249 \\sim +50$ base pairs (where $+1$ refers to the Transcription Start Site). For each species, two subset of promoter regions were created; those with TATA motifs and those without. This results in four different datasets of positive promoter regions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60c7f0",
   "metadata": {},
   "source": [
    "## Running on colab\n",
    "You can use this [link](https://colab.research.google.com/github/NBISweden/workshop-neural-nets-and-deep-learning/blob/master/session_recurrentNeuralNetworks/lab_promoterprediction/promoter_prediction.ipynb) to run the notebook on Google Colab. If you do so, it's advised that you first make a copy to your own Google Drive before starting you work on the notebbok. Otherwise changes you make to the notebook will not be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f66e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you don't have the data and you are running the notebook on colab. It'll download it from github and exctract it to the current directory.\n",
    "from pathlib import Path\n",
    "data_directory = Path('data')\n",
    "archive_file = data_directory / 'deepromoter_data.zip'\n",
    "\n",
    "data_url = \"https://github.com/NBISweden/workshop-neural-nets-and-deep-learning/raw/master/session_recurrentNeuralNetworks/lab_promoterprediction/data/deepromoter_data.zip\"\n",
    "\n",
    "if not data_directory.exists():\n",
    "    data_directory.mkdir(parents=True)\n",
    "    \n",
    "if not archive_file.exists():\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(data_url, archive_file)\n",
    "    \n",
    "if archive_file.exists():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(archive_file) as zf:\n",
    "        zf.extractall(data_directory)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbe66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "data_directory = Path('data')\n",
    "species = ['human', 'mouse']\n",
    "promoter_type = ['TATA', 'nonTATA']\n",
    "\n",
    "# To train the model on different datasets, you can change this to use 'mouse' or 'human' \n",
    "# for the species and 'TATA' or 'nonTATA' for the promoter type\n",
    "selected_species = 'mouse'\n",
    "selected_promoter_type = 'TATA'\n",
    "selected_dataset_path = data_directory / selected_species / selected_promoter_type\n",
    "\n",
    "selected_sequences = []\n",
    "for sequence_file in selected_dataset_path.glob('*.txt'):\n",
    "    with open(sequence_file) as fp:\n",
    "        sequences = [line.strip() for line in fp]\n",
    "        selected_sequences.extend(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00851a",
   "metadata": {},
   "source": [
    "## The negative dataset\n",
    "\n",
    "To create negative sequences we will use the same method as in the DeePromoter paper, illustrated here:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"images/negative_generation.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The idea is that hard negative examples are introduced by essentially randomly subsituting parts of the positive sequences. We will create as many negative sequences as we have positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "ALPHABET= np.array([\"A\", \"C\", \"G\", \"T\"])\n",
    "\n",
    "def generate_negative_sequence(sequence, chunksize=15, shuffle_ratio=0.6, rng: np.random.Generator = None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # The sequences are strings, which doesnt allow for assignment. \n",
    "    # We convert it to a list first to to the assignment, then \n",
    "    # convert it back to a string in the end\n",
    "    mutable_sequence = np.array(list(sequence))\n",
    "    n_chunks = int(math.ceil(len(mutable_sequence)/chunksize))\n",
    "    n_replaced_chunks = int(math.ceil(n_chunks*shuffle_ratio))\n",
    "    selected_chunks = rng.choice(n_chunks, size=n_replaced_chunks, replace=False)\n",
    "    for i in selected_chunks:\n",
    "        start = i*chunksize\n",
    "        end = start + chunksize\n",
    "        chunk = mutable_sequence[start:end]\n",
    "        # We don't want to rely on just taking the chunksize below, since this \n",
    "        # chunk might be the last one and thus not fully chunksize long if the \n",
    "        # sequence length isnt divisible with chunksize\n",
    "        random_chunk = rng.choice(ALPHABET, size=len(chunk), replace=True)\n",
    "        mutable_sequence[start:end] = random_chunk\n",
    "    negative_sequence = ''.join(mutable_sequence.tolist())\n",
    "    return negative_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = selected_sequences[0]\n",
    "print(\"Promoter seqeunce\", test_sequence)\n",
    "negative_sequence = generate_negative_sequence(test_sequence)\n",
    "print(\"Negative seqeunce\", negative_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a291e2",
   "metadata": {},
   "source": [
    "## Creating a promoter dataset\n",
    "As in the language modelling lab, we will now create a keras `Sequence` class to handle the loading of data. We will reuse much of the code from the previous lab when it comes to encoding the data into integers suitable of ingestion into a neural network. A difference here is that we will train the model for classification, so the dataset will generate batches where the inputs are the sequences, and the target is a binary variable indicating whether it's a promoter region or a negative example.\n",
    "\n",
    "Also since the vocabulary is known and very constrained beforehand. We will not be using an additional `<UNK>` token. If there are any unknown nucleotides not denoted with an 'N' it's an error. We will still use an plug in tokenizer, there might be cases where the user might want to use some larger motifs as the basic token unit.\n",
    "\n",
    "We will create the negative sequences inside the dataset, so that this is transparently done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, pad_sequences\n",
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "import numpy \n",
    "\n",
    "\n",
    "def character_tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "class PromoterSequenceDataset(Sequence):\n",
    "    def __init__(self, positive_sequences, batch_size, tokenizer=character_tokenizer, rng=None) -> None:\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = numpy.random.default_rng()\n",
    "        self.rng = rng\n",
    "                \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.positive_sequences = positive_sequences\n",
    "        self.negative_sequences = [generate_negative_sequence(promoter_sequence, rng=self.rng) for promoter_sequence in self.positive_sequences]\n",
    "        self.sequences = self.positive_sequences + self.negative_sequences\n",
    "        self.tokenized_data = [tokenizer(text) for text in self.sequences]\n",
    "        \n",
    "        self.token_encoding_map = {'<empty>': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
    "        self.inverse_token_encoding_map = {i: token for token, i in self.token_encoding_map.items()}\n",
    "        self.encoded_sequences = [self.encode_tokenized_text(text) for text in self.tokenized_data]\n",
    "        \n",
    "        self.n = len(self.encoded_sequences)//self.batch_size\n",
    "        self.labels = np.concatenate([np.ones(len(positive_sequences), dtype=np.int32), np.zeros(len(self.negative_sequences), dtype=np.int32)])\n",
    "        self.sequence_indices = np.arange(len(self.encoded_sequences))\n",
    "        self.shuffle_examples()\n",
    "        \n",
    "    def shuffle_examples(self):\n",
    "        self.rng.shuffle(self.sequence_indices)\n",
    "        self.batches = np.reshape(self.sequence_indices[:self.n*self.batch_size], (self.n, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_sequences_indices = self.batches[item]\n",
    "        batch_sequences = [self.encoded_sequences[i] for i in encoded_sequences_indices]\n",
    "        batch_labels = self.labels[encoded_sequences_indices]\n",
    "        \n",
    "        pad_size = max(len(s) for s in batch_sequences)\n",
    "        padded_sequences = pad_sequences(batch_sequences, pad_size, padding=\"post\", value=0)\n",
    "        # The loss function expects the labels to have the same shape as the neural \n",
    "        # network prediction, so we create a column vector out of it.\n",
    "        expanded_batch_labels = batch_labels[..., np.newaxis]  \n",
    "        return padded_sequences, expanded_batch_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # We want to rearrange the order of samples so that we don't get the same batches all the time\n",
    "        self.shuffle_examples()\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        tokenized_text = self.tokenize_text(sequence)\n",
    "        encoded_text = self.encode_tokenized_text(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def tokenize_text(self, sequence):\n",
    "        return self.tokenzier(sequence)\n",
    "    \n",
    "    def encode_tokenized_text(self, tokenized_sequence):\n",
    "        encoded_text = [self.token_encoding_map[c] for c in tokenized_sequence]\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_encoded_text(self, encoded_text):\n",
    "        decoded_text = [self.inverse_token_encoding_map[x] for x in encoded_text]\n",
    "        return decoded_text\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_encoding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "rng = np.random.default_rng(1729) # Change this to None if you want to get different random sequences each time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "visible_sequences, test_sequences = train_test_split(selected_sequences, test_size=0.1, random_state=rng.integers(2**31))\n",
    "train_sequences, dev_sequences = train_test_split(visible_sequences, test_size=0.1, random_state=rng.integers(2**31))\n",
    "\n",
    "training_dataset = PromoterSequenceDataset(train_sequences, BATCH_SIZE, rng=rng)\n",
    "dev_dataset = PromoterSequenceDataset(dev_sequences, BATCH_SIZE, rng=rng)\n",
    "test_dataset = PromoterSequenceDataset(test_sequences, BATCH_SIZE, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86dd0f",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In this lab we will use a simple model. The paper which this is based on uses a complex neural network architecture with a Convolutional Neural Network as a first encoder part, followed by a biLSTM encoding the sequences in both directions. Here we will just use a single direction LSTM to illustrate how one can do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ddc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "#from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "# Alternatively:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "embedding_dimension = 4\n",
    "rnn_dimension = 16\n",
    "\n",
    "\n",
    "num_embeddings = training_dataset.get_vocab_size()\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_embeddings, embedding_dimension, mask_zero=True))\n",
    "# Add LSTM layers; X.shape[1] refers to the number of columns in X which is the number of time steps, or window size\n",
    "model.add(LSTM(units=rnn_dimension, activation=\"tanh\", unit_forget_bias=True, recurrent_dropout=0, dropout=0.2, use_bias=True))\n",
    "\n",
    "# Add layers for promoter classification. We're disabling these for now so that the \n",
    "# coeffients of the output layer can be used to interpret the recurrent state\n",
    "#output_projection_dimension = 8\n",
    "# Add dense layer with activation for categorical output\n",
    "#model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855170b",
   "metadata": {},
   "source": [
    "# The training loop\n",
    "\n",
    "We're now ready to start training the model, before that we'll launch a tensorboard session so that we can more easily inspect the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aca66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ec87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRateVal = 0.01\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "Path('./logs/no_pretrain').mkdir(exist_ok=True, parents=True)\n",
    "Path('./models/no_pretrain').mkdir(exist_ok=True, parents=True)\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='models/no_pretrain/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/no_pretrain'),\n",
    "]\n",
    "model.fit(training_dataset, epochs=100, validation_data=dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c022c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a4f63",
   "metadata": {},
   "source": [
    "## Inspecting the RNN state\n",
    "We can have a look at the LSTM recurrent state to get an idea of how the model performs it's prediction. We first make a copy of our sequence classifier model and remove the output layers so that we only have the `Embedding` and `LSTM` layers left. The classifier we trained used only the last output of the LSTM to make its prediction, so we change the attribute `return_sequences=True` so that we get an output at each input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspection_model = keras.models.clone_model(model)\n",
    "inspection_model.set_weights(model.get_weights())\n",
    "inspection_model.pop()\n",
    "inspection_model.pop()\n",
    "# Adjust the return_sequences of the LSTM layer and set the layers to not be trainable\n",
    "for layer in inspection_model.layers:\n",
    "    if isinstance(layer, LSTM):\n",
    "        layer.return_sequences = True\n",
    "\n",
    "# When just predicting, we don't have to compile the model.\n",
    "#learnRateVal = 0.01\n",
    "#opt = Adam(learning_rate=learnRateVal)\n",
    "#loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#inspection_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "n_test_sequences = 10\n",
    "first_test_batch = test_dataset[0]\n",
    "first_batch_test_sequences, first_batch_test_labels = first_test_batch\n",
    "\n",
    "tf_prediction = inspection_model(first_batch_test_sequences[:n_test_sequences])\n",
    "np_prediction = np.array(tf_prediction).squeeze()\n",
    "test_labels = first_batch_test_labels[:n_test_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b53083",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_prediction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_color_maps = {0.: plt.get_cmap('viridis'), 1.: plt.get_cmap('plasma')}\n",
    "fig, axes = plt.subplots(len(np_prediction), figsize=(20,12))\n",
    "for ax, prediction, label in zip(axes.flatten(), np_prediction, test_labels):\n",
    "    cmap = label_color_maps[float(label)]\n",
    "    ax.imshow(prediction.T, cmap=cmap)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cef8a",
   "metadata": {},
   "source": [
    "# Inspecting the output layer\n",
    "\n",
    "A natural question when looking at these kinds of plots is \"is there a correlation between the state and the class\", and of course there is. For the simple model we've chosen with only a single dense layer going from the RNN state to the prediction we actually already have the coeficcient vector for linearly regressing the predicted class on this RNN state and we can inspect these weights to get an idea of how the models state is used.\n",
    "\n",
    "Note that we're once again looking at the trained model (not `inspection_model`) since what we actually want to inspect is the weights of the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949adb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_coefficients, linear_regression_intercept = model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89014d13",
   "metadata": {},
   "source": [
    "We'll now plot this using a bar chart. It will clearly show us whether there are any units in the RNN state output which contributes more or less strongly to the prediction of whether the sequence is a promoter or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(len(linear_regression_coefficients))\n",
    "y = linear_regression_coefficients.squeeze()\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"RNN unit\")\n",
    "plt.ylabel(\"Linear regression coefficient\")\n",
    "plt.title(\"Weights of the output layer. Values above 0 contribute to\\nthe positive prediction, below 0 to the negative.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0452f",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "Change the size of the recurrent state to be smaller (e.g. 8 units).\n",
    "- How does this affect the trainability and performance of the model?\n",
    "- Does this change how spread out the coefficients of the last layer are?\n",
    "\n",
    "### Exercise 2\n",
    "The network trained is fairly small. Try increasing the number of LSTM layers and their dimension. What is the top performance you can achieve?\n",
    "\n",
    "### Exercise 3\n",
    "Change what species and whether you use a TATA dataset and train a new model. Does the model perform worse when you switch to one of the nonTATA datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c2c56",
   "metadata": {},
   "source": [
    "## Language model pre-training\n",
    "\n",
    "A common strategy in contemporary machine learning is to try to use datasets with large amounts of data to train a model. This model is then _transferred_ to a new problem and because of the pretraining on the larger set might be able to learn faster or better before overfitting. Here we will illustrate how we might be able to Language Modelling to first train a model on predicting what nucleotides are likely to follow in sequence, and then take the model which has learned this and train it on the classification task. The goal is to reduce the time it takes before the model starts learning the promoter classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, pad_sequences\n",
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "import numpy \n",
    "\n",
    "\n",
    "def character_tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "class PromoterLMDataset(Sequence):\n",
    "    def __init__(self, positive_sequences, batch_size, tokenizer=character_tokenizer, rng=None) -> None:\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = numpy.random.default_rng()\n",
    "        self.rng = rng\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.positive_sequences = positive_sequences\n",
    "        self.sequences = self.positive_sequences\n",
    "        self.tokenized_data = [tokenizer(text) for text in self.sequences]\n",
    "        \n",
    "        self.token_encoding_map = {'<empty>': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
    "        self.inverse_token_encoding_map = {i: token for token, i in self.token_encoding_map.items()}\n",
    "        self.encoded_sequences = [self.encode_tokenized_text(text) for text in self.tokenized_data]\n",
    "        \n",
    "        self.n = len(self.encoded_sequences)//self.batch_size\n",
    "        self.sequence_indices = np.arange(len(self.encoded_sequences))\n",
    "        self.shuffle_examples()\n",
    "        \n",
    "    def shuffle_examples(self):\n",
    "        self.rng.shuffle(self.sequence_indices)\n",
    "        self.batches = np.reshape(self.sequence_indices[:self.n*self.batch_size], (self.n, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_sequences_indices = self.batches[item]\n",
    "        batch_sequences = [self.encoded_sequences[i] for i in encoded_sequences_indices]\n",
    "        input_sequences = [encoded_sequence[:-1] for encoded_sequence in batch_sequences]        \n",
    "        target_sequences = [encoded_sequence[1:] for encoded_sequence in batch_sequences]        \n",
    "        pad_size = max(len(s) for s in input_sequences)\n",
    "        padded_input_sequences = pad_sequences(input_sequences, pad_size, padding=\"post\", value=0)\n",
    "        padded_target_sequences = pad_sequences(target_sequences, pad_size, padding=\"post\", value=0)\n",
    "        \n",
    "        return padded_input_sequences, padded_target_sequences\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # We want to rearrange the order of samples so that we don't get the same batches all the time\n",
    "        self.shuffle_examples()\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        tokenized_text = self.tokenize_text(sequence)\n",
    "        encoded_text = self.encode_tokenized_text(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def tokenize_text(self, sequence):\n",
    "        return self.tokenzier(sequence)\n",
    "    \n",
    "    def encode_tokenized_text(self, tokenized_sequence):\n",
    "        encoded_text = [self.token_encoding_map[c] for c in tokenized_sequence]\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_encoded_text(self, encoded_text):\n",
    "        decoded_text = [self.inverse_token_encoding_map[x] for x in encoded_text]\n",
    "        return decoded_text\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_encoding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train_dataset = PromoterLMDataset(train_sequences, batch_size=BATCH_SIZE)\n",
    "lm_dev_dataset = PromoterLMDataset(train_sequences, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ca1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37e56e",
   "metadata": {},
   "source": [
    "## The Language Model\n",
    "In this problem we change the model slightly. Instead of using only one dense layer as an output we'll use two with a nonlinearity inbetween them. Generally when doing transfer learning, you like to \"push\" specialization of the task towards the part of the neural network which will not be transferred. Also, if you have a pretrained network, you'll often get more _transferrable_ representations if you don't pick the ones closest to the output layer of the pretraining task, but rather the ones from a couple of layers down the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f618c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "#from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "# Alternatively:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "embedding_dimension = 4\n",
    "rnn_dimension = 16\n",
    "output_projection_dimension = 8  # We set this a bit higher, meaning that for the language modelling task we actually give the specialized prediction head quite a lot of capacity. The intuition here is that the specialization of predicting the next token is promoted into being solved at this layer\n",
    "\n",
    "num_embeddings = lm_train_dataset.get_vocab_size()\n",
    "lm_model = Sequential()\n",
    "lm_model.add(Embedding(num_embeddings, embedding_dimension, mask_zero=True))\n",
    "# Add LSTM layers; X.shape[1] refers to the number of columns in X which is the number of time steps, or window size\n",
    "lm_model.add(LSTM(units=rnn_dimension, return_sequences=True, activation=\"tanh\", unit_forget_bias=True, recurrent_dropout=0, dropout=0.2, use_bias=True))\n",
    "# Add dense layer with activation for categorical output\n",
    "lm_model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "lm_model.add(Dense(num_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRateVal = 0.01\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "lm_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "Path('./logs/lm_pretrain').mkdir(exist_ok=True, parents=True)\n",
    "Path('./models/lm_pretrain').mkdir(exist_ok=True, parents=True)\n",
    "callbacks = [\n",
    "    #keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='models/lm_pretrain/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/lm_pretrain'),\n",
    "]\n",
    "\n",
    "lm_model.fit(lm_train_dataset, epochs=50, validation_data=lm_dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea84cc",
   "metadata": {},
   "source": [
    "## Transferring the network\n",
    "We will transfer what the network has learnt by:\n",
    "1. removing the two dense layers which took the recurrent state to predict the next token\n",
    "2. disable the return of all states (the classifier only uses the final state)\n",
    "3. Set the trainability of the transferred layers to False: `layer.trainable = False`.\n",
    "4. add a new classification layer at the end to solve the binary classification task. \n",
    "\n",
    "We start by making a copy of the language modelling network so we can continue to train it if we would like.\n",
    "\n",
    "### Freezing the transferred layers\n",
    "The reason to make the transferred layers non-trainable, or _frozen_, is that we add a newly initialized random layer on top of them. \n",
    "Since the error propagated through the network goes through this layer we risk destroying what was previously learned if we train the whole model with a random output layer especially when we have so few parameters as we do here. A change in just a single parameter can have  devastating effects on what the neural network has learned.\n",
    "\n",
    "To not make this happen we start by only training the output layer for a couple of epochs. Then when that seems to have converged we enable training of the whole network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = keras.models.clone_model(lm_model) \n",
    "\n",
    "# Cloning the model doesn't copy the weights, we need to set them explicitly\n",
    "#learnRateVal = 0.01\n",
    "#opt = Adam(learning_rate=learnRateVal)\n",
    "#loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#finetuned_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "finetuned_model.set_weights(lm_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop the last two dense layers\n",
    "finetuned_model.pop()\n",
    "finetuned_model.pop()\n",
    "\n",
    "# Adjust the return_sequences of the LSTM layer and set the layers to not be trainable\n",
    "for layer in finetuned_model.layers:\n",
    "    layer.trainable = False\n",
    "    if isinstance(layer, LSTM):\n",
    "        layer.return_sequences = False\n",
    "\n",
    "# Add layers for promoter classification. We're disabling these for now so that the \n",
    "# coeffients of the output layer can be used to interpret the recurrent state\n",
    "# output_projection_dimension = 16\n",
    "#finetuned_model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "finetuned_model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d9f94",
   "metadata": {},
   "source": [
    "## Initial fine tuning\n",
    "\n",
    "We make a first training run where the `Embedding` and `LSTM` layers are _frozen_, meaning that they will not be changed during training. The reason for this is to allow for the newly added, randomly initialized, layers to first adapt to the representations of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaeccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRateVal = 0.001\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "finetuned_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "finetuned_model.summary()\n",
    "\n",
    "Path('./logs/finetuned').mkdir(exist_ok=True, parents=True)\n",
    "Path('./models/finetuned').mkdir(exist_ok=True, parents=True)\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='models/finetuned/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/finetuned'),\n",
    "]\n",
    "\n",
    "finetuned_model.fit(training_dataset, epochs=10, validation_data=dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81433e28",
   "metadata": {},
   "source": [
    "## Full fine tuning\n",
    "\n",
    "Now that we've fine tuned the output layers for a while, let's fine tune the whole model. We start by setting all layers to trainable again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in finetuned_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5222c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRateVal = 0.001\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "finetuned_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "finetuned_model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='finetuned_models/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./finetuned_logs'),\n",
    "]\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "finetuned_model.fit(training_dataset, epochs=10, validation_data=dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f947b",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Try to change the `lm_model` to have a higher capacity, like you did with the network without fine tuning. Are you able to now train larger networks faster and to better performance?\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Implement the visualization for the `lm_model` layers and look at their activations. How do they differ from the activations of the first classifier model?\n",
    "Make the visualization for the finetuned classifier. How do these visualization differ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
