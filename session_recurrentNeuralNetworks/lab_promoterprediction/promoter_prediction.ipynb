{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad537c",
   "metadata": {},
   "source": [
    "# Preparations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d82f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 00:17:38.508796: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 00:17:39.075183: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-21 00:17:39.075251: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-21 00:17:39.862196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-21 00:17:39.862522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-21 00:17:39.862531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ed72b",
   "metadata": {},
   "source": [
    "# Promoter region classification\n",
    "\n",
    "Promoter region prediction is an important part to understand the transcription process. In this lab we'll take a look at a simple way of training a recurrent neural network to solve this task. We'll take the approach from the [DeePromoter](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2019.00286/full) model using data from the implementation [here](https://github.com/egochao/DeePromoter).\n",
    "\n",
    "The goal here is to see how we can easily use recurrent neural networks to classify sequences.\n",
    "\n",
    "\n",
    "## The dataset\n",
    "The dataset is made up known promoter regions from two different species; human and mouse. The regions are 300 base pairs long, and were extracted from $-249 \\sim +50$ base pairs (where $+1$ refers to the Transcription Start Site). For each species, two subset of promoter regions were created; those with TATA motifs and those without. This results in four different datasets of positive promoter regions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60c7f0",
   "metadata": {},
   "source": [
    "## Running on colab\n",
    "You can use this [link](https://colab.research.google.com/github/NBISweden/workshop-neural-nets-and-deep-learning/blob/rnn_labs/session_recurrentNeuralNetworks/lab_promoterprediction/promoter_prediction.ipynb) to run the notebook on Google Colab. If you do so, it's advised that you first make a copy to your own Google Drive before starting you work on the notebbok. Otherwise changes you make to the notebook will not be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f66e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you don't have the data and you are running the notebook on colab. It'll download it from github and exctract it to the current directory.\n",
    "from pathlib import Path\n",
    "data_directory = Path('data')\n",
    "archive_file = data_directory / 'deepromoter_data.zip'\n",
    "\n",
    "data_url = \"https://github.com/NBISweden/workshop-neural-nets-and-deep-learning/raw/rnn_labs/session_recurrentNeuralNetworks/lab_promoterprediction/data/deepromoter_data.zip\"\n",
    "\n",
    "if not data_directory.exists():\n",
    "    data_directory.mkdir(parents=True)\n",
    "    \n",
    "if not archive_file.exists():\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(data_url, archive_file)\n",
    "    \n",
    "if archive_file.exists():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(archive_file) as zf:\n",
    "        zf.extractall(data_directory)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cbe66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "data_directory = Path('data')\n",
    "species = ['human', 'mouse']\n",
    "promoter_type = ['TATA', 'nonTATA']\n",
    "\n",
    "# To train the model on different datasets, you can change this to use 'mouse' or 'human' \n",
    "# for the species and 'TATA' or 'nonTATA' for the promoter type\n",
    "selected_species = 'mouse'\n",
    "selected_promoter_type = 'TATA'\n",
    "selected_dataset_path = data_directory / selected_species / selected_promoter_type\n",
    "\n",
    "selected_sequences = []\n",
    "for sequence_file in selected_dataset_path.glob('*.txt'):\n",
    "    with open(sequence_file) as fp:\n",
    "        sequences = [line.strip() for line in fp]\n",
    "        selected_sequences.extend(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00851a",
   "metadata": {},
   "source": [
    "## The negative dataset\n",
    "\n",
    "To create negative sequences we will use the same method as in the DeePromoter paper, illustrated here:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"images/negative_generation.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The idea is that hard negative examples are introduced by essentially randomly subsituting parts of the positive sequences. We will create as many negative sequences as we have positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ed5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "ALPHABET= np.array([\"A\", \"C\", \"G\", \"T\"])\n",
    "\n",
    "def generate_negative_sequence(sequence, chunksize=15, shuffle_ratio=0.6, rng: np.random.Generator = None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # The sequences are strings, which doesnt allow for assignment. \n",
    "    # We convert it to a list first to to the assignment, then \n",
    "    # convert it back to a string in the end\n",
    "    mutable_sequence = np.array(list(sequence))\n",
    "    n_chunks = int(math.ceil(len(mutable_sequence)/chunksize))\n",
    "    n_replaced_chunks = int(math.ceil(n_chunks*shuffle_ratio))\n",
    "    selected_chunks = rng.choice(n_chunks, size=n_replaced_chunks, replace=False)\n",
    "    for i in selected_chunks:\n",
    "        start = i*chunksize\n",
    "        end = start + chunksize\n",
    "        chunk = mutable_sequence[start:end]\n",
    "        # We don't want to rely on just taking the chunksize below, since this \n",
    "        # chunk might be the last one and thus not fully chunksize long if the \n",
    "        # sequence length isnt divisible with chunksize\n",
    "        random_chunk = rng.choice(ALPHABET, size=len(chunk), replace=True)\n",
    "        mutable_sequence[start:end] = random_chunk\n",
    "    negative_sequence = ''.join(mutable_sequence.tolist())\n",
    "    return negative_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7652bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoter seqeunce AAAAAGAAAAATGCTTTTCAAAATTCTTATAGAATCACAAATTTCATGAAGGAGGGGGACACACACACATACAGCCTGTGAAGGAGAGAGGATATATGGAAGGAGAGAGGTAAAGAGGAAGTATATCTTAAGATCAAGTTTCACATAATGCCAGATATATTAGCATATACTGGTAGGATTATCTTTTCCCCGCCTACAAATAGCCTAAGCACCCTCCAGTCATAAAAATCAGAGAGAAGTAAGCTGAACAGCAGACACTCCTCCTTACTTCCCACCTCAGAGCAGCTGCATGACTTCCTG\n",
      "Negative seqeunce GACATTGCTGGATGCATGGATGTCGAAGGTAGCTATCATGTTTGAATGAAGGAGGGGGACACACACACATACAGCCTGTGAAGGAGAGAGTGCAATGGTGCCGTTCCCGTCTAGAGACATGTATATCTTAAGATCGACCGTTGCCTGGTGACGCTAAACTGCATGTATACTGGTAGGATTAAAGTTCGAGTGCCGTAACCCAAGATTAGTACCCTCCAGTCATAAAAATCAGAGAGAAGTAAGCTGAACAGCAGAACCAGTATCCTCGCTTTGGCGTGATATTACCCGTAACAAAATACA\n"
     ]
    }
   ],
   "source": [
    "test_sequence = selected_sequences[0]\n",
    "print(\"Promoter seqeunce\", test_sequence)\n",
    "negative_sequence = generate_negative_sequence(test_sequence)\n",
    "print(\"Negative seqeunce\", negative_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a291e2",
   "metadata": {},
   "source": [
    "## Creating a promoter dataset\n",
    "As in the language modelling lab, we will now create a keras `Sequence` class to handle the loading of data. We will reuse much of the code from the previous lab when it comes to encoding the data into integers suitable of ingestion into a neural network. A difference here is that we will train the model for classification, so the dataset will generate batches where the inputs are the sequences, and the target is a binary variable indicating whether it's a promoter region or a negative example.\n",
    "\n",
    "Also since the vocabulary is known and very constrained beforehand. We will not be using an additional `<UNK>` token. If there are any unknown nucleotides not denoted with an 'N' it's an error. We will still use an plug in tokenizer, there might be cases where the user might want to use some larger motifs as the basic token unit.\n",
    "\n",
    "We will create the negative sequences inside the dataset, so that this is transparently done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36bdb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, pad_sequences\n",
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "import numpy \n",
    "\n",
    "\n",
    "def character_tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "class PromoterSequenceDataset(Sequence):\n",
    "    def __init__(self, positive_sequences, batch_size, tokenizer=character_tokenizer, rng=None) -> None:\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = numpy.random.default_rng()\n",
    "        self.rng = rng\n",
    "                \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.positive_sequences = positive_sequences\n",
    "        self.negative_sequences = [generate_negative_sequence(promoter_sequence, rng=self.rng) for promoter_sequence in self.positive_sequences]\n",
    "        self.sequences = self.positive_sequences + self.negative_sequences\n",
    "        self.tokenized_data = [tokenizer(text) for text in self.sequences]\n",
    "        \n",
    "        self.token_encoding_map = {'<empty>': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
    "        self.inverse_token_encoding_map = {i: token for token, i in self.token_encoding_map.items()}\n",
    "        self.encoded_sequences = [self.encode_tokenized_text(text) for text in self.tokenized_data]\n",
    "        \n",
    "        self.n = len(self.encoded_sequences)//self.batch_size\n",
    "        self.labels = np.concatenate([np.ones(len(positive_sequences), dtype=np.int32), np.zeros(len(self.negative_sequences), dtype=np.int32)])\n",
    "        self.sequence_indices = np.arange(len(self.encoded_sequences))\n",
    "        self.shuffle_examples()\n",
    "        \n",
    "    def shuffle_examples(self):\n",
    "        self.rng.shuffle(self.sequence_indices)\n",
    "        self.batches = np.reshape(self.sequence_indices[:self.n*self.batch_size], (self.n, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_sequences_indices = self.batches[item]\n",
    "        batch_sequences = [self.encoded_sequences[i] for i in encoded_sequences_indices]\n",
    "        batch_labels = self.labels[encoded_sequences_indices]\n",
    "        \n",
    "        pad_size = max(len(s) for s in batch_sequences)\n",
    "        padded_sequences = pad_sequences(batch_sequences, pad_size, padding=\"post\", value=0)\n",
    "        # The loss function expects the labels to have the same shape as the neural \n",
    "        # network prediction, so we create a column vector out of it.\n",
    "        expanded_batch_labels = batch_labels[..., np.newaxis]  \n",
    "        return padded_sequences, expanded_batch_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # We want to rearrange the order of samples so that we don't get the same batches all the time\n",
    "        self.shuffle_examples()\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        tokenized_text = self.tokenize_text(sequence)\n",
    "        encoded_text = self.encode_tokenized_text(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def tokenize_text(self, sequence):\n",
    "        return self.tokenzier(sequence)\n",
    "    \n",
    "    def encode_tokenized_text(self, tokenized_sequence):\n",
    "        encoded_text = [self.token_encoding_map[c] for c in tokenized_sequence]\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_encoded_text(self, encoded_text):\n",
    "        decoded_text = [self.inverse_token_encoding_map[x] for x in encoded_text]\n",
    "        return decoded_text\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_encoding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "28ff8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "rng = np.random.default_rng(1729) # Change this to None if you want to get different random sequences each time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "visible_sequences, test_sequences = train_test_split(selected_sequences, test_size=0.1, random_state=rng.integers(2**31))\n",
    "train_sequences, dev_sequences = train_test_split(visible_sequences, test_size=0.1, random_state=rng.integers(2**31))\n",
    "\n",
    "training_dataset = PromoterSequenceDataset(train_sequences, BATCH_SIZE, rng=rng)\n",
    "dev_dataset = PromoterSequenceDataset(dev_sequences, BATCH_SIZE, rng=rng)\n",
    "test_dataset = PromoterSequenceDataset(test_sequences, BATCH_SIZE, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86dd0f",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In this lab we will use a simple model. The paper which this is based on uses a complex neural network architecture with a Convolutional Neural Network as a first encoder part, followed by a biLSTM encoding the sequences in both directions. Here we will just use a single direction LSTM to illustrate how one can do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "419ddc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "#from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "# Alternatively:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "embedding_dimension = 4\n",
    "rnn_dimension = 16\n",
    "output_projection_dimension = 16\n",
    "\n",
    "num_embeddings = training_dataset.get_vocab_size()\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_embeddings, embedding_dimension, mask_zero=True))\n",
    "# Add LSTM layers; X.shape[1] refers to the number of columns in X which is the number of time steps, or window size\n",
    "model.add(LSTM(units=rnn_dimension, activation=\"tanh\", unit_forget_bias=True, recurrent_dropout=0, dropout=0.2, use_bias=True))\n",
    "# Add dense layer with activation for categorical output\n",
    "model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a0ec87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19/19 [==============================] - 3s 143ms/step - loss: 0.4274 - accuracy: 0.7991 - val_loss: 0.4735 - val_accuracy: 0.8008\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 3s 143ms/step - loss: 0.4168 - accuracy: 0.8045 - val_loss: 0.4036 - val_accuracy: 0.8242\n",
      "Epoch 3/100\n",
      "11/19 [================>.............] - ETA: 1s - loss: 0.3970 - accuracy: 0.8200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_820529/2018070981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[0;32m--> 135\u001b[0;31m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniforge3/envs/rnn_labs/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learnRateVal = 0.01\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='models/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "model.fit(training_dataset, epochs=100, validation_data=dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c2c56",
   "metadata": {},
   "source": [
    "## Language model pre-training\n",
    "\n",
    "A common strategy in contemporary machine learning is to try to use datasets with large amounts of data to train a model. This model is then _transferred_ to a new problem and because of the pretraining on the larger set might be able to learn faster or better before overfitting. Here we will illustrate how we might be able to Language Modelling to first train a model on predicting what nucleotides are likely to follow in sequence, and then take the model which has learned this and train it on the classification task. The goal is to reduce the time it takes before the model starts learning the promoter classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d793e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, pad_sequences\n",
    "from collections import Counter # We will use a counter to keep track of which tokens are the most common\n",
    "import numpy \n",
    "\n",
    "\n",
    "def character_tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "class PromoterLMDataset(Sequence):\n",
    "    def __init__(self, positive_sequences, batch_size, tokenizer=character_tokenizer, rng=None) -> None:\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = numpy.random.default_rng()\n",
    "        self.rng = rng\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.positive_sequences = positive_sequences\n",
    "        self.sequences = self.positive_sequences\n",
    "        self.tokenized_data = [tokenizer(text) for text in self.sequences]\n",
    "        \n",
    "        self.token_encoding_map = {'<empty>': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
    "        self.inverse_token_encoding_map = {i: token for token, i in self.token_encoding_map.items()}\n",
    "        self.encoded_sequences = [self.encode_tokenized_text(text) for text in self.tokenized_data]\n",
    "        \n",
    "        self.n = len(self.encoded_sequences)//self.batch_size\n",
    "        self.sequence_indices = np.arange(len(self.encoded_sequences))\n",
    "        self.shuffle_examples()\n",
    "        \n",
    "    def shuffle_examples(self):\n",
    "        self.rng.shuffle(self.sequence_indices)\n",
    "        self.batches = np.reshape(self.sequence_indices[:self.n*self.batch_size], (self.n, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_sequences_indices = self.batches[item]\n",
    "        batch_sequences = [self.encoded_sequences[i] for i in encoded_sequences_indices]\n",
    "        input_sequences = [encoded_sequence[:-1] for encoded_sequence in batch_sequences]        \n",
    "        target_sequences = [encoded_sequence[1:] for encoded_sequence in batch_sequences]        \n",
    "        pad_size = max(len(s) for s in input_sequences)\n",
    "        padded_input_sequences = pad_sequences(input_sequences, pad_size, padding=\"post\", value=0)\n",
    "        padded_target_sequences = pad_sequences(target_sequences, pad_size, padding=\"post\", value=0)\n",
    "        \n",
    "        return padded_input_sequences, padded_target_sequences\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # We want to rearrange the order of samples so that we don't get the same batches all the time\n",
    "        self.shuffle_examples()\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        tokenized_text = self.tokenize_text(sequence)\n",
    "        encoded_text = self.encode_tokenized_text(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def tokenize_text(self, sequence):\n",
    "        return self.tokenzier(sequence)\n",
    "    \n",
    "    def encode_tokenized_text(self, tokenized_sequence):\n",
    "        encoded_text = [self.token_encoding_map[c] for c in tokenized_sequence]\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_encoded_text(self, encoded_text):\n",
    "        decoded_text = [self.inverse_token_encoding_map[x] for x in encoded_text]\n",
    "        return decoded_text\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_encoding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c94a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train_dataset = PromoterLMDataset(train_sequences, batch_size=BATCH_SIZE)\n",
    "lm_dev_dataset = PromoterLMDataset(train_sequences, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "962ca1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4, 2, 2, ..., 4, 3, 4],\n",
       "        [2, 4, 1, ..., 1, 2, 3],\n",
       "        [2, 1, 1, ..., 4, 3, 2],\n",
       "        ...,\n",
       "        [2, 4, 4, ..., 2, 4, 3],\n",
       "        [3, 2, 1, ..., 3, 1, 1],\n",
       "        [1, 2, 4, ..., 3, 3, 3]], dtype=int32),\n",
       " array([[2, 2, 1, ..., 3, 4, 2],\n",
       "        [4, 1, 2, ..., 2, 3, 3],\n",
       "        [1, 1, 2, ..., 3, 2, 2],\n",
       "        ...,\n",
       "        [4, 4, 1, ..., 4, 3, 1],\n",
       "        [2, 1, 3, ..., 1, 1, 4],\n",
       "        [2, 4, 3, ..., 3, 3, 2]], dtype=int32))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d9f618c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "#from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "# Alternatively:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "embedding_dimension = 4\n",
    "rnn_dimension = 64\n",
    "output_projection_dimension = 32\n",
    "\n",
    "num_embeddings = lm_train_dataset.get_vocab_size()\n",
    "lm_model = Sequential()\n",
    "lm_model.add(Embedding(num_embeddings, embedding_dimension, mask_zero=True))\n",
    "# Add LSTM layers; X.shape[1] refers to the number of columns in X which is the number of time steps, or window size\n",
    "lm_model.add(LSTM(units=rnn_dimension, return_sequences=True, activation=\"tanh\", unit_forget_bias=True, recurrent_dropout=0, dropout=0.2, use_bias=True))\n",
    "# Add dense layer with activation for categorical output\n",
    "lm_model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "lm_model.add(Dense(num_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "01f4cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 10s 649ms/step - loss: 1.5189 - accuracy: 0.2553 - val_loss: 1.4052 - val_accuracy: 0.2349\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 4s 501ms/step - loss: 1.3942 - accuracy: 0.2613 - val_loss: 1.3906 - val_accuracy: 0.2354\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 4s 482ms/step - loss: 1.3862 - accuracy: 0.2761 - val_loss: 1.3831 - val_accuracy: 0.3151\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 1.3813 - accuracy: 0.2921 - val_loss: 1.3788 - val_accuracy: 0.3159\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 4s 484ms/step - loss: 1.3747 - accuracy: 0.3110 - val_loss: 1.3697 - val_accuracy: 0.3141\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 1.3689 - accuracy: 0.3121 - val_loss: 1.3662 - val_accuracy: 0.3142\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 1.3682 - accuracy: 0.3127 - val_loss: 1.3644 - val_accuracy: 0.3176\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 4s 494ms/step - loss: 1.3658 - accuracy: 0.3146 - val_loss: 1.3630 - val_accuracy: 0.3164\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 4s 478ms/step - loss: 1.3633 - accuracy: 0.3165 - val_loss: 1.3594 - val_accuracy: 0.3191\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 4s 482ms/step - loss: 1.3599 - accuracy: 0.3175 - val_loss: 1.3543 - val_accuracy: 0.3205\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 1.3562 - accuracy: 0.3206 - val_loss: 1.3512 - val_accuracy: 0.3220\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 4s 488ms/step - loss: 1.3527 - accuracy: 0.3272 - val_loss: 1.3445 - val_accuracy: 0.3356\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 4s 508ms/step - loss: 1.3467 - accuracy: 0.3320 - val_loss: 1.3407 - val_accuracy: 0.3353\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 4s 491ms/step - loss: 1.3429 - accuracy: 0.3345 - val_loss: 1.3378 - val_accuracy: 0.3416\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 4s 482ms/step - loss: 1.3404 - accuracy: 0.3374 - val_loss: 1.3350 - val_accuracy: 0.3414\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 4s 507ms/step - loss: 1.3383 - accuracy: 0.3383 - val_loss: 1.3318 - val_accuracy: 0.3449\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 4s 491ms/step - loss: 1.3365 - accuracy: 0.3398 - val_loss: 1.3304 - val_accuracy: 0.3455\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 4s 481ms/step - loss: 1.3349 - accuracy: 0.3414 - val_loss: 1.3286 - val_accuracy: 0.3477\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 4s 488ms/step - loss: 1.3334 - accuracy: 0.3432 - val_loss: 1.3289 - val_accuracy: 0.3466\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 1.3328 - accuracy: 0.3429 - val_loss: 1.3275 - val_accuracy: 0.3481\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 4s 478ms/step - loss: 1.3319 - accuracy: 0.3442 - val_loss: 1.3261 - val_accuracy: 0.3493\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 4s 474ms/step - loss: 1.3309 - accuracy: 0.3446 - val_loss: 1.3259 - val_accuracy: 0.3493\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 4s 477ms/step - loss: 1.3307 - accuracy: 0.3455 - val_loss: 1.3250 - val_accuracy: 0.3501\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 1.3302 - accuracy: 0.3452 - val_loss: 1.3242 - val_accuracy: 0.3505\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 4s 487ms/step - loss: 1.3292 - accuracy: 0.3461 - val_loss: 1.3244 - val_accuracy: 0.3498\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 4s 488ms/step - loss: 1.3285 - accuracy: 0.3462 - val_loss: 1.3231 - val_accuracy: 0.3518\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 4s 507ms/step - loss: 1.3287 - accuracy: 0.3463 - val_loss: 1.3223 - val_accuracy: 0.3525\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 1.3278 - accuracy: 0.3471 - val_loss: 1.3229 - val_accuracy: 0.3519\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 4s 489ms/step - loss: 1.3272 - accuracy: 0.3478 - val_loss: 1.3227 - val_accuracy: 0.3523\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 1.3268 - accuracy: 0.3478 - val_loss: 1.3218 - val_accuracy: 0.3521\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 4s 487ms/step - loss: 1.3268 - accuracy: 0.3484 - val_loss: 1.3225 - val_accuracy: 0.3522\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 4s 491ms/step - loss: 1.3267 - accuracy: 0.3476 - val_loss: 1.3211 - val_accuracy: 0.3541\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 4s 473ms/step - loss: 1.3260 - accuracy: 0.3495 - val_loss: 1.3215 - val_accuracy: 0.3539\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 1.3255 - accuracy: 0.3494 - val_loss: 1.3210 - val_accuracy: 0.3536\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 4s 482ms/step - loss: 1.3248 - accuracy: 0.3510 - val_loss: 1.3211 - val_accuracy: 0.3547\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 4s 490ms/step - loss: 1.3249 - accuracy: 0.3501 - val_loss: 1.3200 - val_accuracy: 0.3553\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 4s 495ms/step - loss: 1.3239 - accuracy: 0.3512 - val_loss: 1.3196 - val_accuracy: 0.3557\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 4s 491ms/step - loss: 1.3237 - accuracy: 0.3510 - val_loss: 1.3199 - val_accuracy: 0.3554\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 1.3228 - accuracy: 0.3525 - val_loss: 1.3189 - val_accuracy: 0.3563\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 1.3229 - accuracy: 0.3530 - val_loss: 1.3186 - val_accuracy: 0.3565\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 4s 501ms/step - loss: 1.3226 - accuracy: 0.3528 - val_loss: 1.3180 - val_accuracy: 0.3565\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 4s 503ms/step - loss: 1.3221 - accuracy: 0.3530 - val_loss: 1.3176 - val_accuracy: 0.3579\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 4s 490ms/step - loss: 1.3221 - accuracy: 0.3532 - val_loss: 1.3182 - val_accuracy: 0.3571\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 4s 498ms/step - loss: 1.3220 - accuracy: 0.3535 - val_loss: 1.3176 - val_accuracy: 0.3578\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 4s 472ms/step - loss: 1.3220 - accuracy: 0.3529 - val_loss: 1.3188 - val_accuracy: 0.3568\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 1.3212 - accuracy: 0.3541 - val_loss: 1.3182 - val_accuracy: 0.3574\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 6s 685ms/step - loss: 1.3207 - accuracy: 0.3541 - val_loss: 1.3171 - val_accuracy: 0.3587\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 5s 589ms/step - loss: 1.3208 - accuracy: 0.3548 - val_loss: 1.3167 - val_accuracy: 0.3582\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 1.3203 - accuracy: 0.3549 - val_loss: 1.3153 - val_accuracy: 0.3593\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 4s 478ms/step - loss: 1.3192 - accuracy: 0.3559 - val_loss: 1.3152 - val_accuracy: 0.3599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f904d508f50>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnRateVal = 0.01\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "lm_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='lm_models/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./lm_logs'),\n",
    "]\n",
    "Path('lm_models').mkdir(exist_ok=True)\n",
    "lm_model.fit(lm_train_dataset, epochs=50, validation_data=lm_dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea84cc",
   "metadata": {},
   "source": [
    "## Transferring the network\n",
    "We will transfer what this network has learnt by removing the layers which took the recurrent state to predict the next token, disable the return of all states and finally add a new classification layer at the end to solve the binary classification task. We start by making a copy of the language modelling network so we can continue to train it if we would like.\n",
    "\n",
    "We will start training the finetuned model with the embedding and recurrent layers frozen, so that the randomly initialized new Dense layers don't make noisy changes to them. We do this by setting the flag `layer.trainable = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3f47626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    (None, None, 4)           24        \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, None, 64)          17664     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, None, 32)          2080      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, None, 6)           198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,966\n",
      "Trainable params: 19,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = keras.models.clone_model(lm_model)\n",
    "finetuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "74ae0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop the last two dense layers\n",
    "finetuned_model.pop()\n",
    "finetuned_model.pop()\n",
    "\n",
    "# Adjust the return_sequences of the LSTM layer and set the layers to not be trainable\n",
    "for layer in finetuned_model.layers:\n",
    "    #layer.trainable = False\n",
    "    if isinstance(layer, LSTM):\n",
    "        layer.return_sequences = False\n",
    "# Add layers for promoter classification\n",
    "\n",
    "output_projection_dimension = 16\n",
    "finetuned_model.add(Dense(output_projection_dimension, activation=\"relu\"))\n",
    "# Here we use a single output which we will use for the binary cross entropy\n",
    "finetuned_model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d9f94",
   "metadata": {},
   "source": [
    "## Initial fine tuning\n",
    "\n",
    "We make a first training run where the `Embedding` and `LSTM` layers are _frozen_, meaning that they will not be changed during training. The reason for this is to allow for the newly added, randomly initialized, layers to first adapt to the representations of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "deaeccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 10s 366ms/step - loss: 0.6930 - accuracy: 0.5006 - val_loss: 0.6929 - val_accuracy: 0.4863\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.6921 - accuracy: 0.5000 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.6775 - accuracy: 0.5430 - val_loss: 0.6592 - val_accuracy: 0.6074\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 6s 327ms/step - loss: 0.6848 - accuracy: 0.5641 - val_loss: 0.6858 - val_accuracy: 0.5039\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.6906 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5020\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 6s 329ms/step - loss: 0.6917 - accuracy: 0.5006 - val_loss: 0.6915 - val_accuracy: 0.4941\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.6910 - accuracy: 0.4984 - val_loss: 0.6912 - val_accuracy: 0.4961\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.6907 - accuracy: 0.4994 - val_loss: 0.6910 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.6904 - accuracy: 0.4996 - val_loss: 0.6908 - val_accuracy: 0.5098\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 6s 329ms/step - loss: 0.6905 - accuracy: 0.4992 - val_loss: 0.6904 - val_accuracy: 0.4980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fba503350>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnRateVal = 0.001\n",
    "opt = Adam(learning_rate=learnRateVal)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "finetuned_model.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "finetuned_model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='finetuned_models/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./finetuned_logs'),\n",
    "]\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "finetuned_model.fit(training_dataset, epochs=10, validation_data=dev_dataset, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750124e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
