---
title: "Recurrent neural networks"
author:
  - Per Unneberg
date: "22 March, 2023"
institute: NBIS
from: markdown+emoji
format:
  revealjs:
    theme:
      - white
      - ../../common_assets/nbis.scss
      - custom.scss
    self-contained: true
    toc: true
    toc-depth: 1
    slide-level: 3
    slide-number: true
    preview-links: true
    chalkboard: false
    footer: Recurrent neural networks
    logo: https://nbis.se/assets/img/logos/nbislogo-green.svg
    smaller: false
    highlight-style: gruvbox
    fig-width: 12
    fig-height: 10
    fig-align: center
    text-align: center
    width: 1050
    height: 700
    margin: 0.1
    navigation-mode: vertical
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
execute:
  echo: false
  warning: false
  cache: false
  include: true
  autodep: true
  eval: true
  error: true
  freeze: auto
opts:
  knitr:
    code-fold: true
    tidy: true
    fig-format: svg
bibliography: references.bib
nocite: |
  @hochreiter_long_1997
---

### Setup  {visibility="hidden" .unnumbered .unlisted}

```{r  knitr-setup }
#| echo: false
#| eval: true
#| cache: false
library(knitr)
library(tidyverse)
library(kableExtra)
library(reticulate)
options(browser = "firefox")
knitr::knit_hooks$set(inline = function(x) {
                      prettyNum(x, big.mark =" ")
})
font_opts <- list(template = "tikzfig.tex",
                  dvisvgm.opts = "--font-format=woff")
knitr::opts_chunk$set(engine.opts = font_opts)
```
```{r  load-python-libraries, engine='python' }
import os
import sys
import pandas as pd
import rnnutils
import bokeh
from bokeh.models import ColumnDataSource, HoverTool
from bokeh import plotting
from bokeh.io import output_notebook
from bokeh.embed import components 
```

```{r  python-load-bokeh-scripts, results="asis", engine='python' }
print("""
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js"
        crossorigin="anonymous"></script>
""") 
```

:::: {.notes}

1.  Recap perceptron
    -   Even if it has been done before recap perceptron with my notation
    -   want to show what it looks like with a perceptron in a
        sequential model
2.  Sequential models
    -   begin with simple model, e.g. sinus time series
    -   DNA sequence characteristics, language processing, time series
        (maybe intuitively simplest)
    -   solve with perceptron
    -   highlight problems with perceptron
3.  RNNs
    -
4.  LSTMs and GRUs
    -   solution to vanishing gradients
    -   need to explain **what** they do and **how** they solve the
        issue:
        -   gated inputs / outputs
        -   ReLUs (indep from above or part of?)
5.  Practical applications
    -   look in literature to focus on life sciences; possibly also
        languages as this is interesting in itself (e.g. google
        translate)

::::


# Recap
### Perceptron (single neuron) { .smaller }

::::::::::::::::::: { .twocolgrid style="grid-template-columns: 0.5fr 1fr;"}

:::: {}

```{r tikz-rnn-recap-perceptron-simple, fig.ext="svg", engine='tikz' }
#| fig-width: 5
#| out-width: 100%
\begin{tikzpicture}
  \pic {mackayperceptron};
\end{tikzpicture} 
```

::::

:::: {}

#### Architecture

A single neuron has $n$ *inputs* $x_i$ and an *output* $y$. To each
input is associated a *weight* $w_i$.

#### Activity rule

The **activity rule** is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=0,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

[@mackay_information_2003]

::::

::::::::

::: {.notes}

Beware of notation here. Points to make:

-   $w_0=1$ -> bias
-   activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 (MacKay, 2003, p. 471)

(Alexander Amini, 2021, p. 5:43) point out inputs $x_i$ represent
**one** time point

:::


### Perceptron (single neuron) { .smaller }

::::::::::::::::::: { .twocolgrid style="grid-template-columns: 0.5fr 1fr;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-rnn-recap-perceptron-activity
#| fig-width: 5
#| out-width: 100%
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture} 
```

::::

:::: {}

#### Architecture

A single neuron has $n$ *inputs* $x_i$ and an *output* $y$. To each
input is associated a *weight* $w_i$.

#### Activity rule

The **activity rule** is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=0,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

[@mackay_information_2003]
::::

::::::::

::: {.notes}

Beware of notation here. Points to make:

-   $w_0=1$ -> bias
-   activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 (MacKay, 2003, p. 471)

(NO_ITEM_DATA:alexander_amini_mit_2021) point out inputs $x_i$ represent
**one** time point

:::


### Perceptron (single neuron) { .smaller }

:::::::::::::::::::  { .twocolgrid style="grid-template-columns: 0.5fr 1fr;" }

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-rnn-recap-perceptron-vectorized
#| fig-width: 10
#| out-width: 100%
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture} 
```

::::

:::: {.compact}

$$a = w_0 + \sum_{i} w_ix_i, \quad i=1,...,n$$

$$y = y(a) = g\left( w_0 + \sum_{i=1}^{n} w_ix_i \right)$$

:::: {.fragment }

or in vector notation

$$y = g\left(w_0 + \mathbf{X^T} \mathbf{W} \right)$$

where:

$$\quad\mathbf{X}=
\begin{bmatrix}x_1\\ \vdots \\ x_n\end{bmatrix},
\quad \mathbf{W}=\begin{bmatrix}w_1\\ \vdots \\ w_n\end{bmatrix}$$

::::
[@alexander_amini_mit_2021_rnn]
::::

::::::::

::: {.notes}

Follow MIT notation: g() is the non-linear activation (function)

:::

### Simplified illustration and notation

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-rnn-recap-perceptron-simplified
#| fig-width: 10
\begin{tikzpicture}[node distance=4*\basenodesep, >=latex]

\node[input={$\boldsymbol{x}$}] (x) {};
\node[ionode={16pt}{black}{$\sum$}, draw=black, thick, minimum size=16pt, right of=x] (sum) {};
\node[sigtan={16pt}{blue}{4pt}, right of=sum] (tanh) {};
\node[output={$y$}, right of=tanh] (y) {};

\draw[->] (x) -- (sum) node [midway, above] {$\boldsymbol{w}$};
\draw[->] (sum) -- (tanh) node [midway, above] {$\boldsymbol{wx}$};
\draw[->] (tanh) -- (y) node [midway, above] {$\mathrm{tanh(}\boldsymbol{wx}\mathrm{)}$};
\end{tikzpicture} 
```

#### Architecture

Vectorized versions: input $\boldsymbol{x}$, weights $\boldsymbol{w}$,
output $\boldsymbol{y}$

#### Activity rule

$$a = \boldsymbol{wx}$$

:::: {.notes}

FIXME: inconsistent notation? Weights are depicted as attached to
first arrow, then the labels indicate what **value** is passed along

::::

### Feed forward network

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-recap-perceptron-multiout
#| out-height: 500px
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=0},
    nncon/.append style={->},
  }
  \rnntikzset{
    dotted=true
  }
  \pic (i_) {nnlayer={5}{input}{X}{m}{}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/boxed=true] (h1_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=5*\basenodesep, \rnntikzbasekey/boxed=true] (h_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=7*\basenodesep] (o_) {nnlayer={3}{output}{\widehat{Y}}{n}{}};
  \pic {connectlayers={i_}{h1_}{5}{3}};
  \pic {connectlayers={h1_}{h_}{3}{3}};
  \pic {connectlayers={h_}{o_}{3}{3}};

  \node[xshift=3.5*\basenodesep, yshift=-2.5*\basenodesep] {$\dots$};
  \node[xshift=2*\basenodesep, yshift=2*\basenodesep] {$1$};
  \node[xshift=5*\basenodesep, yshift=2*\basenodesep] {$k$};

  \node[yshift=-4*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=7*\basenodesep, yshift=-4*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture} 
```

:::: {.notes}

Show multi-valued (vector) output and hidden layer

::::


### Simplified illustration

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-recap-perceptron-multiout-simple
#| out-height: 300px
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=-270},
    nnconnection/.append style={->},
  }
  \begin{scope}[rotate=270, transform shape]
    \pic {rnnio={}{$X$}{$Y$}};
  \end{scope}
  \node[yshift=-2*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=3*\basenodesep, yshift=-2*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture} 
```

:::: {.notes}

Condense hidden layers to a box.

::::


### Simplified illustration


```{r,  fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-recap-perceptron-multiout-simple-rotated
#| out-height: 400px
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=0},
    nnconnection/.append style={->},
  }
  \begin{scope}[rotate=0, transform shape]
    \pic {rnnio={}{$X$}{$Y$}};
  \end{scope}
  \node[yshift=0*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[yshift=3*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture} 
```


:::: {.notes}

Condense hidden layers to a box.

::::


# Sequential models
### Motivation

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-motivation-time-series
#| out-height: 400px
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=white] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


### Motivation

```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-rnn-motivation-time-series-1
#| out-height: 400px
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


### Motivation

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-motivation-time-series-2
#| out-height: 400px
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


### Motivation

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-motivation-time-series-3
#| out-height: 400px
\begin{tikzpicture}[>=latex]
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\draw[->, thick, black!50, dotted] (x2) to[out=90, in=180] (x1) to[out=0, in=90] (x0);
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::



### Sequences around us {.smaller}

::::::::::::::::::: { style="display: grid; grid-template-columns: 1fr 1fr; grid-template-rows: 1fr 2fr; grid-row-gap: 0px;" }

:::: {.compact}

Word prediction

![](grf/whattimeisit.png){width=250}

::::

:::: {.compact}

Language translation

:::  {}
```{r, fig.ext="svg", engine='tikz' }
#| label:   rnn-example-language-translation
#| out-height: 200px
\begin{tikzpicture}[node distance=2cm, >=latex]
  \node[align=left, font=\ttfamily, text width=22pt, rectangle, draw=black, thick] (vec) {+0.5 +0.2 -0.1 -0.3 +0.4 +1.2};
  \node[left of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=blue!20, draw=blue!80, anchor=east, rounded corners, thick, label={center:Encoder}] (encoder) {};
  \node[left of=encoder, anchor=east] (swedish) {jag är en student};
  \node[right of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=violet!20, draw=violet!80, anchor=west, rounded corners, thick, label={center:Decoder}] (decoder) {};
  \node[right of=decoder, anchor=west] (english) {I am a student};
  \draw[->] (swedish) -- (encoder);
  \draw[->] (encoder) -- (vec);
  \draw[->] (vec) -- (decoder);
  \draw[->] (decoder) -- (english);
\end{tikzpicture} 
```
:::
::::

::::  {.compact style="transform: translate(0, -60px);" }
Time series

![](https://github.com/unit8co/darts/raw/master/static/images/example.png){width=300}

[@herzen2021darts]
::::

::::  {.compact style="transform: translate(0, -60px);" }
Genomics

![](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-33321-1/MediaObjects/41598_2018_33321_Fig5_HTML.png?as=webp){width=240}

[@shen_recurrent_2018]
::::

:::::::::::::::::::

:::: {.notes}

Word prediction according to (Karpathy, 2015):

> model the probability distribution of the next character in the
> sequence given a sequence of previous characters.

::::



### Types of models {.smaller}

:::::::: {.fourcolgrid}

:::: {} 

one to one

```{r, fig.ext="svg", engine='tikz' }
#| label: sequential-models-one-to-one
#| out-height: 100px
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic{rnnio={}{}{}};
\end{tikzpicture} 
```

::::


:::: {.fragment .item2 data-fragment-index="2"}

many to one

```{r, fig.ext="svg", engine='tikz' }
#| label: sequential-models-many-to-one
#| out-height: 100px
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnioin};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioin};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {.fragment data-fragment-index="3"}

one to many

```{r, fig.ext="svg", engine='tikz' }
#| label:   sequential-models-one-to-many
#| out-height: 100px
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioout};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnioout};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {.fragment data-fragment-index="4"}

many to many

```{r, fig.ext="svg", engine='tikz' }
#| label:   sequential-models-many-to-many
#| out-height: 100px
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnio={}{}{}};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {}

Image classification

```{r, engine='python' }
#| label:   fashion-mnist-image-classification
from tensorflow import keras
import matplotlib.pyplot as plt
fashion_mnist = keras.datasets.fashion_mnist
img = fashion_mnist.load_data()[0][0:2][0][0:2]
label = fashion_mnist.load_data()[0][0:2][1][0:2]
class_names = [ "T-shirt/top" , "Trouser" , "Pullover" , "Dress" , "Coat" , "Sandal" , "Shirt" , "Sneaker" , "Bag" , "Ankle boot" ]
plt.figure(figsize=(10,5))
plt.rc('axes', labelsize=40)
for i in range(2):
    plt.subplot(1,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(img[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[label[i]])
plt.show() 
```

::::

:::: {.fragment data-fragment-index="2"}

Sentiment analysis

![](https://d1sjtleuqoc1be.cloudfront.net/wp-content/uploads/2019/04/25112909/shutterstock_1073953772.jpg){height=100}

::::

:::: {.fragment data-fragment-index="3"}

Image captioning

![](https://cocodataset.org/images/captions-splash.jpg){height=150}

::::

:::: {.fragment data-fragment-index="4"}

Machine translation

![](https://img.icons8.com/plasticine/344/google-translate-new-logo.png){height=100}

::::

::::::::

[@karpathy_unreasonable_effectiveness_of_RNNs]

:::: {.notes}

Important point here: each input/output/hidden are **vectors**

(Karpathy, 2015)

Issues with Vanilla NNs and CNNs:

-   dependency on **fixed** size input
-   fixed amount of computational steps

Models:

-   **one to one:** Vanilla processing without RNN, from fixed input to
    fixed output e.g. image classification (aka vanilla neural network)
-   **one to many:** sequence output, e.g. image captioning
-   **many to one:** sequence input, e.g. sentiment analysis (classify
    sequence as happy/sad/&#x2026;)
-   **many to many:** sequence input and sequence output, e.g. machine
    translation

Data:

-   (Xiao et al., 2017)

-   <https://cocodataset.org/#captions-2015>

::::


# Recurrent Neural Networks (RNNs)

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-rnn-folded-only
#| out-height: 400px
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={A}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

:::: {.notes}

We will now look at the essentials of RNNs. As the figure implies, the
output of the network

(NO_ITEM_DATA:alexander_amini_mit_2021) point out inputs $x_i$ represent
**one** time point

input, output, green box: contains vectors of data, arrows represent
operations/functions
(Karpathy, 2015)

Key feature: the recurrence (green) can be applied as many times as we
want, i.e. no constraint on input size

Why recurrent networks?
<https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn>

FFNs

-   Cannot handle sequential data
-   Considers only the current input
-   Cannot memorize previous inputs

and information only flows forward (i.e. no memory)

::::

### Feed forward network implementation to sequential data {.smaller}

::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}
```{r, fig.ext="svg", engine='tikz' }
#| label:  ffn-xt-1
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```
::::

:::: {.fragment	data-fragment-index="2" style="border-left: 2px black solid;"}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-1
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \end{scope}
  \end{tikzpicture} 
```

::::

:::: {}

::::

:::: {.element class="fragment" data-fragment-index="1"}

Assume multiple time points.

::::

:::::::::::::::::::

:::: {.notes}

Rotated FFN: take a moment to recap the ffn. Input $X_t \in
\mathbb{R}^{m}$ is mapped to output $\widehat{Y}_t \in \mathbb{R}^n$ via
the network ($f(\cdot)$

Now assume we have several time steps, starting at e.g. time 0. Also we predict the outputs individually.

::::


### Feed forward network implementation to sequential data {.smaller}

::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-xt-2
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```
::::

:::: { style="border-left: 2px black solid;"}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-2
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

::::

:::::::::::::::::::

:::: {.notes}

Add another time step&#x2026;

::::


### Feed forward network implementation to sequential data {.smaller}

::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-xt-3
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: { style="border-left: 2px black solid; "}

```{r  , fig.ext="svg", engine='tikz' }
#| label: ffn-x0-xt-3
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

> - Dependency of inputs not modelled such that ambiguous sequences
  cannot be distinguished:

:::: fragment

"dog bites man" vs "man bites dog"

::::

::::

:::::::::::::::::::

:::: {.notes}

Use an ambiguous example to point out that ffns can't distinguish
order of words; we explicitly want to model sequential dependencies

Example: "the boat is in the water" vs "the water is in the boat"

Alt example: "man bites dog" vs "dog bites man" (, Zhang et al., 2021, p. 8.1)

Emphasize fact that any prediction is based only on the current input

::::


### Feed forward network implementation to sequential data {.smaller}

::::::::::::::::::: { .compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-xt-4
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-4
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=both] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

-   Time points are modelled **individually** ( $\hat{Y}_t = f(X_t)$ )

::::

:::::::::::::::::::

:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::


### Feed forward network implementation to sequential data {.smaller}

::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:  ffn-xt-5
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-5
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic[\rnntikzbasekey/shade=input] (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

-   Time points are modelled **individually** ( $\hat{Y}_t = f(X_t)$ )
-   Also want dependency on **previous** inputs ( $\hat{Y}_t = f(..., X_1, X_0)$ )

::::

:::::::::::::::::::

:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::


### Adding recurrence relations {.smaller}

::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r  , fig.ext="svg", engine='tikz' }
#| label: ffn-xt-arr-1
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  , fig.ext="svg", engine='tikz' }
#| label: ffn-x0-xt-arr-1
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::

:::: {.notes}

We want to model dependencies over time. Solution is to model the cell
state (a hidden state) and pass this information on to the next 

::::


### Adding recurrence relations {.smaller}
::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:  ffn-xt-arr-2
#| out-height: 250px

\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-arr-2
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::


### Adding recurrence relations {.smaller}
::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-xt-arr-3
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-arr-3
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::

	
### Adding recurrence relations {.smaller}
::::::::::::::::::: {.compact .twocolgrid style="grid-template-columns: 250px auto; grid-template-rows: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-xt-arr-4
#| out-height: 250px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic[\rnntikzbasekey/.cd, add labels=true, folded=true] {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px white solid; "}

```{r, fig.ext="svg", engine='tikz' }
#| label:   ffn-x0-xt-arr-4
#| out-height: 250px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \end{scope}

  \node[font=\Huge, left of=x0_left, node distance=0.7*\ionodesize] {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

Folded representation

::::

:::: {}

Unfolded representation

:::::: fragment

Add a *hidden state* $h$ that introduces a dependency on the previous
step:

$$
\hat{Y}_t = f(X_t, h_{t-1})
$$

::::::

::::

:::::::::::::::::::

:::: {.notes}

$h_t$ is a summary of the inputs we've seen sofar

(Zhang et al., 2021, Chapter 8.4):

> If we want to incorporate the possible effect of words earlier than
> time step t−(n−1) on xt, we need to increase n. However, the number of
> model parameters would also increase exponentially with it, as we need
> to store |V|n numbers for a vocabulary set V. Hence, rather than
> modeling P(xt∣xt−1,…,xt−n+1) it is preferable to use a latent variable
> model:
> 
> P(xt∣xt−1,&#x2026;,x1) ~ P(xt∣ht−1),

IOW, with ht the recurrence becomes a latent variable model.

::::


### Sequential memory of RNNs { .smaller}

RNNs have what one could call "sequential memory" [@phi_illustrated_2020_RNN]

#### Alphabet

Exercise: say alphabet in your head

[A B C ... X Y Z]{style="font-size: 1.5em; text-align: center; font-family: DejaVu Sans Mono;"} 

:::: {.fragment}

Modification: start from e.g. letter F

May take time to get started, but from there on it's easy

::::

:::: {.fragment}

Now read the alphabet in reverse:

[Z Y X ... C B A]{style="font-size: 1.5em; text-align: center; font-family: DejaVu Sans Mono;"} 

::::

:::: {.fragment}

Memory access is *associative* and *context-dependent*

::::

:::: {.notes}

Provide the alphabet example from [@phi_illustrated_2020_RNN]

cf (, Haykin, 2010, p. 203):

> For a neural network to be dynamic, it must be given *short-term
> memory* in one form or the other

::::

### Recurrent Neural Networks {.smaller}


::::::::::::::::::: {.twocolgrid style="grid-template-columns: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-rnn-folded-hidden-eq-1
#| out-width: 300px
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {.fragment .center}

Add recurrence relation where current hidden cell state $h_t$ depends
on input $x_t$ and previous hidden state $h_{t-1}$ via a function
$f_W$ that defines the network parameters (weights):

$$
h_t = f_\mathbf{W}(x_t, h_{t-1})
$$

::::

:::: {.fragment .center}

Note that the same function and weights are used across all time
steps!

::::

:::: 

:::::::::::::::::::


### Recurrent Neural Networks - pseudocode {.smaller}

::::::::::::::::::: {.twocolgrid style="grid-template-columns: 300px auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-rnn-folded-hidden-eq-2
#| out-width: 300px
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {style="font-size: 1.2em"}

```{r, engine='python' }
#| label: rnn-simple-pseudocode
#| echo: true
#| eval: false
class RNN:
  # ...
  # Description of forward pass
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y

rnn = RNN()
ff = FeedForwardNN()

for word in input:
    output = rnn.step(word)

prediction = ff(output) 
```

::::

:::: 

:::::::::::::::::::

:::: {.notes}

Pseudocode examples, my example:

```{r  engine='python' }


class RNN:
    def __init__(self):
        # Initialize weights and cell state
        self._h = [...]
        self._Whh = [...]
        self._Wxh = [...]
        self._Why = [...]

    def update_cell_state(self, x):
        # function is some function that updates cell state
        self._h = function(self._h * self._Whh + x * self.Wxh)
    
    def predict(self):
        return self._h * self._Why

    def update_weights(self, y):
        # Calculate error via some loss function
        error = loss(self.predict() - y)
        # update weights via back propagation...

rnn = RNN()

for x, y in input_data:
    rnn.update_cell_state(x)
    rnn.update_weights(y)

# Retrieve next prediction
yhat = rnn.predict() 
```

(Karpathy, 2015)

```{r  engine='python' }
rnn = RNN()
y = rnn.step(x) # x is an input vector, y is the RNN's output vector

class RNN:
  # ...
  # Description of forward pass
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
 
```

A two-layer network would look as follows:

```{r  engine='python' }
y1 = rnn.step(x)
y2 = rnn.step(y1) 
```

Keras version (<https://keras.io/api/layers/recurrent_layers/rnn/#rnn-class>):

```{r  engine='python' }
class MinimalRNNCell(keras.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = backend.dot(inputs, self.kernel)
        output = h + backend.dot(prev_output, self.recurrent_kernel)
        return output, [output] 
```

Also (Phi, 2020b)

```{r  engine='python' }
rnn = RNN()
ff = FeedForwardNN()
hidden_state = [0.0, 0.0, 0.0, 0.0]

for word in input:
    output, hidden_state = rnn(word, hidden_state)

prediction = ff(output) 
```

Also (Phi, 2020a) :

```{r  engine='python' }
def LSTMCell(prev_ct, prev_ht, input):
    combine = prev_ct + input
    candidate = candidate_layer(combine)
    it = input_layer(combine)
    Ct = prev_ct * ft + candidate * it
    ot = output_layer(combine)
    ht = ot * tanh(Ct)
    return ht, Ct

ct = [0, 0, 0]
ht = [0, 0, 0]

for input in inputs:
    ct, ht = LSTMCell(ct, ht, input) 
```

::::

### Vanilla RNNs {.smaller}

::::::::::::::::::: { .twocolgrid style="grid-template-columns: 400px auto;" }

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-vanilla-rnn-folded-hidden-eq-1
#| out-width: 300px
\begin{tikzpicture}[thick]
  \pic[\rnntikzbasekey/.cd, folded=true, add labels=true] {rnnio={tanh}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {.fragment .center data-fragment-index="3"}

<h3 style="color: red;">Output vector</h3>

$$
\hat{Y}_t = \mathbf{W_{hy}^T}h_t
$$

::::

:::: {.fragment .center data-fragment-index="2"}

<h3 style="color: green;">Update hidden state</h3>

$$
h_t = \mathsf{tanh}(\mathbf{W_{xh}^T}X_t + \mathbf{W_{hh}^T}h_{t-1})
$$

::::

:::: {.fragment .center data-fragment-index="1"}

<h3 style="color: blue;">Input vector</h3>

$$
X_t
$$

::::

:::: 

::::::::::::::::::: 


### Vanilla RNNs

[@olah_christopher_understanding_nodate]

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-vanilla-rnn-unfolded-weights-1
#| out-width: 1200px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3.1*\xd, yshift=0.6*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


### Vanilla RNNs

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-vanilla-rnn-unfolded-weights-2
#| out-width: 1200px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


### Vanilla RNNs

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-vanilla-rnn-unfolded-weights-3
#| out-width: 1200px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units
(Lendave, 2021)

Add pseudocode to exemplify

::::


### Vanilla RNNs

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-vanilla-rnn-unfolded-weights-4
#| out-width: 1200px
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=north west] at (r_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rl_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rr_yt.south) {$\mathbf{W_{hy}}$};
\end{tikzpicture} 
```

:::: {.fragment .center}

Note: $\mathbf{W_{xh}}$, $\mathbf{W_{hh}}$, and $\mathbf{W_{hy}}$ are
shared across all cells!

::::

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


### Desired features of RNN {.smaller}

<div class="fragment">
#### 1. Variable sequence lengths

Not all inputs are of equal length
<br/>
</div>

<div class="fragment">
#### 2. Long-term memory

"I grew up in England, and &#x2026; I speak fluent English"
<br/>
</div>

<div class="fragment">
#### 3. Preserve order
"dog bites man" != "man bites dog"
<br/>
</div>

<div class="fragment">
#### 4. Share parameters

Adresses points 2 and 3.
<br/>
</div>

:::: {.notes}

-   variable sequence lengths

From (Cho et al., 2014):

> architectur that learns to *encode* a variable-length sequence into a
> fixed-length vector representation and to *decode* a given
> fixed-length representation back into a variable-length sequence

::::


# Exercise
### [Example: Box & Jenkins airline passenger data set]{style="font-size: 0.9em;"} {.smaller}

::::::: {.twocolgrid style="grid-template-columns: 60% auto;"}

::: { }

```{r, fig.ext="png", engine='python' }
#| label:   box-jenkins-airline-1
#| out-width: 500px
#| echo: false
#| fig-align: center
df = rnnutils.airlines()
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
rnnutils.plt.show() 
```
[@onnen_temporal_2021]
:::

::: {}



:::



:::::::




:::: {.notes}

(Onnen, 2021)

See also
<https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/>
for rnn example on sunspots

Important: need to explicitly show how data is partitioned as this can
be difficult to understand

<https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/>

Herzen article on darts:
<https://medium.com/unit8-machine-learning-publication/training-forecasting-models-on-multiple-time-series-with-darts-dc4be70b1844>
::::

### Example: generate test and training data {.smaller}

::::::: {.twocolgrid style="grid-template-columns: 60% auto;"}

::: {}

```{r, fig.ext="png", engine='python' }
#| label:  box-jenkins-airline-2
#| echo: false
#| out-width: 500px
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
p = ax.plot(df.time[100:144], df.passengers[100:144], color="red")
p = ax.legend(["train", "test"])
rnnutils.plt.show() 
```

:::

::: {.fragment}

Partition time series into training and test data sets at an e.g. 2:1
ratio:

```{r, engine='python' }
#| label:   box-jenkins-airline-partition-data
#| echo: true
#| eval: true
import rnnutils
import numpy as np
df = rnnutils.airlines()
data = np.array(
    df['passengers'].values
    .astype('float32')
).reshape(-1, 1)
train, test, scaler = rnnutils.make_train_test(data) 
```
:::

:::::::



### Example: prepare data for keras {.smaller}

```{r , fig.ext="svg", engine='tikz' }
#| label:  tikz-prepare-airline-data-for-keras
#| cache: false
#| out-width: 600px
\begin{tikzpicture}[node distance=2cm, align=center, >=latex]
  \node (data) {data = [0, 10, 20, 30, 40, 50, 60, 70]};
  \node[below left of=data, text width=1cm, rectangle, draw=black, node distance=4cm] (t) {t=0,1 t=2,3 t=4,5};
  \node[right of=t, text width=1cm, rectangle, draw=black] (x) {0, 10  20, 30 40, 50};
  \node[right of=x, text width=0.8cm, rectangle, draw=black] (t2) {t=2 t=4 t=6};
  \node[right of=t2, text width=0.5cm, rectangle, draw=black] (y) {20 40 60};
  \node[above of=x, node distance=0.9cm] (xlab) {X};
  \node[above of=y, node distance=0.9cm] (ylab) {Y};
  \draw (xlab.north) edge["predict Y from X by row", ->, bend left] (ylab.north);
\end{tikzpicture} 
```

:::: {.fragment }

```{r, engine='python' }
#| label:  airline-example-makexy
#| echo: true
#| eval: true
time_steps = 12
trainX, trainY, trainX_indices, trainY_indices = rnnutils.make_xy(train, time_steps)
testX, testY, testX_indices, testY_indices = rnnutils.make_xy(test, time_steps) 
```

::::


### Example: create vanilla RNN model {.smaller}

```{r, engine='python' }
#| label:   airline-rnn-model
#| echo: true
#| eval: true
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN

model = Sequential()
model.add(SimpleRNN(units=3, input_shape=(time_steps, 1),
                    activation="tanh"))
model.add(Dense(units=1, activation="tanh"))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary() 
```

:::: {.notes}

On RNN layers and time steps (<https://keras.io/guides/working_with_rnns/>):

-   the units correspond to the output size (yhat)
-   an RNN layer processes batches of input sequences
-   an RNN layer loops RNN cells that process one input at a time (e.g.
    one word, one time point)

Also from the source code (LSTMCell):

> This class processes one step within the whole time sequence input, whereas
> \`tf.keras.layer.LSTM\` processes the whole sequence.

The number of LSTMCells is defined by the units parameter, e.g.:

```{r  engine='python' }
# Stacked layer
rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)] 
```

::::


### Example: fit the model and evaluate {.smaller}

:::: {style="font-size: 1.0em"}

```{r, engine='python' }
#| label:   airline-model-fit
#| echo: true
#| eval: false
history = model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)
Ytrainpred = model.predict(trainX)
Ytestpred = model.predict(testX) 
```

::::

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% 50%; grid-column-gap: 10px; grid-row-gap: 0px"}

:::: {style="font-size: 1.0em"}

```{r, engine='python' }
#| label:   airline-plot-history-command
#| echo: true
#| eval: false
rnnutils.plot_history(history) 
```

:::: 

:::: {style="font-size: 1.0em"}

```{r, engine='python' }
#| label:   airline-plot-model-fit-command
#| echo: true
#| eval: false
data = {'train': (model.predict(trainX), train, trainY_indices),
        'test': (model.predict(testX), test, testY_indices)}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20),
                   labels=df.year[range(0, 144, 20)]) 
```

:::: 

:::: {}

```{r, fig.ext="png", engine='python' }
#| label:   airline-plot-history
#| echo: false
#| eval: true
#| out-width: 80%
d = {'loss': [0.15468591451644897, 0.04875592887401581,
              0.026622500270605087, 0.02037993259727955, 0.01717367395758629,
              0.01483415998518467, 0.013102399185299873, 0.011407987214624882,
              0.01016605831682682, 0.009000968188047409, 0.008080212399363518,
              0.007306812796741724, 0.006582655943930149, 0.005831228103488684,
              0.0052399300038814545, 0.004740677773952484, 0.004358494654297829,
              0.003976745996624231, 0.0037244476843625307, 0.003332830499857664]}
rnnutils.plot_history(d) 
```

::::

:::: {}

```{r, fig.ext="png", engine='python' }
#| label:   airline-plot-model-fit
#| out-width: 80%
#| eval: true
#| echo: false
data = {"train": [np.array([[0.07300922274589539], [0.049322694540023804], [0.12263555824756622], [0.10771659016609192], [0.08678102493286133], [0.0807582437992096], [0.16047000885009766], [0.13826081156730652], [0.12276159226894379], [0.09296061843633652], [0.005158169195055962], [0.07526839524507523], [0.058306947350502014], [0.050476402044296265], [0.1301475316286087], [0.11820383369922638], [0.05991018936038017], [0.08855298906564713], [0.18303704261779785], [0.2147040069103241], [0.12630178034305573], [0.18395473062992096], [-0.05559101700782776], [0.12674248218536377], [0.13126012682914734], [0.07171593606472015], [0.2608124911785126], [0.1329176276922226], [0.22195808589458466], [0.16103020310401917], [0.2596116065979004], [0.2631970942020416], [0.222457155585289], [0.19196732342243195], [0.08305624127388], [0.13445650041103363], [0.23295335471630096], [0.12711040675640106], [0.2997269332408905], [0.19363194704055786], [0.16322757303714752], [0.29337525367736816], [0.2702861726284027], [0.3810942769050598], [0.24219587445259094], [0.29155629873275757], [0.1209971085190773], [0.22091759741306305], [0.31695908308029175], [0.14803078770637512], [0.37239915132522583], [0.33496564626693726], [0.3264579474925995], [0.3433589041233063], [0.32992351055145264], [0.41659021377563477], [0.2793201804161072], [0.3563149571418762], [0.11158497631549835], [0.229257732629776], [0.33076655864715576], [0.10553482174873352], [0.3575865626335144], [0.30193302035331726], [0.3180869221687317], [0.33221328258514404], [0.4005788266658783], [0.406070739030838], [0.3288029730319977], [0.34573444724082947], [0.18024373054504395], [0.2741532623767853], [0.3648494482040405], [0.25930526852607727], [0.37611520290374756], [0.36360082030296326], [0.3635270297527313], [0.3676840662956238], [0.43250808119773865], [0.41396385431289673], [0.37228304147720337], [0.38717105984687805], [0.23085232079029083], [0.3464904725551605]]), np.array([0.015444010496139526, 0.027027025818824768, 0.054054051637649536, 0.04826255142688751, 0.03281852602958679, 0.059845566749572754, 0.08494207262992859, 0.08494207262992859, 0.06177607178688049, 0.028957530856132507, 0.0, 0.027027025818824768, 0.021235525608062744, 0.042471036314964294, 0.0714285671710968, 0.059845566749572754, 0.04054054617881775, 0.08687257766723633, 0.12741312384605408, 0.12741312384605408, 0.1042470932006836, 0.055984556674957275, 0.019305020570755005, 0.06949806213378906, 0.07915058732032776, 0.08880308270454407, 0.1428571343421936, 0.11389961838722229, 0.13127413392066956, 0.1428571343421936, 0.18339768052101135, 0.18339768052101135, 0.15444016456604004, 0.11196911334991455, 0.0810810923576355, 0.11969110369682312, 0.12934362888336182, 0.14671814441680908, 0.1718146800994873, 0.14864864945411682, 0.1525096595287323, 0.22007721662521362, 0.2432432472705841, 0.2664092481136322, 0.20270270109176636, 0.16795367002487183, 0.13127413392066956, 0.17374518513679504, 0.17760616540908813, 0.17760616540908813, 0.25482624769210815, 0.2528957426548004, 0.24131274223327637, 0.26833975315093994, 0.3088802993297577, 0.3243243396282196, 0.2567567527294159, 0.20656371116638184, 0.14671814441680908, 0.18725869059562683, 0.19305017590522766, 0.1621621549129486, 0.2528957426548004, 0.2374517321586609, 0.2509652376174927, 0.3088802993297577, 0.38223937153816223, 0.36486485600471497, 0.2992278039455414, 0.24131274223327637, 0.1911197006702423, 0.24131274223327637, 0.2664092481136322, 0.24903473258018494, 0.3146717846393585, 0.318532794713974, 0.3204633295536041, 0.40733590722084045, 0.5019304752349854, 0.46911194920539856, 0.4015444219112396, 0.3281853497028351, 0.2567567527294159, 0.33590731024742126, 0.3474903404712677, 0.3339768350124359, 0.41119691729545593, 0.403474897146225, 0.4131273925304413, 0.521235466003418, 0.5965250730514526, 0.5810810327529907, 0.4845559895038605, 0.3899613916873932, 0.3223938047885895, 0.3899613916873932]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])], "test": [np.array([[0.41985487937927246], [0.2897593379020691], [0.43206876516342163], [0.3984266519546509], [0.39088255167007446], [0.43195009231567383], [0.4271005094051361], [0.42042967677116394], [0.37909582257270813], [0.3912075459957123], [0.33704182505607605], [0.35475632548332214], [0.43653494119644165], [0.28377869725227356], [0.4360058009624481], [0.35397568345069885], [0.3680872619152069], [0.4023270905017853], [0.3970481753349304], [0.40373513102531433], [0.3252890706062317], [0.37355899810791016], [0.2793232798576355], [0.31504255533218384], [0.38788625597953796], [0.25943249464035034], [0.414558470249176], [0.3525553047657013], [0.3922974467277527], [0.3646024167537689], [0.3950953483581543], [0.34701961278915405], [0.3233894407749176], [0.32918328046798706], [0.2957551181316376], [0.3380872905254364]]), np.array([0.40733590722084045, 0.3803088963031769, 0.4864864647388458, 0.4710424840450287, 0.4845559895038605, 0.6138995885848999, 0.6969112157821655, 0.7007721662521362, 0.5791505575180054, 0.46911194920539856, 0.38803085684776306, 0.4478764235973358, 0.4555984437465668, 0.4131273925304413, 0.49806949496269226, 0.4710424840450287, 0.4999999701976776, 0.6389961242675781, 0.747104287147522, 0.7741312980651855, 0.5791505575180054, 0.49227800965309143, 0.39768341183662415, 0.44980695843696594, 0.4942084848880768, 0.45945945382118225, 0.5830116271972656, 0.5637065172195435, 0.6100386381149292, 0.7104246616363525, 0.8571429252624512, 0.8783783912658691, 0.6930501461029053, 0.584942102432251, 0.49806949496269226, 0.5810810327529907, 0.6042470932006836, 0.5540540218353271, 0.6081080436706543, 0.6891891956329346, 0.7104246616363525, 0.832046389579773, 1.0, 0.9691120386123657, 0.7799227237701416, 0.6891891956329346, 0.5521235466003418, 0.6332045793533325]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])]}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20), labels=df.year[range(0, 144, 20)]) 
```

::::

::::::::::::::::::: 


### Example: model topology writ out {.smaller}

:::: {style="font-size: 1.0em; text-align: center;"}

```{r, engine='python' }
#| label:   airline-rnn-model-2
#| echo: false
#| eval: true
model.summary() 
```

::::


:::: {.twocolgrid style="grid-template-columns: 2fr 1fr;"}

::: {.compact}


```{r , fig.ext='svg', engine='tikz' }
#| label:  tikz-rnn-model-topology-writ-out-1
#| out-height: 300px
\begin{tikzpicture}[rotate=90, transform shape]
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \begin{scope}[yshift=0]
  \pic (xt) {nnlayer={1}{input}{}{}{blue}};
  \pic[xshift=3*\basenodesep] (ht) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=6*\basenodesep] (yt) {nnlayer={1}{output}{}{}{red}};
  \end{scope}

\begin{scope}[yshift=5*\basenodesep]
    \pic[xshift=3*\basenodesep, opacity=0.0] (h1) {nnlayer={3}{hidden}{}{}{green}};
  \end{scope}

\begin{scope}[yshift=-5*\basenodesep]
    \pic[xshift=3*\basenodesep, opacity=0.0] (hn) {nnlayer={3}{hidden}{}{}{green}};
  \end{scope}
  \pic {connectlayers={xt}{ht}{1}{3}};
  \pic {connectlayers={ht}{yt}{3}{1}};
  \node[iolabel] at (xt_n1) {$X_t$};
  \node[iolabel] at (yt_n1) {$Y_t$};
\end{tikzpicture}
```


:::

::: {}

:::

::::

:::: {.notes}

(Verma, 2021)

::::


### Example: model topology writ out {.smaller}

:::: {style="font-size: 1.0em; text-align: center;"}

```{r, engine='python' }
#| label:   airline-rnn-model-3
#| echo: false
#| eval: true
model.summary() 
```

::::

:::: {.twocolgrid style="grid-template-columns: 2fr 1fr;"}


::: {.compact}

```{r, fig.ext='svg', engine='tikz' }
#| label:   tikz-rnn-model-topology-writ-out-2
#| out-height: 300px
\begin{tikzpicture}[rotate=90, transform shape]
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \begin{scope}[yshift=0]
  \pic (xt) {nnlayer={1}{input}{}{}{blue}};
  \pic[xshift=3*\basenodesep] (ht) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=6*\basenodesep] (yt) {nnlayer={1}{output}{}{}{red}};
  \end{scope}

  \begin{scope}[yshift=5*\basenodesep]
    \pic (x1) {nnlayer={1}{input}{}{}{blue}};
    \pic[xshift=3*\basenodesep] (h1) {nnlayer={3}{hidden}{}{}{green}};
    \pic[xshift=6*\basenodesep] (y1) {nnlayer={1}{output}{}{}{red}};
  \end{scope}

  \begin{scope}[yshift=-5*\basenodesep]
    \pic (xn) {nnlayer={1}{input}{}{}{blue}};
    \pic[xshift=3*\basenodesep] (hn) {nnlayer={3}{hidden}{}{}{green}};
    \pic[xshift=6*\basenodesep] (yn) {nnlayer={1}{output}{}{}{red}};
  \end{scope}

  \pic {connectlayers={x1}{h1}{1}{3}};
  \pic {connectlayers={h1}{y1}{3}{1}};
  \pic {connectlayers={xt}{ht}{1}{3}};
  \pic {connectlayers={ht}{yt}{3}{1}};
  \pic {connectlayers={xn}{hn}{1}{3}};
  \pic {connectlayers={hn}{yn}{3}{1}};
  
  \draw (h1_n1.east) edge[->, bend left] (ht_n1.east)
    (h1_n2.east) edge[->, bend left] (ht_n2.east)
    (h1_n3.east) edge[->, bend left] (ht_n3.east)

    (h1_n1.west) edge[->, bend right] (ht_n2.west)
    (h1_n1.west) edge[->, bend right] (ht_n3.west)

    (h1_n2.west) edge[->, bend right] (ht_n1.west)
    (h1_n2.west) edge[->, bend right] (ht_n3.west)

    (h1_n3.west) edge[->, bend right] (ht_n1.west)
    (h1_n3.west) edge[->, bend right] (ht_n2.west)
    
    (h1_n3.south) edge[->] (ht_n1.north);

    \draw (ht_n1.east) edge[->, bend left] (hn_n1.east)
    (ht_n2.east) edge[->, bend left] (hn_n2.east)
    (ht_n3.east) edge[->, bend left] (hn_n3.east)

    (ht_n1.west) edge[->, bend right] (hn_n2.west)
    (ht_n1.west) edge[->, bend right] (hn_n3.west)

    (ht_n2.west) edge[->, bend right] (hn_n1.west)
    (ht_n2.west) edge[->, bend right] (hn_n3.west)

    (ht_n3.west) edge[->, bend right] (hn_n1.west)
    (ht_n3.west) edge[->, bend right] (hn_n2.west)
    
    (ht_n3.south) edge[->] (hn_n1.north);

    \node[iolabel] at (xt_n1) {$X_t$};
    \node[iolabel] at (yt_n1) {$Y_t$};
    \node[iolabel] at (x1_n1) {$X_1$};
    \node[iolabel] at (y1_n1) {$Y_1$};
    \node[iolabel] at (xn_n1) {$X_{12}$};
    \node[iolabel] at (yn_n1) {$Y_{12}$};

    \node at ($(h1_n3.west) !.5! (ht_n1.west)$) {\Huge$\vdots$};
\node at ($(ht_n3.west) !.5! (hn_n1.west)$) {\Huge$\vdots$};

\end{tikzpicture} 
```

:::

::: {}

:::: {.fragment }

NB! In keras, RNN input is a 3D tensor with shape `[batch, timesteps, feature]`

[@verma_understanding_2021]

::::


:::

::::

:::: {.notes}

(Verma, 2021)

::::

### An RNN in numbers {.smaller}

[@karpathy_unreasonable_effectiveness_of_RNNs]

```{r, fig.ext='svg', engine='tikz' }
#| label:   tikz-karpathy-rnn-example-numbers
#| out-width: 70%
\begin{tikzpicture}
  \tikzset{
    input/.style={rectangle, draw=black, fill=blue!30, text width=0.8cm, align=center},
    hidden/.style={rectangle, draw=black, fill=green!30, text width=0.8cm, align=center},
    output/.style={rectangle, draw=black, fill=red!30, text width=0.8cm, align=center},
  }

  \node[input] (i1) {1\\ 0\\ 0\\ 0};
  \node[input, right of=i1, node distance=3cm] (i2) {0\\ 1\\ 0\\ 0};
  \node[input, right of=i2, node distance=3cm] (i3) {0\\ 0\\ 1\\ 0};
  \node[input, right of=i3, node distance=3cm] (i4) {0\\ 0\\ 1\\ 0};

  \node[hidden, above of=i1, node distance=3cm] (h1) {0.3\\ -0.1\\ 0.9};
  \node[hidden, above of=i2, node distance=3cm] (h2) {1.0\\ 0.3\\ 0.1};
  \node[hidden, above of=i3, node distance=3cm] (h3) {0.1\\ -0.5\\ -0.3};
  \node[hidden, above of=i4, node distance=3cm] (h4) {-0.3\\ 0.9\\ 0.7};

  \newcommand{\op}[1]{\color{blue!70}\textbf{#1}\color{black}}
  \newcommand{\on}[1]{\color{red!70}\textbf{#1}\color{black}}

  \node[output, above of=h1, node distance=3cm] (o1) {\on{1.0}\\ \op{2.2} \\ \on{-3.0}\\\on{4.1}};
  \node[output, above of=h2, node distance=3cm] (o2) {\on{0.5}\\ \on{0.3} \\ \op{-1.0}\\\on{1.2}};
  \node[output, above of=h3, node distance=3cm] (o3) {\on{0.1}\\ \on{0.5} \\ \op{1.9}\\\on{-0.1}};
  \node[output, above of=h4, node distance=3cm] (o4) {\on{0.2}\\ \on{-1.5} \\ \on{-0.1}\\\op{2.2}};

  \node[left of=i1, node distance=2cm] (il) {input layer};
  \node[left of=h1, node distance=2cm] {hidden layer};
  \node[left of=o1, node distance=2cm] (ol) {output layer};
  \node[below of=il, node distance=1.25cm]  {input chars:};
  \node[above of=ol, node distance=1.25cm]  {target chars:};

  \node[above of=o1, anchor=south] {\op{e}};
  \node[above of=o2, anchor=south] {\op{l}};
  \node[above of=o3, anchor=south] {\op{l}};
  \node[above of=o4, anchor=south] {\op{o}};

  \node[below of=i1, anchor=north] {\textbf{h}};
  \node[below of=i2, anchor=north] {\textbf{e}};
  \node[below of=i3, anchor=north] {\textbf{l}};
  \node[below of=i4, anchor=north] {\textbf{l}};

  \draw[color=blue] (i1) edge[->] (h1)
  (i2) edge[->] (h2)
  (i3) edge[->] (h3)
  (i4) edge[->] node[left] {\color{blue!70}$\mathbf{W_{xh}}$\color{black}} (h4);

  \draw[color=green] (h1) edge[->] (h2)
  (h2) edge[->] (h3)
  (h3) edge[->] node[above] {\color{green!70}$\mathbf{W_{hh}}$\color{black}} (h4);

  \draw[color=red] (h1) edge[->] (o1)
  (h2) edge[->] (o2)
  (h3) edge[->] (o3)
  (h4) edge[->] node[left] {\color{red!70}$\mathbf{W_{hy}}$\color{black}} (o4);
\end{tikzpicture} 
```

Example network trained on "hello" showing activations in forward pass
given input "hell". The outputs contain confidences in outputs
(vocabulary={h, e, l, o}). We want blue numbers high, red numbers low.
P(e) is in context of "h", P(l) in context of "he" and so on.

:::: {.columns}

::: {.column width="50%"}

:::: {.fragment .center}

What is the topology of the network?

::::

:::

::: {.column width="50%"}

:::: {.fragment .center}

4 input units (features), 4 time steps, 3 hidden units, 4 output units

::::

:::

::::

:::: {.notes}

NB! This is what it could look like after a forward pass! During
training, we **want** to increase confidence for blue characters. Also,
for output 2 and more the output depends on all preceding **hidden**
states + the input.

Mention: e is conditional on h

l is conditional on input e + hidden state based on h. Quoting Karpathy:

> This training sequence is in fact a source of 4 separate training
> examples: 1. The probability of “e” should be likely given the context
> of “h”, 2. “l” should be likely in the context of “he”, 3. “l” should
> also be likely given the context of “hel”, and finally 4. “o” should
> be likely given the context of “hell”.

Ask for input\_shape: what is timesteps? (=4) What is features? (=4)
::::




### Exercise

See if you can improve the airline passenger model. Some things to
try:

-   change the number of units
-   change time\_steps
-   change the number of epochs


# Training

### Recap: backpropagation algorithm in ffns

[@alexander_amini_mit_2021_rnn]

<br/>

::::::::::::::::::: {.twocolgrid style="grid-template-columns: 50% auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-backpropagation-ffn-1
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


### Recap: backpropagation algorithm in ffns

[@alexander_amini_mit_2021_rnn]

<br/>

::::::::::::::::::: {.twocolgrid style="grid-template-columns: 50% auto;"}

:::: {}


```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-backpropagation-ffn-2
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[->, thick] (start) -- (end);
\end{tikzpicture} 
```

:::: 

:::: {}

1.  perform forward pass and generate prediction

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


### Recap: backpropagation algorithm in ffns

[@alexander_amini_mit_2021_rnn]

<br/>

::::::::::::::::::: {.twocolgrid style="grid-template-columns: 50% auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-backpropagation-ffn-3
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
\end{tikzpicture} 
```

:::: 

:::: {}

1.  perform forward pass and generate prediction
2.  calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
       \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$

:::: 

::::::::::::::::::: 




:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::



### Recap: backpropagation algorithm in ffns

[@alexander_amini_mit_2021_rnn]

<br/>

::::::::::::::::::: {.twocolgrid style="grid-template-columns: 50% auto;"}

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-backpropagation-ffn-4
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
  \node[below of=end] (bptt_start) {};
  \node[below of=start] (bptt_end) {};
  \draw[thick, ->, color=red] (bptt_start) -- (bptt_end);
\end{tikzpicture} 
```

:::: 

:::: {}

1.  perform forward pass and generate prediction
2.  calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
       \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$
3.  back propagate errors and update weights to minimize loss

:::: 

::::::::::::::::::: 



:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::

### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-1
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture} 
```






### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-2
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
\end{tikzpicture} 
```

### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-3
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
  \end{scope}
\end{tikzpicture} 
```


### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-4
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\end{tikzpicture} 
```


### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-5
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
\end{scope}  

\end{tikzpicture} 
```

### Backpropagation through time (BPTT) {.smaller}

[@alexander_amini_mit_2021_rnn]
```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-backpropagation-unfolded-6
#| out-height: 400px
#| out-width: 100%
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
  \node[ultra thick, node distance=.7*\basenodesep, below right of=xt_input]  (gradstart) {};
  \node[ultra thick, node distance=.7*\basenodesep, below left of=x0_input]  (gradend) {};
  \draw (gradstart) edge [->] (gradend);
\end{scope}  

\end{tikzpicture} 
```
:::::: {style="transform: translate(0, -30px);"}

:::: {.columns }

::: {.column width="50%"}
Errors are propagated backwards in time from $t=t$ to $t=0$.
:::

::: {.column width="50%"}

:::: {.fragment .center}
Problem: calculating gradient may depend on large powers of
$\mathbf{W_{hh}}^{\mathsf{T}}$ (e.g. $\delta\mathcal{L} / \delta h_0
\sim f((\mathbf{W_{hh}}^{\mathsf{T}})^t)$
::::
:::

::::

::::::






:::: {.notes}

Wording: gradient $(dL/dh_0) ~ f((W_hh^T)^t)$, i.e. gradient may depend on
large powers of $W^T_hh$. So gradient is $\propto a^t$, so if

(a > 1: exploding gradients; just mention here)

a < 1: vanishing gradients

This is problematic since the **size** of weight adjustments depend on
size of gradient

::::


### The effect of vanishing gradients on long-term memory {.smaller}

::::::::::::::::::: {style="display: grid; grid-template-columns: 30% auto; grid-column-gap: 0px; font-size: 0.7em; align: center;"}

:::: {}

:::: {.fragment .center}

In layer $i$ gradient size ~ $(\mathbf{W_{hh}}^{\mathsf{T}})^{t-i}$

::::

:::: {.fragment .center}

$\downarrow$

Weight adjustments depend on size of gradient

::::

:::: {.fragment .center}

$\downarrow$

Early layers tend to "see" small gradients and do very little updating

::::

:::: {.fragment .center}

$\downarrow$

Bias parameters to learn recent events 

::::

:::: {.fragment .center}

$\downarrow$

RNN suffer short term memory

::::

:::: 

:::: {}

:::: {.fragment .center .compact}

[@olah_christopher_understanding_nodate]

"The clouds are in the <span class="underline">\_</span>"

::::

:::: {.fragment .center .compact}

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-clouds-are-in-the-sky
#| out-height: 150px
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3*\rnnioxshiftsmall] (x3) {rnnio={}{$X_3$}{$\widehat{Y}_3$}};
  \pic[xshift=4*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (x4) {rnnio={}{$X_4$}{$\widehat{Y}_4$}};
  \pic[xshift=5*\rnnioxshiftsmall] (x5) {rnnio={}{$X_5$}{$\widehat{Y}_5$}};
  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (x3_left)
  (x3_right) edge (x4_left)
  (x4_right) edge (x5_left);
\end{tikzpicture} 
```

<br/>

::::

:::: {.fragment .center .compact}

"I grew up in England &#x2026; I speak fluent <span class="underline">\_</span>"

::::

:::: {.fragment .center .compact}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-I-speak-fluent-english
#| out-height: 150px
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  
  \pic[xshift=4.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt1) {rnnio={}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=5.5*\rnnioxshiftsmall] (xt2) {rnnio={}{$X_{t+2}$}{$\widehat{Y}_{t+2}$}};

  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (xt_left)
  (xt_right) edge (xt1_left)
  (xt1_right) edge (xt2_left);

\end{tikzpicture} 
```

::::

:::: 

::::::::::::::::::: 

:::: {.notes}

(Thomas, 2018)

The bigger the gradient, the bigger the adjustment and **vice versa**.
Gradients are calculated wrt to effects of gradients in previous
layer. If those adjustments were small, gradients will be small, which
in time leads to exponentially declining values. Early layers fail to
do any learning.

In flowchart: **given** that W is smaller than one, the gradients tend
to vanish and be negligible for early layers

Examples: highlight the context dependency of the prediction. Sky is
easy to infer, but in the second example, if the intervening paragraph
is long, we need context from much farther back.

::::






### Solutions to vanishing gradient

:::: {.columns}

::: {.column width="50%"}

#### 1. Activation function

ReLU (or leaky ReLU) instead of sigmoid or tanh.

Prevents small gradient: for $\mathbb{x>0}$, gradient positive
constant

:::

::: {.column width="50%"}

Derivatives of $\sigma$, $\mathsf{tanh}$ and $\mathsf{ReLU}$
activation functions.

```{r, fig.ext='svg', engine='tikz' }
#| label:   tikz-vanishing-gradient-trick-1
#| out-height: 400px
\begin{tikzpicture}
  \tikzset{declare function={
      sigma(\x)=1/(1+exp(-\x));
      sigmap(\x)=sigma(\x)*(1-sigma(\x));
      relu(\x)=x;
      tanhp(\x)=1-(exp(\x) - exp(-\x))^2/(exp(\x)+exp(-\x))^2;
    }
  }
  \pgfplotsset{every axis plot/.append style={thick}}
\begin{axis}%
  [
  grid=major,     
  xmin=-4,
  xmax=4,
  axis x line=bottom,
  ymax=1.1,
  ymin=-0.1,
  axis y line=middle,
  samples=100,
  domain=-4:4,
  legend style={at={(1,0.9)}},
  ytick=\empty
  ]
  \addplot[blue,mark=none]   (x,{sigmap(x)});
  \addplot[green, mark=none] (x, {tanhp(x)});
  \addplot[red,mark=none,domain=0:4]   (x,1);
  \addplot[red,mark=none,domain=-4:0]   (x, 0);
  \addplot +[red,mark=none] coordinates {(0, 0) (0, 1)};

  \legend{$\sigma'(x)$, $\mathsf{tanh}'(x)$, $\mathsf{ReLU}'$}
\end{axis}
\end{tikzpicture} 
```
:::

::::

### Solutions to vanishing gradient

:::: {.columns}

::: {.column width="50%"}

#### 2. Weight initialization

Set bias=0, weights to identity matrix

:::

::: {.column width="50%"}
```{r, fig.ext='svg', engine='tikz' }
#| label: tikz-vanishing-gradient-trick-2
#| out-height: 400px
\begin{tikzpicture}
  \matrix [matrix of math nodes,left delimiter=(,right delimiter=), anchor=west](W){ 
    1 & 0 & \dots  & 0\\
    0 & 1 & \dots  & 0\\
    \vdots  & \vdots  & \ddots & \vdots\\
    0 & 0 & \dots  & 1\\
  };
  \node [left of=W, anchor=east, node distance=1.5cm] {W =};

\end{tikzpicture} 
```

:::

::::

::: {.notes}

Prevents weights from shrinking too much

:::


### Solutions to vanishing gradient
:::: {.columns}

::: {.column width="50%"}


#### 3. More complex cells using "gating"
    
For example LSTM. Idea is to control what information is retained
within each RNN unit.

Make use regular multiplication (x) and addition (+).

:::

::: {.column width="50%"}

```{r, fig.ext='svg', engine='tikz' }
#| label:   tikz-vanishing-gradient-trick-3
#| out-height: 400px
\begin{tikzpicture}
  \pic {lstm};  
\end{tikzpicture} 
```

:::

::::


:::: {.notes}

Note that ReLUs not used in LSTMs / GRU as ReLU is non-negative
(however, see
https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/).
The tanh activation is needed so that values can be added **and**
subtracted. Sigmoid is in (0, 1).

::::




# LSTMs and GRUs
### Motivation behind LSTMs and GRUs {.smaller}

::::::::::::::::::: {.twocolgrid .compact}

:::: {.compact}

LSTM

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm
#| fig-width: 5
#| out-width: 350px
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture} 
```

::::

:::: {.compact}

GRU

```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-gru
#| fig-width: 5
#| out-width: 350px
\begin{tikzpicture}
  \pic {gru};
\end{tikzpicture} 
```

::::

:::::::::::::::::::

::: { .compact }

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-gru-lstm-legend
#| fig-width: 5
#| out-width: 600px
\tikzset{legend/.style={
        font=\sffamily\bfseries\tiny,
        text width=1.4cm,
        align=center
        }
}
\begin{tikzpicture}[node distance=1cm]
\node[pwise=X, node distance=2cm, label={[legend]below:pointwise multiplication}] (pmult) {};
\node[tanh, left of=pmult, label={[legend]below:tanh}] (tanh) {};
\node[sigmoid, left of=tanh, label={[legend]below:sigmoid}] (sigmoid) {};
\node[pwise=+, right of=pmult, label={[legend]below:pointwise addition}] (padd) {};
\node[pwise={1-}, right of=padd, label={[legend]below:pointwise inversion}] (pinvert) {};
\node[vcon=1cm, right of=pinvert, label={[legend]below:vector concatenation}] (vconcat) {};
\node[vcopy=1cm, right of=vconcat, label={[legend]below:vector copy}] (vcopy) {};
\end{tikzpicture} 
```

Long Short Term Memory (LSTM)  (Hochreiter &#38; Schmidhuber, 1997) and Gated
Recurrent Unit (GRU) (Cho et al., 2014) architectures were
proposed to solve the vanishing gradient problem.

:::

:::: {.notes}

Based on (Phi, 2020a)

-   solution to short-term memory
-   gates **regulate** the flow of information, concentrating on the important parts

In contrast to RNN, LSTM has *four* neural network layers that
interact via the gates

::::


### Intuition {.smaller}
	
:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;"}

In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN).
One RNN encodes a sequence of symbols into a fixed-length vector
representation, and the other decodes the representation into another
sequence of symbols. The encoder and decoder of the proposed model are
jointly trained to maximize the conditional probability of a target
sequence given a source sequence. The performance of a statistical
machine translation system is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
semantically and syntactically meaningful representation of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  (Cho et al., 2014)

::::

:::: {.notes}

(Phi, 2020a)

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

-   solution to vanishing gradient problem
-   gates regulate flow of information, focusing on the important parts

::::


### Intuition {.smaller}

:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;"}

In this paper, we propose a **novel neural network** model called **RNN**
**Encoder-Decoder** that consists of **two recurrent neural networks** (RNN).
One RNN **encodes** a **sequence of symbols** into a fixed-length vector
representation, and the other **decodes the representation** into another
sequence of symbols. The encoder and decoder of the proposed model are
**jointly trained** to maximize the conditional probability of a target
sequence given a source sequence. The performance of a **statistical**
**machine translation system** is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
**semantically and syntactically meaningful representation** of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  (Cho et al., 2014)

::::

<br/>

:::: {.fragment .center}

Remember the important parts, pay less attention to (forget) the rest.

::::

:::: {.notes}

(Phi, 2020a)

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

-   solution to vanishing gradient problem
-   gates regulate flow of information, focusing on the important parts

::::


### LSTM: Cell state flow and gating {.smaller}

::::::::::::::::::: {.twocolgrid }

:::: {.compact}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-cellstateflow
#| out-height: 250px
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, add labels=true, highlight=cellstateflow] {lstm} ;
\end{tikzpicture} 
```

:::: { .fragment .compact}


LSTM adds *cell state* that in effect provides the long-term memory

::::

:::: {.fragment .compact}

Information flows in the cell state from $c_{t-1}$ to $c_t$.

::::

::::

:::: { .compact}

[@olah_christopher_understanding_nodate]

::: {.fragment}

```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-lstmgate
#| out-height: 250px
\begin{tikzpicture}
  \pic {lstmgate} ;
\end{tikzpicture} 
```

Gates affect the amount of information let through. The sigmoid layer
outputs anything from 0 (nothing) to 1 (everything).

:::

::::

:::::::::::::::::::

:::: {.fragment .compact } 

[@cho_learning_2014]

> In our preliminary experiments, we found that it is crucial to use
> this new unit with gating units. We were not able to get meaningful
> result with an oft-used tanh unit without any gating.

::::

:::: {.notes}

(Olah, 2015)

NB: Olah's example revolves around a language model where we try to
predict the next output

(, Cho et al., 2014, p. 1726) on the hidden unit:

> In our preliminary experiments, we found that it is crucial to use
> this new unit with gating units. We were not able to get meaningful
> result with an oft-used tanh unit without any gating.

Difference between cell and hidden state (<https://datascience.stackexchange.com/questions/82808/difference-between-lstm-cell-state-and-hidden-state>):

-   Cell state: Long term memory of the model, only part of LSTM models
-   Hidden state: Working memory, part of LSTM and RNN models

for RNN, *every* previous state is considered in calculation of
backpropagation

LSTM: introduce cell state, in addition to hidden state, simply
providing longer memory, enabled by

-   the storage of useful beliefs from new inputs
-   the loading of beliefs into the working memory (i.e. cell state)
    that are immediately useful.

::::



### Forget, input, and output gates {.smaller}

::::::::::::::::::: { .threecolgrid }

:::: {.compact}

<h5>forget gate</h5>

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-forget-gate-only
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=forgetgate, \rnntikzbasekey/add labels=true] {lstm};
\end{tikzpicture} 
```

**Purpose:** reset content of cell state

::::

:::: {.compact}

<h5>input gate</h5>

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-input-gate-only
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, add labels=true, highlight=inputgate] {lstm};
\end{tikzpicture} 
```

**Purpose:** decide when to read data into cell state
::::

:::: {.compact}

<h5>output gate</h5>

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-output-gate-only
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=outputgate, \rnntikzbasekey/add labels=true] {lstm};
\end{tikzpicture} 
```

**Purpose:** read entries from cell state

::::

:::::::::::::::::::

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$ for each value in cell
state $c_{t-1}$, where 0 means "reset entry", 1 "keep it"

:::: {.notes}

::::


### The forget gate {.smaller .compact} 

::: {  }

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-forget-gate
#| fig-width: 5
\begin{tikzpicture}
\pic[\rnntikzbasekey/.cd, highlight=forgetgate, add labels=true] {lstm};
\end{tikzpicture} 
```
:::

**Purpose**: decide what information to keep or throw away

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$ for each value in cell
state $c_{t-1}$, where 0 means "forget entry", 1 "keep it"

:::: {.fragment}

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

::::

:::: {.notes}

> Let’s go back to our example of a language model trying to predict the
> next word based on all the previous ones. In such a problem, the cell
> state might include the gender of the present subject, so that the
> correct pronouns can be used. When we see a new subject, we want to
> forget the gender of the old subject.

::::


### Add new information - the input gate

::: { .compact }

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-candidatecellstate-input
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=inputgate, add labels=true] {lstm};
\end{tikzpicture} 
```

:::

::: { .compact }

Two steps to adding new information:

1.  sigmoid layer decides which values to update

:::


:::: {.notes}

> In the example of our language model, we’d want to add the gender of
> the new subject to the cell state, to replace the old one we’re
> forgetting.

::::


### Add new information - get candidate values {.smaller}

::: {.compact  style="transform: translate(0, -50px);" }

```{r, fig.ext="svg", engine='tikz' }
#| label:    tikz-lstm-candidatecellstate-input-1
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=candidatecellstate, add labels=true] {lstm};
\end{tikzpicture} 
```

:::

::: { .compact  style="transform: translate(0, -80px);"}


Two steps to adding new information:

1.  sigmoid layer decides which values to update
2.  tanh layer creates vector of new candidate values $\tilde{c}_t$

:::: {.fragment}

$$
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

::::


:::

:::: {.notes}

> In the example of our language model, we’d want to add the gender of
> the new subject to the cell state, to replace the old one we’re
> forgetting.

::::


### Updating the cell state {.smaller} 

::: { .compact  }

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-update-cell-state
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=updatecellstate, add labels=true] {lstm};
\end{tikzpicture} 
```

:::

::: { .compact }

1.  multiply old cell state by $f_t$ to forget what was decided to
    forget
2.  add new candidate values scaled by how much we want to update them
    $i_t * \tilde{c}_t$

:::: {.fragment}

$$
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t
$$

::::

:::

:::: {.notes}

> In the case of the language model, this is where we’d actually drop
> the information about the old subject’s gender and add the new
> information, as we decided in the previous steps.

::::


### Cell output {.smaller}

::: {.compact style="transform: translate(0, -50px);"}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-output-gate
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=celloutput, add labels=true] {lstm};
\end{tikzpicture} 
```
:::

::: {.compact style="transform: translate(0, -80px);"}

Output is filtered version of cell state.

1.  sigmoid output gate decides what parts of cell state to output
2.  push cell state through tanh and multiply by sigmoid output

:::: {.fragment}

$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
$$

::::

:::

:::: {.notes}

> For the language model example, since it just saw a subject, it might
> want to output information relevant to a verb, in case that’s what is
> coming next. For example, it might output whether the subject is
> singular or plural, so that we know what form a verb should be
> conjugated into if that’s what follows next.

::::


### LSTM: putting it together

```{r, fig.ext="svg", engine='tikz' }
#| label: tikz-lstm-intuition
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/add labels=true] {lstm};
\end{tikzpicture} 
```


### Intuition

-   if forget ~ 1, input ~ 0, $c_{t-1}$ will be saved to next time step
    (input irrelevant for cell state)
-   if forget ~ 0, input ~ 1, pay attention to the current input

:::: {.notes}

From (, Zhang et al., 2021, p. 9.2.1.3):

-   if forget ~ 1, input ~ 0, C<sub>t-1</sub> will be saved over time

::::


### LSTM: putting it together {.smaller}

[@zhang2021dive]

::::::::::::::::::: {.twocolgrid }

:::: {}

```{r, fig.ext="svg", engine='tikz' }
#| label:   tikz-lstm-2
#| fig-width: 5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/add labels=true] {lstm};
\end{tikzpicture} 
```

::::

:::: {}

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\\
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)\\
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t\\
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
$$

::::

:::::::::::::::::::

$$
x_t \in \mathbb{R}^{n\times d}, h_{t-1} \in \mathbb{n \times h},
i_t \in \mathbb{R}^{n\times h}, f_t \in \mathbb{R}^{n\times h}, o_t \in \mathbb{R}^{n\times h},
$$

and

$$
W_f \in \mathbb{R}^{n \times (h+d)}, W_i \in \mathbb{R}^{n \times (h+d)}, W_o \in \mathbb{R}^{n \times (h+d)}, W_c \in \mathbb{R}^{n \times (h+d)}
$$


### GRU

```{r, fig.ext="svg", engine='tikz' }
#| label:  tikz-big-gru
#| fig-width: 6
\begin{tikzpicture}
\pic {gru};  
\end{tikzpicture} 
```

-   forget and input states combined to single *update* gate
-   merge cell and hidden state
-   simpler model than LSTM

:::: {.notes}

(Olah, 2015)

Notes that GRU has been gaining traction lately (where lately=2015!)

Comparison (Lendave, 2021):

-   GRU has fewer parameters so uses less memory and executes faster
-   LSTM more accurate on larger datasets

::::


# Concluding remarks

### Example applications in genomics {.smaller}

:::: { .twocolgrid style="grid-template-columns: 1fr 1.3fr; align-items: center;" }

::: {}

Prediction of transcriptor factor binding sites

![](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-33321-1/MediaObjects/41598_2018_33321_Fig5_HTML.png?as=webp){width=350}

[@shen_recurrent_2018]

:::

::: {}

Recombination landscape prediction

![](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/mbe/37/6/10.1093_molbev_msaa038/1/m_msaa038f1.jpeg?Expires=1682056940&Signature=xYVVC0r-NfiwbF6VFCYWlzZRXKjhy2Z79R1k69YOfgMxfnNunKVZOrgbINEsih4d2esZEZTXZLlaE5slTFergEmhepvDCxgnzBCZiEwn4EmymE31QZpoXoib4ROT~jHPrgYWVUgR6JB2sqqi8KFwCh352mnO3CgNGt0xZj18gQhaAmH3nbbV3ZejsSRBy1NF88ejSh3BXROuPEA7-EwNLUeqbqetpDR35zvZ71VU-jiYJZkx1z6QTH3zJRhcg4zpbcHq0kV4VQdh5ld6CZhDDQyisONbw3w5SENibVYTUMFZtDNgfowyBQHlfvoZveFfV-6LHAdMycv8vybvOJRdBg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA){width=500}

[@adrion_PredictingLandscapeRecombination_2020]

:::

::::

### Limitations of recurrent neural networks

::: { .incremental }

- Encoding bottleneck
  - How to represent (embed) and compress data?
- Slow and difficult to parallelize
  - Slow convergence
  - Sequential nature not well adapted for parallelization
- Short memory
  - Don't scale to sequences > thousands of time steps


:::

### Attention {.smaller}

:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;"}

In this paper, we propose a **novel neural network** model called **RNN**
**Encoder-Decoder** that consists of **two recurrent neural networks** (RNN).
One RNN **encodes** a **sequence of symbols** into a fixed-length vector
representation, and the other **decodes the representation** into another
sequence of symbols. The encoder and decoder of the proposed model are
**jointly trained** to maximize the conditional probability of a target
sequence given a source sequence. The performance of a **statistical**
**machine translation system** is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
**semantically and syntactically meaningful representation** of linguistic
phrases.

::::


### Transformers

- process sequential input data (e.g., natural language)
- process **entire** input at once
- apply *attention* mechanism to provide positional context


:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;" .fragment}

Transformers were introduced in 2017 by a team at Google Brain and are
increasingly the model of choice for NLP problems, replacing RNN
models such as long short-term memory (LSTM). The additional training
parallelization allows training on larger datasets

[@_TransformerMachineLearning_2023]

::::


	
### Summary

::: { .incremental }

1. Sequential data can be modelled with **RNNs**
2. **Recurrence** to model sequences
3. Training with **back-propagation through time**
4. Gated units (**LSTM**, **GRU**) partially solve the vanishing
   gradient problem
5. **Transformers** model sequences **without** recurrence and have
   increasingly become the model of choice for many natural language
   processing (NLP) problems

:::


# Exercise
###


#### 1. Analyse airline passengers with LSTM

Modify the airline passenger model to use an LSTM and compare the
results. Try out different parameters to improve test predictions.


#### 2. Time-series forecasting with LSTM, discrete state space

LSTM with Variable Length Input Sequences to One Character Output

:::: {.notes}

Good example at (Koehrsen, 2018)

::::


### Time-series forecasting with LSTM, discrete state space


#### Objective

Predict next character in sequence of strings


#### Comments

-   you could use several LSTM layers, in which all but the last
    layer should return sequences (set `return_sequences=True`)
    [@brownlee_stacked_2017]

:::: {.notes}

Use (Karpathy, 2015) hello example
to illustrate how the modelling is done

::::



### Bibliography {.smaller .unnumbered .unlisted}

::: { #refs }
:::


<!--
  Local Variables:
  eval: (tree-sitter-mode 0)
  End:
  -->
