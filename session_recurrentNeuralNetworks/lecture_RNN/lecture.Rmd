
---
title: "Recurrent neural networks"
author:
  - Per Unneberg
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  revealjs::revealjs_presentation:
    css: revealjs.css
    includes:
      in_header: footer.html
    self_contained: false
    reveal_plugins: []
    highlight: breezedark
    fig_caption: false
    toc: false
    toc_depth: 2
    slide_level: 2
    transition: none
    reveal_options:
      slideNumber: true
      previewLinks: true
      minScale: 1
      maxScale: 1
      height: 800
      width: 1200
csl: /home/peru/opt/styles/apa.csl
mainfont: Liberation Serif
monofont: Liberation Mono
bibliography: references.bib
nocite: |
  @hochreiter_long_1997
---

```{r  knitr-setup, echo=FALSE, include=FALSE }
library(knitr)
library(tidyverse)
library(kableExtra)
library(reticulate)
options(browser="firefox")
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      autodep=TRUE, echo=FALSE,
                      cache=FALSE, include=TRUE, eval=TRUE, tidy=FALSE, error=TRUE
                      )
#class.source = "numberLines lineAnchors", comment="",
#                      class.output = c("numberLines lineAnchors chunkout"))
knitr::knit_hooks$set(inline = function(x) {
                      prettyNum(x, big.mark=" ")
})
knitr::opts_chunk$set(engine.opts=list(template="tikzfig.tex", dvisvgm.opts = "--font-format=woff")) 
```

```{r  load-python-libraries, engine='python' }
import os
import sys
import pandas as pd
import rnnutils
import bokeh
from bokeh.models import ColumnDataSource, HoverTool
from bokeh import plotting
from bokeh.io import output_notebook
from bokeh.embed import components 
```

```{r  python-load-bokeh-scripts, results="asis", engine='python' }
print("""
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js"
        crossorigin="anonymous"></script>
""") 
```


# Overview

<h3>Recap</h3>
<h3>Sequential models</h3>
<h3>Recurrent neural networks (RNNs)</h3>
<h3>LSTMs and GRUs</h3>
<h3>Practical applications</h3>

:::: {.notes}

1.  Recap perceptron
    -   Even if it has been done before recap perceptron with my notation
    -   want to show what it looks like with a perceptron in a sequential
        model
2.  Sequential models
    -   begin with simple model, e.g. sinus time series
    -   DNA sequence characteristics, language processing, time series (maybe intuitively simplest)
    -   solve with perceptron
    -   highlight problems with perceptron
3.  RNNs
    -
4.  LSTMs and GRUs
    -   solution to vanishing gradients
    -   need to explain **what** they do and **how** they solve the issue:
        -   gated inputs / outputs
        -   ReLUs (indep from above or part of?)
5.  Practical applications
    -   look in literature to focus on life sciences; possibly also languages as this is interesting in itself (e.g. google translate)

::::


# Recap


## Perceptron (single neuron)

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-rnn-recap-perceptron-simple, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {mackayperceptron};
\end{tikzpicture} 
```

::::

:::: {}

<h3>Architecture</h3>

A single neuron has $n$ *inputs* $x_i$ and an *output* $y$. To each
input is associated a *weight* $w_i$.

<h3>Activity rule</h3>
The **activity rule** is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=1,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

[@mackay_information_2003]

::::

::::::::

::: {.notes}

Beware of notation here. Points to make:

-   $w_0=1$ -> bias
-   activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 (MacKay, 2003, p. 471)

(Alexander Amini, 2021, p. 5:43) point out inputs $x_i$ represent
**one** time point

:::


## Perceptron (single neuron)

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-rnn-recap-perceptron-activity, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture} 
```

::::

:::: {}

<h3>Architecture</h3>

A single neuron has $n$ *inputs* $x_i$ and an *output* $y$. To each
input is associated a *weight* $w_i$.

<h3>Activity rule</h3>
The **activity rule** is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=1,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

[@mackay_information_2003]
::::

::::::::

::: {.notes}

Beware of notation here. Points to make:

-   $w_0=1$ -> bias
-   activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 (MacKay, 2003, p. 471)

(NO_ITEM_DATA:alexander_amini_mit_2021) point out inputs $x_i$ represent
**one** time point

:::


## Perceptron (single neuron)

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-rnn-recap-perceptron-vectorized, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture} 
```

::::

:::: {}

$$a = w_0 + \sum_{i} w_ix_i, \quad i=1,...,m$$

$$y = y(a) = g\left( w_0 + \sum_{i=1}^{m} w_ix_i \right)$$

:::: {.element: class="fragment"}

or in vector notation

$$y = g\left(w_0 + \mathbf{X^T} \mathbf{W} \right)$$

where:

$$\quad\mathbf{X}=
\begin{bmatrix}x_1\\ \vdots \\ x_m\end{bmatrix},
\quad \mathbf{W}=\begin{bmatrix}w_1\\ \vdots \\ w_m\end{bmatrix}$$

::::
[@alexander_amini_mit_2021_rnn]
::::

::::::::

::: {.notes}

Follow MIT notation: g() is the non-linear activation (function)

:::


## Simplified illustration and notation

```{r  tikz-rnn-recap-perceptron-simplified, cache=FALSE, fig.ext="svg", fig.width=10, engine='tikz' }
\begin{tikzpicture}[node distance=4*\basenodesep, >=latex]

\node[input={$\boldsymbol{x}$}] (x) {};
\node[ionode={16pt}{black}{$\sum$}, draw=black, thick, minimum size=16pt, right of=x] (sum) {};
\node[sigtan={16pt}{blue}{4pt}, right of=sum] (tanh) {};
\node[output={$y$}, right of=tanh] (y) {};

\draw[->] (x) -- (sum) node [midway, above] {$\boldsymbol{w}$};
\draw[->] (sum) -- (tanh) node [midway, above] {$\boldsymbol{wx}$};
\draw[->] (tanh) -- (y) node [midway, above] {$\mathrm{tanh(}\boldsymbol{wx}\mathrm{)}$};
\end{tikzpicture} 
```

<h3>Architecture</h3>

Vectorized versions: input $\boldsymbol{x}$, weights $\boldsymbol{w}$,
output $\boldsymbol{y}$

<h3>Activity rule</h3>

$$a = \boldsymbol{wx}$$

:::: {.notes}

FIXME: inconsistent notation? Weights are depicted as attached to
first arrow, then the labels indicate what **value** is passed along

::::


## Feed forward network

```{r  tikz-rnn-recap-perceptron-multiout, cache=FALSE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=0},
    nnconnection/.append style={->},
  }
  \rnntikzset{
    dotted=true
  }
  \pic (i_) {nnlayer={5}{input}{X}{m}{}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/boxed=true] (h1_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=5*\basenodesep, \rnntikzbasekey/boxed=true] (h_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=7*\basenodesep] (o_) {nnlayer={3}{output}{\widehat{Y}}{n}{}};
  \pic {connectlayers={i_}{h1_}{5}{3}};
  \pic {connectlayers={h1_}{h_}{3}{3}};
  \pic {connectlayers={h_}{o_}{3}{3}};

  \node[xshift=3.5*\basenodesep, yshift=-2.5*\basenodesep] {$\dots$};
  \node[xshift=2*\basenodesep, yshift=2*\basenodesep] {$1$};
  \node[xshift=5*\basenodesep, yshift=2*\basenodesep] {$k$};

  \node[yshift=-4*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=7*\basenodesep, yshift=-4*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture} 
```

:::: {.notes}

Show multi-valued (vector) output and hidden layer

::::


## Simplified illustration

```{r  tikz-rnn-recap-perceptron-multiout-simple, cache=FALSE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=-270},
    nnconnection/.append style={->},
  }
  \begin{scope}[rotate=270, transform shape]
    \pic {rnnio={}{$X$}{$Y$}};
  \end{scope}
  \node[yshift=-2*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=3*\basenodesep, yshift=-2*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture} 
```

:::: {.notes}

Condense hidden layers to a box.

::::


# Sequential models


## Motivation

```{r  tikz-rnn-motivation-time-series, cache=TRUE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=white] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


## Motivation

```{r  tikz-rnn-motivation-time-series-1, cache=TRUE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


## Motivation

```{r  tikz-rnn-motivation-time-series-2, cache=TRUE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


## Motivation

```{r  tikz-rnn-motivation-time-series-3, cache=FALSE, fig.ext="svg", fig.width=8, engine='tikz' }
\begin{tikzpicture}[>=latex]
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\draw[->, thick, black!50, dotted] (x2) to[out=90, in=180] (x1) to[out=0, in=90] (x0);
\end{tikzpicture} 
```

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights

-   dependency on previous time point
-   (weaker) dependency on more distant time points

:::


## A few concrete examples of real-world sequences

<h5>Language translation</h5>

```{r  rnn-example-language-translation, fig.ext="svg", fig.width=6, engine='tikz' }
\begin{tikzpicture}[node distance=2cm, >=latex]
  \node[align=left, font=\ttfamily, text width=22pt, rectangle, draw=black, thick] (vec) {+0.5 +0.2 -0.1 -0.3 +0.4 +1.2};
  \node[left of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=blue!20, draw=blue!80, anchor=east, rounded corners, thick, label={center:Encoder}] (encoder) {};
  \node[left of=encoder, anchor=east] (swedish) {jag är en student};
  \node[right of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=violet!20, draw=violet!80, anchor=west, rounded corners, thick, label={center:Decoder}] (decoder) {};
  \node[right of=decoder, anchor=west] (english) {I am a student};
  \draw[->] (swedish) -- (encoder);
  \draw[->] (encoder) -- (vec);
  \draw[->] (vec) -- (decoder);
  \draw[->] (decoder) -- (english);
\end{tikzpicture} 
```

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2px;"}

:::: {}

<h5>Time series</h5>

```{r  sequential-model-time-series-example, out.width="500px" }
knitr::include_graphics("https://github.com/unit8co/darts/raw/master/static/images/example.png") 
```

[@herzen2021darts]

::::

:::: {}

<h5>Genomics</h5>

```{r  sequential-model-genomics-example, out.width="300px" }
knitr::include_graphics("https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-33321-1/MediaObjects/41598_2018_33321_Fig5_HTML.png?as=webp") 
```

[@shen_recurrent_2018]

::::

:::::::::::::::::::


## Types of models

::::::::::::::::::: {style="display: grid; grid-template-columns: 10fr 1fr 10fr 10fr 10fr; grid-gap: 0px 0px; grid-template-rows: 1fr 1fr;" }

:::: {}

<h5>one to one</h5>

```{r  sequential-models-one-to-one, fig.ext="svg", fig.width=1, cache=FALSE, engine='tikz' }
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic{rnnio={}{}{}};
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {.element: class="fragment" data-fragment-index="2"}

<h5>many to one</h5>

```{r  sequential-models-many-to-one, fig.ext="svg", fig.width=3, cache=FALSE, engine='tikz' }
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnioin};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioin};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {.element: class="fragment" data-fragment-index="3"}

<h5>one to many</h5>

```{r  sequential-models-one-to-many, fig.ext="svg", fig.width=3, cache=FALSE, engine='tikz' }
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioout};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnioout};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {.element: class="fragment"  data-fragment-index="4"}

<h5>many to many</h5>

```{r  sequential-models-many-to-many, fig.ext="svg", fig.width=3, cache=FALSE, engine='tikz' }
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnio={}{}{}};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture} 
```

::::

:::: {}

<h6>Image classification</h6>

```{r  fashion-mnist-image-classification, cache=TRUE, engine='python' }
from tensorflow import keras
import matplotlib.pyplot as plt
fashion_mnist = keras.datasets.fashion_mnist
img = fashion_mnist.load_data()[0][0:2][0][0:2]
label = fashion_mnist.load_data()[0][0:2][1][0:2]
class_names = [ "T-shirt/top" , "Trouser" , "Pullover" , "Dress" , "Coat" , "Sandal" , "Shirt" , "Sneaker" , "Bag" , "Ankle boot" ]
plt.figure(figsize=(10,5))
plt.rc('axes', labelsize=40)
for i in range(2):
    plt.subplot(1,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(img[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[label[i]])
plt.show() 
```

::::

:::: {}

::::

:::: {.element: class="fragment" data-fragment-index="2"}

<h6>Sentiment analysis</h6>

```{r  sequential-model-sentiment-analysis, out.width="300px" }
knitr::include_graphics("https://d1sjtleuqoc1be.cloudfront.net/wp-content/uploads/2019/04/25112909/shutterstock_1073953772.jpg") 
```

::::

:::: {.element: class="fragment" data-fragment-index="3"}

<h6>Image captioning</h6>

```{r  sequential-model-image-captioning, out.width="350px" }
knitr::include_graphics("https://cocodataset.org/images/captions-splash.jpg") 
```

::::

:::: {.element: class="fragment" data-fragment-index="4"}

<h6>Machine translation</h6>

```{r  sequential-model-google-translate, out.width="200px" }
knitr::include_graphics("https://img.icons8.com/plasticine/344/google-translate-new-logo.png") 
```

::::

:::::::::::::::::::

[@karpathy_unreasonable_effectiveness_of_RNNs]

:::: {.notes}

Important point here: each input/output/hidden are **vectors**

(Karpathy, 2015)

Issues with Vanilla NNs and CNNs:

-   dependency on **fixed** size input
-   fixed amount of computational steps

Models:

-   **one to one:** Vanilla processing without RNN, from fixed input to
    fixed output e.g. image classification (aka vanilla neural network)
-   **one to many:** sequence output, e.g. image captioning
-   **many to one:** sequence input, e.g. sentiment analysis (classify
    sequence as happy/sad/&#x2026;)
-   **many to many:** sequence input and sequence output, e.g. machine
    translation

Data:

-   (Xiao et al., 2017)

-   <https://cocodataset.org/#captions-2015>

::::


# Recurrent Neural Networks (RNNs)

<br/>

```{r  tikz-rnn-folded-only, fig.ext="svg", cache=FALSE, fig.width=4, engine='tikz' }
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={A}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

:::: {.notes}

We will now look at the essentials of RNNs. As the figure implies, the
output of the network

(NO_ITEM_DATA:alexander_amini_mit_2021) point out inputs $x_i$ represent
**one** time point

input, output, green box: contains vectors of data, arrows represent
operations/functions
(Karpathy, 2015)

Key feature: the recurrence (green) can be applied as many times as we
want, i.e. no constraint on input size

Why recurrent networks?
<https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn>

FFNs

-   Cannot handle sequential data
-   Considers only the current input
-   Cannot memorize previous inputs

and information only flows forward (i.e. no memory)

::::


## Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-1, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {.element: class="fragment" data-fragment-index="2" style="border-left: 2px black solid;"}

```{r  ffn-x0-xt-1, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \end{scope}
  \end{tikzpicture} 
```

::::

:::: {}

::::

:::: {.element class="fragment" data-fragment-index="1"}

Assume multiple time points.

::::

:::::::::::::::::::

:::: {.notes}

Rotated FFN: take a moment to recap the ffn. Input $X_t \in
\mathbb{R}^{m}$ is mapped to output $\widehat{Y}_t \in \mathbb{R}^n$ via
the network ($f(\cdot)$

Now assume we have several time steps, starting at e.g. time 0. Also we predict the outputs individually.

::::


## Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-2, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid;"}

```{r  ffn-x0-xt-2, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

::::

:::::::::::::::::::

:::: {.notes}

Add another time step&#x2026;

::::


## Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-3, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-3, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

> - Dependency of inputs not modelled such that ambiguous sequences
  cannot be be distinguished:

:::: fragment

"the boat is in the water" vs "the water is in the boat"

::::

::::

:::::::::::::::::::

:::: {.notes}

Use an ambiguous example to point out that ffns can't distinguish
order of words; we explicitly want to model sequential dependencies

Example: "the boat is in the water" vs "the water is in the boat"

Alt example: "man bites dog" vs "dog bites man" (, Zhang et al., 2021, p. 8.1)

Emphasize fact that any prediction is based only on the current input

::::


## Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-4, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-4, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=both] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

-   Time points are modelled **individually** ( $\hat{Y}_t = f(X_t)$ )

::::

:::::::::::::::::::

:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::


## Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-5, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-5, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

Assume multiple time points.

-   Time points are modelled **individually** ( $\hat{Y}_t = f(X_t)$ )
-   Also want dependency on **previous** inputs ( $\hat{Y}_t = f(..., X_2, X_1)$ )

::::

:::::::::::::::::::

:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::


## Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-arr-1, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-arr-1, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::

:::: {.notes}

We want to model dependencies over time. Solution is to model the cell
state (a hidden state) and pass this information on to the next 

::::


## Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-arr-2, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-arr-2, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::


## Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-arr-3, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px black solid; "}

```{r  ffn-x0-xt-arr-3, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::


## Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

```{r  ffn-xt-arr-4, fig.ext="svg", out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic[\rnntikzbasekey/.cd, add labels=true, folded=true] {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture} 
```

::::

:::: {style="border-left: 2px white solid; "}

```{r  ffn-x0-xt-arr-4, fig.ext="svg", out.height="300px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \end{scope}

  \node[font=\Huge, left of=x0_left, node distance=0.7*\ionodesize] {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture} 
```

::::

:::: {}

Folded representation

::::

:::: {}

Unfolded representation

:::::: fragment

Add a *hidden state* $h$ that introduces a dependency on the previous
step:

$$
\hat{Y}_t = f(X_t, h_{t-1})
$$

::::::

::::

:::::::::::::::::::

:::: {.notes}

$h_t$ is a summary of the inputs we've seen sofar

(Zhang et al., 2021, Chapter 8.4):

> If we want to incorporate the possible effect of words earlier than
> time step t−(n−1) on xt, we need to increase n. However, the number of
> model parameters would also increase exponentially with it, as we need
> to store |V|n numbers for a vocabulary set V. Hence, rather than
> modeling P(xt∣xt−1,…,xt−n+1) it is preferable to use a latent variable
> model:
> 
> P(xt∣xt−1,&#x2026;,x1) ~ P(xt∣ht−1),

IOW, with ht the recurrence becomes a latent variable model.

::::


## Sequential memory of RNNs

RNNs have what one could call "sequential memory" [@phi_illustrated_2020_RNN]


### Alphabet

Exercise: say alphabet in your head

```
    A B C ... X Y Z


```

:::: {.element: class="fragment"}

Modification: start from e.g. letter F

May take time to get started, but from there on it's easy

::::

:::: {.element: class="fragment"}

Now read the alphabet in reverse:

```
    Z Y X ... C B A


```

::::

:::: {.element: class="fragment"}

Memory access is associative and context-dependent

::::

:::: {.notes}

Provide the alphabet example from [@phi_illustrated_2020_RNN]

cf (, Haykin, 2010, p. 203):

> For a neural network to be dynamic, it must be given *short-term
> memory* in one form or the other

::::


## Recurrent Neural Networks

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-rnn-folded-hidden-eq-1, fig.ext="svg", cache=FALSE, out.width="400px", engine='tikz' }
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {.element: class="fragment"}

Add recurrence relation where current hidden cell state $h_t$ depends
on input $x_t$ and previous hidden state $h_{t-1}$ via a function
$f_W$ that defines the network parameters (weights):

$$
h_t = f_\mathbf{W}(x_t, h_{t-1})
$$

::::

:::: {.element: class="fragment"}

Note that the same function and weights are used across all time
steps!

::::

:::: 

:::::::::::::::::::


## Recurrent Neural Networks - pseudocode

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-rnn-folded-hidden-eq-2, fig.ext="svg", cache=FALSE, out.width="400px", engine='tikz' }
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {style="font-size: 0.8em"}

```{r  rnn-simple-pseudocode, echo=TRUE, eval=FALSE, engine='python' }
class RNN:
    def __init__(self):
        # Initialize weights and cell state
        self._h = [...]
        self._Whh = [...]
        self._Wxh = [...]
        self._Why = [...]

    def update_cell_state(self, x):
        # function is some function that updates cell state
        self._h = function(self._h * self._Whh + x * self.Wxh)
    
    def predict(self):
        return self._h * self._Why

    def update_weights(self, y):
        # Calculate error via some loss function
        error = loss(self.predict() - y)
        # update weights via back propagation...

rnn = RNN()

for x, y in input_data:
    rnn.update_cell_state(x)
    rnn.update_weights(y)

# Retrieve next prediction
yhat = rnn.predict() 
```

::::

:::: 

:::::::::::::::::::


## Vanilla RNNs

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-vanilla-rnn-folded-hidden-eq-1, fig.ext="svg", cache=FALSE, out.width="400px", engine='tikz' }
\begin{tikzpicture}[thick]
  \pic[\rnntikzbasekey/.cd, folded=true, add labels=true] {rnnio={tanh}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: {.element: class="fragment" data-fragment-index="3"}

<h3 style="color: red;">Output vector</h3>

$$
\hat{Y}_t = \mathbf{W_{hy}^T}h_t
$$

::::

:::: {.element: class="fragment" data-fragment-index="2"}

<h3 style="color: green;">Update hidden state</h3>

$$
h_t = \mathsf{tanh}(\mathbf{W_{xh}^T}X_t + \mathbf{W_{hh}^T}h_{t-1})
$$

::::

:::: {.element: class="fragment" data-fragment-index="1"}

<h3 style="color: blue;">Input vector</h3>

$$
X_t
$$

::::

:::: 

::::::::::::::::::: 


## Vanilla RNNs

<br/>

[@olah_christopher_understanding_nodate]

```{r  tikz-vanilla-rnn-unfolded-weights-1, fig.ext="svg", cache=FALSE, out.width="1200px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3.1*\xd, yshift=0.6*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


## Vanilla RNNs

<br/>

```{r  tikz-vanilla-rnn-unfolded-weights-2, fig.ext="svg", cache=FALSE, out.width="1200px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


## Vanilla RNNs

<br/>

```{r  tikz-vanilla-rnn-unfolded-weights-3, fig.ext="svg", cache=FALSE, out.width="1200px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
\end{tikzpicture} 
```

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units
(Lendave, 2021)

Add pseudocode to exemplify

::::


## Vanilla RNNs

<br/>

```{r  tikz-vanilla-rnn-unfolded-weights-4, fig.ext="svg", cache=FALSE, out.width="1200px", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=north west] at (r_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rl_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rr_yt.south) {$\mathbf{W_{hy}}$};
\end{tikzpicture} 
```

:::: {.element: class="fragment"}

Note: $\mathbf{W_{xh}}$, $\mathbf{W_{hh}}$, and $\mathbf{W_{hy}}$ are
shared across all cells!

::::

:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

$$
h_t = f_W(x_t, h_{t-1})
$$

and point out that f, W are **shared** across all units

Add pseudocode to exemplify

::::


## Desired features of RNN

<div class="fragment">
<h3>1. Variable sequence lengths</h3>

Not all inputs are of equal length
<br/>
</div>

<div class="fragment">
<h3>2. Long-term memory</h3>

"I grew up in England, and &#x2026; I speak fluent English"
<br/>
</div>

<div class="fragment">
<h3>3. Preserve order</h3>
"dog bites man" != "man bites dog"
<br/>
</div>

<div class="fragment">
<h3>4. Share parameters</h3>

Adresses points 2 and 3.
<br/>
</div>

:::: {.notes}

-   variable sequence lengths

From (Cho et al., 2014):

> architectur that learns to *encode* a variable-length sequence into a
> fixed-length vector representation and to *decode* a given
> fixed-length representation back into a variable-length sequence

::::


## Example: Box & Jenkins airline passenger data set

```{r  box-jenkins-airline-1, echo=FALSE, fig.ext="png", out.width="500px", engine='python' }
df = rnnutils.airlines()
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
rnnutils.plt.show() 
```

[@onnen_temporal_2021]

:::: {.notes}

(Onnen, 2021)

See also
<https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/>
for rnn example on sunspots

Important: need to explicitly show how data is partitioned as this can
be difficult to understand

<https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/>

Herzen article on darts:
<https://medium.com/unit8-machine-learning-publication/training-forecasting-models-on-multiple-time-series-with-darts-dc4be70b1844>
::::


## Example: generate test and training data

```{r  box-jenkins-airline-2, echo=FALSE, fig.ext="png", out.width="500px", engine='python' }
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
p = ax.plot(df.time[100:144], df.passengers[100:144], color="red")
p = ax.legend(["train", "test"])
rnnutils.plt.show() 
```

Partition time series into training and test data sets at an e.g. 2:1
ratio:

```{r  box-jenkins-airline-partition-data, echo=TRUE, eval=TRUE, engine='python' }
import rnnutils
import numpy as np
df = rnnutils.airlines()
data = np.array(df['passengers'].values.astype('float32')).reshape(-1, 1)
train, test, scaler = rnnutils.make_train_test(data) 
```


## Example: prepare data for keras

```{r  tikz-prepare-airline-data-for-keras, fig.ext="svg", cache=FALSE, out.width="800px", engine='tikz' }
\begin{tikzpicture}[node distance=2cm, align=center, >=latex]
  \node (data) {data = [0, 10, 20, 30, 40, 50, 60, 70]};
  \node[below left of=data, text width=1cm, rectangle, draw=black, node distance=4cm] (t) {t=0,1 t=2,3 t=4,5};
  \node[right of=t, text width=1cm, rectangle, draw=black] (x) {0, 10  20, 30 40, 50};
  \node[right of=x, text width=0.8cm, rectangle, draw=black] (t2) {t=2 t=4 t=6};
  \node[right of=t2, text width=0.5cm, rectangle, draw=black] (y) {20 40 60};
  \node[above of=x, node distance=0.9cm] (xlab) {X};
  \node[above of=y, node distance=0.9cm] (ylab) {Y};
  \draw (xlab.north) edge["predict Y from X by row", ->, bend left] (ylab.north);
\end{tikzpicture} 
```

:::: {.element: class="fragment"}

```{r  airline-example-makexy, echo=TRUE, eval=TRUE, engine='python' }
time_steps = 12
trainX, trainY, trainX_indices, trainY_indices = rnnutils.make_xy(train, time_steps)
testX, testY, testX_indices, testY_indices = rnnutils.make_xy(test, time_steps) 
```

::::


## Example: create vanilla RNN model

```{r  airline-rnn-model, echo=TRUE, eval=TRUE, engine='python' }
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN

model = Sequential()
model.add(SimpleRNN(units=3, input_shape=(time_steps, 1),
                    activation="tanh"))
model.add(Dense(units=1, activation="tanh"))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary() 
```


## Example: fit the model and evaluate

:::: {style="font-size: 0.8em"}

```{r  airline-model-fit, echo=TRUE, eval=FALSE, engine='python' }
history = model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)
Ytrainpred = model.predict(trainX)
Ytestpred = model.predict(testX) 
```

::::

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% 50%; grid-column-gap: 10px; grid-row-gap: 0px"}

:::: {style="font-size: 0.8em"}

```{r  airline-plot-history-command, echo=TRUE, eval=FALSE, engine='python' }
rnnutils.plot_history(history) 
```

:::: 

:::: {style="font-size: 0.8em"}

```{r  airline-plot-model-fit-command, echo=TRUE, eval=FALSE, engine='python' }
data = {'train': (model.predict(trainX), train, trainY_indices),
        'test': (model.predict(testX), test, testY_indices)}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20), labels=df.year[range(0, 144, 20)]) 
```

:::: 

:::: {}

```{r  airline-plot-history, echo=FALSE, eval=TRUE, fig.ext="png", out.width="80%", engine='python' }
d = {'loss': [0.15468591451644897, 0.04875592887401581,
              0.026622500270605087, 0.02037993259727955, 0.01717367395758629,
              0.01483415998518467, 0.013102399185299873, 0.011407987214624882,
              0.01016605831682682, 0.009000968188047409, 0.008080212399363518,
              0.007306812796741724, 0.006582655943930149, 0.005831228103488684,
              0.0052399300038814545, 0.004740677773952484, 0.004358494654297829,
              0.003976745996624231, 0.0037244476843625307, 0.003332830499857664]}
rnnutils.plot_history(d) 
```

::::

:::: {}

```{r  airline-plot-model-fit, echo=FALSE, eval=TRUE, fig.ext="png", out.width="80%", engine='python' }
data = {"train": [np.array([[0.07300922274589539], [0.049322694540023804], [0.12263555824756622], [0.10771659016609192], [0.08678102493286133], [0.0807582437992096], [0.16047000885009766], [0.13826081156730652], [0.12276159226894379], [0.09296061843633652], [0.005158169195055962], [0.07526839524507523], [0.058306947350502014], [0.050476402044296265], [0.1301475316286087], [0.11820383369922638], [0.05991018936038017], [0.08855298906564713], [0.18303704261779785], [0.2147040069103241], [0.12630178034305573], [0.18395473062992096], [-0.05559101700782776], [0.12674248218536377], [0.13126012682914734], [0.07171593606472015], [0.2608124911785126], [0.1329176276922226], [0.22195808589458466], [0.16103020310401917], [0.2596116065979004], [0.2631970942020416], [0.222457155585289], [0.19196732342243195], [0.08305624127388], [0.13445650041103363], [0.23295335471630096], [0.12711040675640106], [0.2997269332408905], [0.19363194704055786], [0.16322757303714752], [0.29337525367736816], [0.2702861726284027], [0.3810942769050598], [0.24219587445259094], [0.29155629873275757], [0.1209971085190773], [0.22091759741306305], [0.31695908308029175], [0.14803078770637512], [0.37239915132522583], [0.33496564626693726], [0.3264579474925995], [0.3433589041233063], [0.32992351055145264], [0.41659021377563477], [0.2793201804161072], [0.3563149571418762], [0.11158497631549835], [0.229257732629776], [0.33076655864715576], [0.10553482174873352], [0.3575865626335144], [0.30193302035331726], [0.3180869221687317], [0.33221328258514404], [0.4005788266658783], [0.406070739030838], [0.3288029730319977], [0.34573444724082947], [0.18024373054504395], [0.2741532623767853], [0.3648494482040405], [0.25930526852607727], [0.37611520290374756], [0.36360082030296326], [0.3635270297527313], [0.3676840662956238], [0.43250808119773865], [0.41396385431289673], [0.37228304147720337], [0.38717105984687805], [0.23085232079029083], [0.3464904725551605]]), np.array([0.015444010496139526, 0.027027025818824768, 0.054054051637649536, 0.04826255142688751, 0.03281852602958679, 0.059845566749572754, 0.08494207262992859, 0.08494207262992859, 0.06177607178688049, 0.028957530856132507, 0.0, 0.027027025818824768, 0.021235525608062744, 0.042471036314964294, 0.0714285671710968, 0.059845566749572754, 0.04054054617881775, 0.08687257766723633, 0.12741312384605408, 0.12741312384605408, 0.1042470932006836, 0.055984556674957275, 0.019305020570755005, 0.06949806213378906, 0.07915058732032776, 0.08880308270454407, 0.1428571343421936, 0.11389961838722229, 0.13127413392066956, 0.1428571343421936, 0.18339768052101135, 0.18339768052101135, 0.15444016456604004, 0.11196911334991455, 0.0810810923576355, 0.11969110369682312, 0.12934362888336182, 0.14671814441680908, 0.1718146800994873, 0.14864864945411682, 0.1525096595287323, 0.22007721662521362, 0.2432432472705841, 0.2664092481136322, 0.20270270109176636, 0.16795367002487183, 0.13127413392066956, 0.17374518513679504, 0.17760616540908813, 0.17760616540908813, 0.25482624769210815, 0.2528957426548004, 0.24131274223327637, 0.26833975315093994, 0.3088802993297577, 0.3243243396282196, 0.2567567527294159, 0.20656371116638184, 0.14671814441680908, 0.18725869059562683, 0.19305017590522766, 0.1621621549129486, 0.2528957426548004, 0.2374517321586609, 0.2509652376174927, 0.3088802993297577, 0.38223937153816223, 0.36486485600471497, 0.2992278039455414, 0.24131274223327637, 0.1911197006702423, 0.24131274223327637, 0.2664092481136322, 0.24903473258018494, 0.3146717846393585, 0.318532794713974, 0.3204633295536041, 0.40733590722084045, 0.5019304752349854, 0.46911194920539856, 0.4015444219112396, 0.3281853497028351, 0.2567567527294159, 0.33590731024742126, 0.3474903404712677, 0.3339768350124359, 0.41119691729545593, 0.403474897146225, 0.4131273925304413, 0.521235466003418, 0.5965250730514526, 0.5810810327529907, 0.4845559895038605, 0.3899613916873932, 0.3223938047885895, 0.3899613916873932]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])], "test": [np.array([[0.41985487937927246], [0.2897593379020691], [0.43206876516342163], [0.3984266519546509], [0.39088255167007446], [0.43195009231567383], [0.4271005094051361], [0.42042967677116394], [0.37909582257270813], [0.3912075459957123], [0.33704182505607605], [0.35475632548332214], [0.43653494119644165], [0.28377869725227356], [0.4360058009624481], [0.35397568345069885], [0.3680872619152069], [0.4023270905017853], [0.3970481753349304], [0.40373513102531433], [0.3252890706062317], [0.37355899810791016], [0.2793232798576355], [0.31504255533218384], [0.38788625597953796], [0.25943249464035034], [0.414558470249176], [0.3525553047657013], [0.3922974467277527], [0.3646024167537689], [0.3950953483581543], [0.34701961278915405], [0.3233894407749176], [0.32918328046798706], [0.2957551181316376], [0.3380872905254364]]), np.array([0.40733590722084045, 0.3803088963031769, 0.4864864647388458, 0.4710424840450287, 0.4845559895038605, 0.6138995885848999, 0.6969112157821655, 0.7007721662521362, 0.5791505575180054, 0.46911194920539856, 0.38803085684776306, 0.4478764235973358, 0.4555984437465668, 0.4131273925304413, 0.49806949496269226, 0.4710424840450287, 0.4999999701976776, 0.6389961242675781, 0.747104287147522, 0.7741312980651855, 0.5791505575180054, 0.49227800965309143, 0.39768341183662415, 0.44980695843696594, 0.4942084848880768, 0.45945945382118225, 0.5830116271972656, 0.5637065172195435, 0.6100386381149292, 0.7104246616363525, 0.8571429252624512, 0.8783783912658691, 0.6930501461029053, 0.584942102432251, 0.49806949496269226, 0.5810810327529907, 0.6042470932006836, 0.5540540218353271, 0.6081080436706543, 0.6891891956329346, 0.7104246616363525, 0.832046389579773, 1.0, 0.9691120386123657, 0.7799227237701416, 0.6891891956329346, 0.5521235466003418, 0.6332045793533325]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])]}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20), labels=df.year[range(0, 144, 20)]) 
```

::::

::::::::::::::::::: 


## Exercise

See if you can improve the airline passenger model. Some things to
try:

-   change the number of units
-   change time\_steps
-   change the number of epochs


# Training


## Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-ffn-1, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
\end{tikzpicture} 
```

:::: 

:::: {}

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


## Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-ffn-2, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[->, thick] (start) -- (end);
\end{tikzpicture} 
```

:::: 

:::: {}
<br/>

1.  perform forward pass and generate prediction

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


## Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-ffn-3, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
\end{tikzpicture} 
```

:::: 

:::: {}
<br/>

1.  perform forward pass and generate prediction
2.  calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
       \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


## Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-ffn-4, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
  \node[below of=end] (bptt_start) {};
  \node[below of=start] (bptt_end) {};
  \draw[thick, ->, color=red] (bptt_start) -- (bptt_end);
\end{tikzpicture} 
```

:::: 

:::: {}
<br/>

1.  perform forward pass and generate prediction
2.  calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
       \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$
3.  back propagate errors and update weights to minimize loss

:::: 

::::::::::::::::::: 

:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.

::::


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-1, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture} 
```


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-2, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
\end{tikzpicture} 
```


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-3, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
  \end{scope}
\end{tikzpicture} 
```


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-5, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\end{tikzpicture} 
```


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-6, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
\end{scope}  

\end{tikzpicture} 
```


## Backpropagation through time (BPTT)

[@alexander_amini_mit_2021_rnn]

```{r  tikz-backpropagation-unfolded-7, fig.ext="svg", out.height="500px", out.width="100%", engine='tikz' }
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
  \node[ultra thick, node distance=.7*\basenodesep, below right of=xt_input]  (gradstart) {};
  \node[ultra thick, node distance=.7*\basenodesep, below left of=x0_input]  (gradend) {};
  \draw (gradstart) edge [->] (gradend);
\end{scope}  

\end{tikzpicture} 
```

Errors are propagated backwards in time from $t=t$ to $t=0$.

:::: {.element: class="fragment"}

Problem: calculating gradient may depend on large powers of
$\mathbf{W_{hh}}^{\mathsf{T}}$ (e.g. $\delta\mathcal{L} / \delta h_0
\sim f((\mathbf{W_{hh}}^{\mathsf{T}})^t)$

::::

:::: {.notes}

Wording: gradient $(dL/dh_0) ~ f((W_hh^T)^t)$, i.e. gradient may depend on
large powers of $W^T_hh$. So gradient is $\propto a^t$, so if

(a > 1: exploding gradients; just mention here)

a < 1: vanishing gradients

This is problematic since the **size** of weight adjustments depend on
size of gradient

::::


## The effect of vanishing gradients on long-term memory

::::::::::::::::::: {style="display: grid; grid-template-columns: 30% auto; grid-column-gap: 0px; font-size: 0.8em;"}

:::: {}

:::: fragment

In layer $i$ gradient size ~ $(\mathbf{W_{hh}}^{\mathsf{T}})^{t-i}$

::::

:::: fragment

$\downarrow$

Weight adjustments depend on size of gradient

::::

:::: fragment

$\downarrow$

Early layers tend to "see" small gradients and do very little updating

::::

:::: fragment

$\downarrow$

Bias parameters to learn recent events 

::::

:::: fragment

$\downarrow$

RNN suffer short term memory

::::

:::: 

:::: {}

:::: {.element: class="fragment"}

[@olah_christopher_understanding_nodate]

"The clouds are in the <span class="underline">\_</span>"

::::

:::: {.element: class="fragment"}

```{r  tikz-clouds-are-in-the-sky, fig.ext="svg", out.height="200px", engine='tikz' }
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3*\rnnioxshiftsmall] (x3) {rnnio={}{$X_3$}{$\widehat{Y}_3$}};
  \pic[xshift=4*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (x4) {rnnio={}{$X_4$}{$\widehat{Y}_4$}};
  \pic[xshift=5*\rnnioxshiftsmall] (x5) {rnnio={}{$X_5$}{$\widehat{Y}_5$}};
  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (x3_left)
  (x3_right) edge (x4_left)
  (x4_right) edge (x5_left);
\end{tikzpicture} 
```

<br/>

::::

:::: {.element: class="fragment"}

"I grew up in England &#x2026; I speak fluent <span class="underline">\_</span>"

::::

:::: {.element: class="fragment"}

```{r  tikz-I-speak-fluent-english, fig.ext="svg", out.height="200px", engine='tikz' }
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  
  \pic[xshift=4.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt1) {rnnio={}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=5.5*\rnnioxshiftsmall] (xt2) {rnnio={}{$X_{t+2}$}{$\widehat{Y}_{t+2}$}};

  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (xt_left)
  (xt_right) edge (xt1_left)
  (xt1_right) edge (xt2_left);

\end{tikzpicture} 
```

::::

:::: 

::::::::::::::::::: 

:::: {.notes}

(Thomas, 2018)

The bigger the gradient, the bigger the adjustment and **vice versa**.
Gradients are calculated wrt to effects of gradients in previous
layer. If those adjustments were small, gradients will be small, which
in time leads to exponentially declining values. Early layers fail to
do any learning.

In flowchart: **given** that W is smaller than one, the gradients tend
to vanish and be negligible for early layers

Examples: highlight the context dependency of the prediction. Sky is
easy to infer, but in the second example, if the intervening paragraph
is long, we need context from much farther back.

::::


## Solutions to vanishing gradient

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-template-rows: auto auto auto; grid-row-gap: 0px; grid-column-gap: 0px;"}

:::: {.element: class="fragment" data-fragment-index="1"}

1.  Activation function
    
    ReLU (or leaky ReLU) instead of sigmoid or tanh

::::

:::: {.element: class="fragment" data-fragment-index="1"}

```{r  tikz-vanishing-gradient-trick-1, fig.ext='svg', out.height="200px", engine='tikz' }
\begin{tikzpicture}
  \tikzset{declare function={
      sigma(\x)=1/(1+exp(-\x));
      sigmap(\x)=sigma(\x)*(1-sigma(\x));
      relu(\x)=x;
      tanhp(\x)=1-(exp(\x) - exp(-\x))^2/(exp(\x)+exp(-\x))^2;
    }
  }
  \pgfplotsset{every axis plot/.append style={thick}}
\begin{axis}%
  [
  grid=major,     
  xmin=-4,
  xmax=4,
  axis x line=bottom,
  ymax=1.1,
  ymin=-0.1,
  axis y line=middle,
  samples=100,
  domain=-4:4,
  legend style={at={(1,0.9)}},
  ytick=\empty
  ]
  \addplot[blue,mark=none]   (x,{sigmap(x)});
  \addplot[green, mark=none] (x, {tanhp(x)});
  \addplot[red,mark=none,domain=0:4]   (x,1);
  \addplot[red,mark=none,domain=-4:0]   (x, 0);
  \addplot +[red,mark=none] coordinates {(0, 0) (0, 1)};

  \legend{$\sigma'(x)$, $\mathsf{tanh}'(x)$, $\mathsf{ReLU}'$}
\end{axis}
\end{tikzpicture} 
```

:::: 

:::: {.element: class="fragment" data-fragment-index="2"}

2.  Weight initialization
    
    Set bias=0, weights to identity matrix

::::

::::  {.element: class="fragment" data-fragment-index="2" }

```{r  tikz-vanishing-gradient-trick-2, fig.ext='svg', out.height="200px", engine='tikz' }
\begin{tikzpicture}
  \matrix [matrix of math nodes,left delimiter=(,right delimiter=), anchor=west](W){ 
    1 & 0 & \dots  & 0\\
    0 & 0 & \dots  & 0\\
    \vdots  & \vdots  & \ddots & \vdots\\
    0 & 0 & \dots  & 1\\
  };
  \node [left of=W, anchor=east, node distance=1.5cm] {W =};

\end{tikzpicture} 
```

:::: 

:::: {.element: class="fragment" data-fragment-index="3"}

3.  More complex cells using "gating"
    
    For example LSTM

::::

::::  {.element: class="fragment" data-fragment-index="3"}

```{r  tikz-vanishing-gradient-trick-3, fig.ext='svg', out.height="200px", engine='tikz' }
\begin{tikzpicture}
  \pic {lstm};  
\end{tikzpicture} 
```

:::: 

::::::::::::::::::: 

:::: {.notes}

Note that ReLUs not used in LSTMs / GRU as ReLU is non-negative. The
tanh activation is needed so that values can be added **and**
subtracted. Sigmoid is in (0, 1).

::::


# LSTMs and GRUs


## Motivation behind LSTMs and GRUs

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

<h5 align="center">LSTM</h5>

```{r  tikz-lstm, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture} 
```

::::

:::: {}

<h5 align="center">GRU</h5>

```{r  tikz-gru, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {gru};
\end{tikzpicture} 
```

::::

:::::::::::::::::::

```{r  tikz-gru-lstm-legend, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\tikzset{legend/.style={
        font=\sffamily\bfseries\tiny,
        text width=1.4cm,
        align=center
        }
}
\begin{tikzpicture}[node distance=1cm]
\node[pwise=X, node distance=2cm, label={[legend]below:pointwise multiplication}] (pmult) {};
\node[tanh, left of=pmult, label={[legend]below:tanh}] (tanh) {};
\node[sigmoid, left of=tanh, label={[legend]below:sigmoid}] (sigmoid) {};
\node[pwise=+, right of=pmult, label={[legend]below:pointwise addition}] (padd) {};
\node[pwise={1-}, right of=padd, label={[legend]below:pointwise inversion}] (pinvert) {};
\node[vcon=1cm, right of=pinvert, label={[legend]below:vector concatenation}] (vconcat) {};
\node[vcopy=1cm, right of=vconcat, label={[legend]below:vector copy}] (vcopy) {};
\end{tikzpicture} 
```

Long Short Term Memory (LSTM)  (Hochreiter &#38; Schmidhuber, 1997) and Gated
Recurrent Unit (GRU) (Cho et al., 2014) architectures were
proposed to solve the vanishing gradient problem.

::: {.notes}

Based on (Phi, 2020)

-   solution to short-term memory
-   gates **regulate** the flow of information, concentrating on the important parts

:::


## Intuition

:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;"}

In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN).
One RNN encodes a sequence of symbols into a fixed-length vector
representation, and the other decodes the representation into another
sequence of symbols. The encoder and decoder of the proposed model are
jointly trained to maximize the conditional probability of a target
sequence given a source sequence. The performance of a statistical
machine translation system is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
semantically and syntactically meaningful representation of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  (Cho et al., 2014)

::::

:::: {.notes}

(Phi, 2020)

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

-   solution to vanishing gradient problem
-   gates regulate flow of information, focusing on the important parts

::::


## Intuition

:::: {.highlight}

In this paper, we propose a **novel neural network** model called **RNN**
**Encoder-Decoder** that consists of **two recurrent neural networks** (RNN).
One RNN **encodes** a **sequence of symbols** into a fixed-length vector
representation, and the other **decodes the representation** into another
sequence of symbols. The encoder and decoder of the proposed model are
**jointly trained** to maximize the conditional probability of a target
sequence given a source sequence. The performance of a **statistical**
**machine translation system** is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
**semantically and syntactically meaningful representation** of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  (Cho et al., 2014)

::::

<br/>

:::: {.element: class="fragment"}

Remember the important parts, pay less attention to (forget) the rest.

::::

:::: {.notes}

(Phi, 2020)

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

-   solution to vanishing gradient problem
-   gates regulate flow of information, focusing on the important parts

::::


## LSTM: Cell state flow and gating

[@olah_christopher_understanding_nodate]

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-cellstateflow, fig.ext="svg", cache=FALSE, out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=cellstateflow] {lstm} ;
\end{tikzpicture} 
```

Information flows in the cell state from $c_{t-1}$ to $c_t$.

::::

:::: {.element: class="fragment"}

```{r  tikz-lstmgate, fig.ext="svg", cache=FALSE, out.height="300px", engine='tikz' }
\begin{tikzpicture}
  \pic {lstmgate} ;
\end{tikzpicture} 
```

Gates affect the amount of information let through. The sigmoid layer
outputs anything from 0 (nothing) to 1 (everything).

::::

:::::::::::::::::::

:::: {.element: class="fragment" style="font-size: 0.8em"}

[@cho_learning_2014]

> In our preliminary experiments, we found that it is crucial to use
> this new unit with gating units. We were not able to get meaningful
> result with an oft-used tanh unit without any gating.

::::

:::: {.notes}

(Olah, 2015)

(, Cho et al., 2014, p. 1726) on the hidden unit:

> In our preliminary experiments, we found that it is crucial to use
> this new unit with gating units. We were not able to get meaningful
> result with an oft-used tanh unit without any gating.

::::


## Forget, input, and output gates

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

<h5>forget gate</h5>

```{r  tikz-lstm-forget-gate-only, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=forgetgate] {lstm};
\end{tikzpicture} 
```

**Purpose:** reset content of cell

::::

:::: {}

<h5>input gate</h5>

```{r  tikz-lstm-input-gate-only, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=inputgate] {lstm};
\end{tikzpicture} 
```

**Purpose:** decide when to read data into cell
::::

:::: {}

<h5>output gate</h5>

```{r  tikz-lstm-output-gate-only, fig.ext="svg", engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=outputgate] {lstm};
\end{tikzpicture} 
```

**Purpose:** read entries from cell

::::

:::::::::::::::::::

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where anything from no information (0) to all information (1) passes through the gate.


## The forget gate

```{r  tikz-lstm-forget-gate, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
\pic[\rnntikzbasekey/.cd, highlight=forgetgate] {lstm};
\end{tikzpicture} 
```

**Purpose**: decide what information to keep or throw away

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where 0=forget, 1=keep.

:::: {.element: class="fragment"}

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

::::


## Add new information - the input gate

```{r  tikz-lstm-candidatecellstate-input, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=inputgate] {lstm};
\end{tikzpicture} 
```

Two steps to adding new information:

1.  sigmoid layer decides which values to update


## Add new information - get candidate values

```{r  tikz-lstm-candidatecellstate-input-1, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=candidatecellstate] {lstm};
\end{tikzpicture} 
```

Two steps to adding new information:

1.  sigmoid layer decides which values to update
2.  tanh layer creates vector of new candidate values $\tilde{c}_t$

:::: {.element: class="fragment"}

$$
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

::::


## Updating the cell state

```{r  tikz-lstm-update-cell-state, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=updatecellstate] {lstm};
\end{tikzpicture} 
```

1.  multiply old cell state by $f_t$ to forget what was decided to
    forget
2.  add $i_t * \tilde{c}_t$

:::: {.element: class="fragment"}

$$
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t
$$

::::


## Cell output

```{r  tikz-lstm-output-gate, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=celloutput] {lstm};
\end{tikzpicture} 
```

Output is filtered version of cell state.

1.  sigmoid output gate decides what parts of cell state to output
2.  push cell state through tanh and multiply by sigmoid output

:::: {.element: class="fragment"}

$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
$$

::::


## LSTM: putting it together

```{r  tikz-lstm-intuition, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture} 
```


### Intuition

-   if forget ~ 1, input ~ 0, $c_{t-1}$ will be saved over time

:::: {.notes}

From (, Zhang et al., 2021, p. 9.2.1.3):

-   if forget ~ 1, input ~ 0, C<sub>t-1</sub> will be saved over time

::::


## LSTM: putting it together

[@zhang2021dive]

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

```{r  tikz-lstm-2, cache=FALSE, fig.ext="svg", fig.width=5, engine='tikz' }
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture} 
```

::::

:::: {}

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\\
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)\\
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t\\
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
$$

::::

:::::::::::::::::::

$$
x_t \in \mathbb{R}^{n\times d}, h_{t-1} \in \mathbb{n \times h},
i_t \in \mathbb{R}^{n\times h}, f_t \in \mathbb{R}^{n\times h}, o_t \in \mathbb{R}^{n\times h},
$$

and

$$
W_f \in \mathbb{R}^{n \times (h+d)}, W_i \in \mathbb{R}^{n \times (h+d)}, W_o \in \mathbb{R}^{n \times (h+d)}, W_c \in \mathbb{R}^{n \times (h+d)}
$$


## GRU

```{r  tikz-big-gru, fig.ext="svg", out.width="80%", engine='tikz' }
\begin{tikzpicture}
\pic {gru};  
\end{tikzpicture} 
```

-   forget and input states combined to single *update* gate
-   merge cell and hidden state
-   simpler model than LSTM

:::: {.notes}

(Olah, 2015)

Notes that GRU has been gaining traction lately (where lately=2015!)

Comparison (Lendave, 2021):

-   GRU has fewer parameters so uses less memory and executes faster
-   LSTM more accurate on larger datasets

::::


## Exercise


### 1. Analyse airline passengers with LSTM

Modify the airline passenger model to use an LSTM and compare the
results. Try out different parameters to improve test predictions.


### 2. Time-series forecasting with LSTM, discrete state space

LSTM with Variable Length Input Sequences to One Character Output

:::: {.notes}

Good example at (Koehrsen, 2018)

::::


## Time-series forecasting with LSTM, discrete state space


### Objective

Predict next character in sequence of strings


### Comments

-   use several LSTM layers, where the last one should return sequences
    [@brownlee_stacked_2017]


# Bibliography

<div id="refs" class="references hanging-indent" role="doc-bibliography" style="font-size: 60%;"></div>

