@article{bourgeois_overview_2021,
	title = {An overview of current population genomics methods for the analysis of whole-genome resequencing data in eukaryotes},
	volume = {30},
	issn = {1365-294X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mec.15989},
	doi = {10.1111/mec.15989},
	abstract = {Characterizing the population history of a species and identifying loci underlying local adaptation is crucial in functional ecology, evolutionary biology, conservation and agronomy. The constant improvement of high-throughput sequencing techniques has facilitated the production of whole genome data in a wide range of species. Population genomics now provides tools to better integrate selection into a historical framework, and take into account selection when reconstructing demographic history. However, this improvement has come with a profusion of analytical tools that can confuse and discourage users. Such confusion limits the amount of information effectively retrieved from complex genomic data sets, and impairs the diffusion of the most recent analytical tools into fields such as conservation biology. It may also lead to redundancy among methods. To address these isssues, we propose an overview of more than 100 state-of-the-art methods that can deal with whole genome data. We summarize the strategies they use to infer demographic history and selection, and discuss some of their limitations. A website listing these methods is available at www.methodspopgen.com.},
	language = {en},
	number = {23},
	urldate = {2021-12-02},
	journal = {Molecular Ecology},
	author = {Bourgeois, Yann X. C. and Warren, Ben H.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mec.15989},
	keywords = {bioinformatics, demography, population genomics, selection, whole-genome sequencing},
	pages = {6036--6071},
	file = {Full Text PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/Y3IF9AHH/Bourgeois and Warren - 2021 - An overview of current population genomics methods.pdf:application/pdf;Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/CQVTTTNZ/mec.html:text/html},
}
@misc{aiml_vanishing_2018,
	title = {The vanishing gradient problem and {ReLUs} – a {TensorFlow} investigation – {Adventures} in {Machine} {Learning}},
	url = {https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/},
	language = {en-US},
	urldate = {2021-12-21},
        year = {2018},
        author = {Thomas, Andy},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/FZ9CQMQX/vanishing-gradient-problem-tensorflow.html:text/html},
}
@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-12-03},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/JBAZT7EA/neco.1997.9.8.html:text/html}
}
@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2020-12-03},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: EMNLP 2014},
	file = {arXiv Fulltext PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/E76B63SQ/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf}
}

@misc{phi_illustrated_2020_lstm,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2021-12-28},
	journal = {Medium},
	author = {Phi, Michael},
	month = jun,
	year = {2020},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/R5N9PK5P/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html},
}
@misc{olah_christopher_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2021-12-28},
        author = {Christopher Olah},
        date = {2015-08-27},
	file = {Understanding LSTM Networks -- colah's blog:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/IV2GHNSK/2015-08-Understanding-LSTMs.html:text/html},
}

@book{mackay_information_2003,
	address = {Cambridge, UK ; New York},
	edition = {Illustrated edition},
	title = {Information {Theory}, {Inference} and {Learning} {Algorithms}},
	isbn = {978-0-521-64298-9},
	language = {English},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	month = sep,
	year = {2003},
}

@misc{alexander_amini_mit_2021_rnn,
	title = {{MIT} 6.{S191}: {Recurrent} {Neural} {Networks}},
	shorttitle = {{MIT} 6.{S191}},
	url = {https://www.youtube.com/watch?v=qjrad0V0uJE},
	abstract = {MIT Introduction to Deep Learning 6.S191: Lecture 2
Recurrent Neural Networks
Lecturer: Ava Soleimany
January 2021

For all lectures, slides, and lab materials: http://introtodeeplearning.com​

Lecture Outline
0:00​ - Introduction
2:37​ - Sequence modeling
4:54​ - Neurons with recurrence
12:07​ - Recurrent neural networks
14:13​ - RNN intuition
17:01​ - Unfolding RNNs
18:39 - RNNs from scratch
22:12 - Design criteria for sequential modelling
23:37 - Word prediction example
31:31​ - Backpropagation through time
33:40​ - Gradient issues
38:46​ - Long short term memory (LSTM)
47:47​ - RNN applications
52:15​ - Attention
59:24​ - Summary

Subscribe to stay up to date with new deep learning lectures at MIT, or follow us @MITDeepLearning on Twitter and Instagram to stay fully-connected!!},
	urldate = {2021-12-31},
	author = {{Alexander Amini}},
	month = feb,
	year = {2021},
}

@misc{alexander_amini_mit_2021_intro,
	title = {{MIT} {Introduction} to {Deep} {Learning} {\textbar} 6.{S191}},
	url = {https://www.youtube.com/watch?v=5tvmMX8r_OM},
	abstract = {MIT Introduction to Deep Learning 6.S191: Lecture 1
*New 2021 Edition*
Foundations of Deep Learning
Lecturer: Alexander Amini

For all lectures, slides, and lab materials: http://introtodeeplearning.com/

Lecture Outline
0:00​ - Introduction
4:48 ​ - Course information
10:18​ - Why deep learning?
12:28​ - The perceptron
14:42​ - Activation functions
17:48​ - Perceptron example
21:43​ - From perceptrons to neural networks
27:42​ - Applying neural networks 
30:21​ - Loss functions
33:23​ - Training and gradient descent
38:05​ - Backpropagation
43:06​ - Setting the learning rate
47:17​ - Batched gradient descent
49:49​ - Regularization: dropout and early stopping
55:55​ - Summary

Subscribe to stay up to date with new deep learning lectures at MIT, or follow us on @MITDeepLearning on Twitter and Instagram to stay fully-connected!!},
	urldate = {2022-01-02},
	author = {{Alexander Amini}},
	month = feb,
	year = {2021},
}
@misc{herzen2021darts,
      title={Darts: User-Friendly Modern Machine Learning for Time Series},
      author={Julien Herzen and Francesco Lässig and Samuele Giuliano Piazzetta and Thomas Neuer and Léo Tafti and Guillaume Raille and Tomas Van Pottelbergh and Marek Pasieka and Andrzej Skrodzki and Nicolas Huguenin and Maxime Dumonal and Jan Kościsz and Dennis Bader and Frédérick Gusset and Mounir Benheddi and Camila Williamson and Michal Kosinski and Matej Petrik and Gaël Grosch},
      year={2021},
      eprint={2110.03224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{shen_recurrent_2018,
	title = {Recurrent {Neural} {Network} for {Predicting} {Transcription} {Factor} {Binding} {Sites}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-33321-1},
	doi = {10.1038/s41598-018-33321-1},
	abstract = {It is well known that DNA sequence contains a certain amount of transcription factors (TF) binding sites, and only part of them are identified through biological experiments. However, these experiments are expensive and time-consuming. To overcome these problems, some computational methods, based on k-mer features or convolutional neural networks, have been proposed to identify TF binding sites from DNA sequences. Although these methods have good performance, the context information that relates to TF binding sites is still lacking. Research indicates that standard recurrent neural networks (RNN) and its variants have better performance in time-series data compared with other models. In this study, we propose a model, named KEGRU, to identify TF binding sites by combining Bidirectional Gated Recurrent Unit (GRU) network with k-mer embedding. Firstly, DNA sequences are divided into k-mer sequences with a specified length and stride window. And then, we treat each k-mer as a word and pre-trained word representation model though word2vec algorithm. Thirdly, we construct a deep bidirectional GRU model for feature learning and classification. Experimental results have shown that our method has better performance compared with some state-of-the-art methods. Additional experiments about embedding strategy show that k-mer embedding will be helpful to enhance model performance. The robustness of KEGRU is proved by experiments with different k-mer length, stride window and embedding vector dimension.},
	language = {en},
	number = {1},
	urldate = {2022-01-02},
	journal = {Scientific Reports},
	author = {Shen, Zhen and Bao, Wenzheng and Huang, De-Shuang},
	month = oct,
	year = {2018},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Gene regulatory networks
Subject\_term\_id: computational-models;gene-regulatory-networks},
	keywords = {Computational models, Gene regulatory networks},
	pages = {15270},
	file = {Full Text PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/9HUY2A33/Shen et al. - 2018 - Recurrent Neural Network for Predicting Transcript.pdf:application/pdf;Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/J95F4H22/s41598-018-33321-1.html:text/html},
}
@misc{phi_illustrated_2020_RNN,
	title = {Illustrated {Guide} to {Recurrent} {Neural} {Networks}},
	url = {https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9},
	abstract = {Understanding the Intuition},
	language = {en},
	urldate = {2022-01-03},
	journal = {Medium},
	author = {Phi, Michael},
	month = jun,
	year = {2020},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/N2632JLK/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9.html:text/html},
}
@misc{karpathy_unreasonable_effectiveness_of_RNNs,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
        author = {Andrej Karpathy},
        year = 2015,
        date = {2015-05-21},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2022-01-03},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/M3694VS6/rnn-effectiveness.html:text/html},
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2022-01-03},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Dataset is freely available at https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
	file = {arXiv Fulltext PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/EUH44XNQ/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf;arXiv.org Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/BTAR6N9T/1708.html:text/html},
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@book{haykin_neural_2010,
	title = {Neural {Networks} and {Learning} {Machines}, 3/e},
	isbn = {978-93-325-8625-3},
	url = {https://books.google.co.in/books?id=ivK0DwAAQBAJ},
	publisher = {PHI Learning},
	author = {Haykin, S.},
	year = {2010},
}

@misc{onnen_temporal_2021,
	title = {Temporal {Loops}: {Intro} to {Recurrent} {Neural} {Networks} for {Time} {Series} {Forecasting} in {Python}},
	shorttitle = {Temporal {Loops}},
	url = {https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f},
	abstract = {A Tutorial on LSTM, GRU, and Vanilla RNNs —  Wrapped by the Darts Multi-Method Forecast Library},
	language = {en},
	urldate = {2022-01-10},
	journal = {Medium},
	author = {Onnen, Heiko},
	month = nov,
	year = {2021},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/RWELPZTW/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b039896.html:text/html},
}

@misc{koehrsen_recurrent_2018,
	title = {Recurrent {Neural} {Networks} by {Example} in {Python}},
	url = {https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470},
	abstract = {Using a Recurrent Neural Network to Write Patents},
	language = {en},
	urldate = {2022-01-13},
	journal = {Medium},
	author = {Koehrsen, Will},
	month = nov,
	year = {2018},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/99JFKSI7/recurrent-neural-networks-by-example-in-python-ffd204f99470.html:text/html},
}

@misc{brownlee_stacked_2017,
	title = {Stacked {Long} {Short}-{Term} {Memory} {Networks}},
	url = {https://machinelearningmastery.com/stacked-long-short-term-memory-networks/},
	abstract = {Gentle introduction to the Stacked LSTM with example code in Python. The original LSTM model is comprised of a single […]},
	language = {en-US},
	urldate = {2022-01-13},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = aug,
	year = {2017},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/7HRII4VU/stacked-long-short-term-memory-networks.html:text/html},
}

@misc{lendave_vijaysinh_lstm_2021,
	title = {{LSTM} {Vs} {GRU} in {Recurrent} {Neural} {Network}: {A} {Comparative} {Study}},
	shorttitle = {{LSTM} {Vs} {GRU} in {Recurrent} {Neural} {Network}},
	url = {https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/},
	abstract = {Long Short Term Memory in short LSTM is a special kind of RNN capable of learning long term sequences. They were introduced by Schmidhuber and Hochreiter in 1997. It is explicitly designed to avoid long term dependency problems. Remembering the long sequences for a long period of time is its way of working.},
	language = {en-US},
	urldate = {2022-01-13},
	journal = {Analytics India Magazine},
        author = {Vijaysinh Lendave},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/4JD33JEM/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study.html:text/html},
}
@book{box1976time,
  title={Time Series Analysis: Forecasting and Control},
  author={Box, G.E.P. and Jenkins, G.M. and Day, H.},
  isbn={9780816211043},
  lccn={76008713},
  series={Holden-Day series in time series analysis and digital processing},
  url={https://books.google.se/books?id=1WVHAAAAMAAJ},
  year={1976},
  publisher={Holden-Day}
}

@misc{verma_understanding_2021,
	title = {Understanding {Input} and {Output} shapes in {LSTM} {\textbar} {Keras}},
	url = {https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e},
	abstract = {When I started working with the LSTM networks, I was quite confused about the Input and Output shape. This article will help you to…},
	language = {en},
	urldate = {2022-01-17},
	journal = {Medium},
	author = {Verma, Shiva},
	month = oct,
	year = {2021},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/TZNHDHA4/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e.html:text/html},
}
