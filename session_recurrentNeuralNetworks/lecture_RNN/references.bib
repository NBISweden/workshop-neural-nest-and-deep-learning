@article{bourgeois_overview_2021,
	title = {An overview of current population genomics methods for the analysis of whole-genome resequencing data in eukaryotes},
	volume = {30},
	issn = {1365-294X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mec.15989},
	doi = {10.1111/mec.15989},
	abstract = {Characterizing the population history of a species and identifying loci underlying local adaptation is crucial in functional ecology, evolutionary biology, conservation and agronomy. The constant improvement of high-throughput sequencing techniques has facilitated the production of whole genome data in a wide range of species. Population genomics now provides tools to better integrate selection into a historical framework, and take into account selection when reconstructing demographic history. However, this improvement has come with a profusion of analytical tools that can confuse and discourage users. Such confusion limits the amount of information effectively retrieved from complex genomic data sets, and impairs the diffusion of the most recent analytical tools into fields such as conservation biology. It may also lead to redundancy among methods. To address these isssues, we propose an overview of more than 100 state-of-the-art methods that can deal with whole genome data. We summarize the strategies they use to infer demographic history and selection, and discuss some of their limitations. A website listing these methods is available at www.methodspopgen.com.},
	language = {en},
	number = {23},
	urldate = {2021-12-02},
	journal = {Molecular Ecology},
	author = {Bourgeois, Yann X. C. and Warren, Ben H.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mec.15989},
	keywords = {bioinformatics, demography, population genomics, selection, whole-genome sequencing},
	pages = {6036--6071},
	file = {Full Text PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/Y3IF9AHH/Bourgeois and Warren - 2021 - An overview of current population genomics methods.pdf:application/pdf;Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/CQVTTTNZ/mec.html:text/html},
}
@misc{aiml_vanishing_2018,
	title = {The vanishing gradient problem and {ReLUs} – a {TensorFlow} investigation – {Adventures} in {Machine} {Learning}},
	url = {https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/},
	language = {en-US},
	urldate = {2021-12-21},
        year = {2018},
        author = {Thomas, Andy},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/FZ9CQMQX/vanishing-gradient-problem-tensorflow.html:text/html},
}
@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-12-03},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/JBAZT7EA/neco.1997.9.8.html:text/html}
}
@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2020-12-03},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: EMNLP 2014},
	file = {arXiv Fulltext PDF:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/E76B63SQ/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf}
}

@misc{phi_illustrated_2020,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2021-12-28},
	journal = {Medium},
	author = {Phi, Michael},
	month = jun,
	year = {2020},
	file = {Snapshot:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/R5N9PK5P/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html},
}
@misc{olah_christopher_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2021-12-28},
        author = {Christopher Olah},
        date = {2015-08-27},
	file = {Understanding LSTM Networks -- colah's blog:/home/peru/.mozilla/firefox/ib8s5x0s.default-1472463039947/zotero/storage/IV2GHNSK/2015-08-Understanding-LSTMs.html:text/html},
}

@book{mackay_information_2003,
	address = {Cambridge, UK ; New York},
	edition = {Illustrated edition},
	title = {Information {Theory}, {Inference} and {Learning} {Algorithms}},
	isbn = {978-0-521-64298-9},
	language = {English},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	month = sep,
	year = {2003},
}

@misc{alexander_amini_mit_2021,
	title = {{MIT} 6.{S191}: {Recurrent} {Neural} {Networks}},
	shorttitle = {{MIT} 6.{S191}},
	url = {https://www.youtube.com/watch?v=qjrad0V0uJE},
	abstract = {MIT Introduction to Deep Learning 6.S191: Lecture 2
Recurrent Neural Networks
Lecturer: Ava Soleimany
January 2021

For all lectures, slides, and lab materials: http://introtodeeplearning.com​

Lecture Outline
0:00​ - Introduction
2:37​ - Sequence modeling
4:54​ - Neurons with recurrence
12:07​ - Recurrent neural networks
14:13​ - RNN intuition
17:01​ - Unfolding RNNs
18:39 - RNNs from scratch
22:12 - Design criteria for sequential modelling
23:37 - Word prediction example
31:31​ - Backpropagation through time
33:40​ - Gradient issues
38:46​ - Long short term memory (LSTM)
47:47​ - RNN applications
52:15​ - Attention
59:24​ - Summary

Subscribe to stay up to date with new deep learning lectures at MIT, or follow us @MITDeepLearning on Twitter and Instagram to stay fully-connected!!},
	urldate = {2021-12-31},
	author = {{Alexander Amini}},
	month = feb,
	year = {2021},
}
