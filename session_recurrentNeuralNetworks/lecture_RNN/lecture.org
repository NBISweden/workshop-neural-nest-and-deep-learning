#+STARTUP: indent
#+OPTIONS: toc:nil num:t  \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:nil H:4 rmd_yaml:nil
#+EXPORT_FILE_NAME: lecture.Rmd
#+CITE_EXPORT: csl /home/peru/opt/styles/apa.csl

* Header                                                             :ignore:
#+begin_export markdown
---
title: "Recurrent neural networks"
author:
  - Per Unneberg
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  revealjs::revealjs_presentation:
    css: revealjs.css
    includes:
      in_header: footer.html
    self_contained: false
    reveal_plugins: []
    highlight: breezedark
    fig_caption: false
    toc: false
    toc_depth: 2
    slide_level: 2
    transition: none
    reveal_options:
      slideNumber: true
      previewLinks: true
      minScale: 1
      maxScale: 1
      height: 800
      width: 1200
nocite: '@*'
csl: /home/peru/opt/styles/apa.csl
mainfont: Liberation Serif
monofont: Liberation Mono
bibliography: references.bib
---
#+end_export
* Knitr setup                                                        :ignore:
#+name: knitr-setup
#+begin_src R :ravel echo=FALSE, include=FALSE
library(knitr)
library(tidyverse)
library(kableExtra)
library(reticulate)
options(browser="firefox")
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      autodep=TRUE, echo=FALSE,
                      cache=FALSE, include=TRUE, eval=TRUE, tidy=FALSE, error=TRUE)
#class.source = "numberLines lineAnchors", comment="",
#                      class.output = c("numberLines lineAnchors chunkout"))
knitr::knit_hooks$set(inline = function(x) {
                      prettyNum(x, big.mark=" ")
})
knitr::opts_chunk$set(engine.opts=list(template="tikzfig.tex", dvisvgm.opts = "--font-format=woff"))
#+end_src

#+name: load-python-libraries
#+begin_src jupyter-python :ravel
import os
import sys
import pandas as pd
import bokeh
from bokeh.models import ColumnDataSource, HoverTool
from bokeh import plotting
from bokeh.io import output_notebook
from bokeh.embed import components
#+end_src


#+name: python-load-bokeh-scripts
#+begin_src python :ravel results="asis"
print("""
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js"
        crossorigin="anonymous"></script>
""")
#+end_src

* Overview

<h3>Recap</h3>
<h3>Sequential models</h3>
<h3>Recurrent neural networks (RNNs)</h3>
<h3>LSTMs and GRUs</h3>
<h3>Practical applications</h3>

** Outline                                                        :noexport:
1. Recap perceptron
   - Even if it has been done before recap perceptron with my notation
   - want to show what it looks like with a perceptron in a sequential
     model
2. Sequential models
   - begin with simple model, e.g. sinus time series
   - DNA sequence characteristics, language processing, time series (maybe intuitively simplest)
   - solve with perceptron
   - highlight problems with perceptron
3. RNNs
   -
4. LSTMs and GRUs
   - solution to vanishing gradients
   - need to explain *what* they do and *how* they solve the issue:
     - gated inputs / outputs
     - ReLUs (indep from above or part of?)
5. Practical applications
   - look in literature to focus on life sciences; possibly also languages as this is interesting in itself (e.g. google translate)



* Sandbox slides
FIXME: Remove this section; for testing purposes only

[cite/t/b:@bourgeois_overview_2021]

[cite:@aiml_vanishing_2018]


** Motivation
Show images of some sequential models

$$f(x) = \sum_{i=1}^n x_iw_i$$

#+name: tikz-perceptron
#+begin_src tikz :ravel cache=FALSE, fig.width=3, fig.ext="svg"
\begin{tikzpicture}[node distance=2cm, auto, >=latex', thick]
\draw[draw=white,use as bounding box](0,0) rectangle (4, 7);
                     % \path (0, 0)  rectangle (4, 7);
\node[ionode={16pt}{black}{$\boldsymbol{x_{0}}$}] (x0) at (1, 6) {};
\node[input={$\boldsymbol{x_1}$}] (x1) [below of=x0] {};
\node[input={}, draw=none, fill=none] (x2) [below of=x1] {};
\node[output={}, draw=none, fill=none] (y) [right of=x1] {};
\end{tikzpicture}
#+end_src


** Motivation
Show images of some sequential models

$$f(x) = \sum_{i=1}^n x_iw_i$$

#+name: tikz-perceptron-2
#+begin_src tikz :ravel cache=FALSE, fig.width=3, fig.ext="svg"
\begin{tikzpicture}[node distance=2cm, auto, >=latex', thick]
\draw[draw=white,use as bounding box](0,0) rectangle (4, 7);
                     % \path (0, 0)  rectangle (4, 7);
\node[ionode={16pt}{black}{$\boldsymbol{x_{0}}$}] (x0) at (1, 6) {};
\node[input={$\boldsymbol{x_1}$}] (x1) [below of=x0] {};
\node[input={$\boldsymbol{x_1}$}] (x2) [below of=x1] {};
\node[output={$\boldsymbol{y}$}] (y) [right of=x1] {};
\end{tikzpicture}
#+end_src
** Alternative motivation
Show simple example, e.g. of a time series with 1, 2, or 3 points
#+name: tensorflow-block
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
import tensorflow as tf

rnn = tf.layer.SimpleRNN()
#+end_src
** Bokeh plot
#+name: bokeh-test-plot
#+begin_src jupyter-python :ravel results="asis", fig.align="right", out.width="800px"
# prepare some data
x = [1, 2, 3, 4, 5]
y1 = [4, 5, 5, 7, 2]
y2 = [2, 3, 4, 5, 6]

# create a new plot
p = plotting.figure(title="Legend example")

# add circle renderer with legend_label arguments
line = p.line(x, y1, legend_label="Temp.", line_color="blue", line_width=2)
circle = p.circle(
    x,
    y2,
    legend_label="Objects",
    fill_color="red",
    fill_alpha=0.5,
    line_color="blue",
    size=80,
)

# display legend in top left corner (default is top right corner)
p.legend.location = "top_left"

# add a title to your legend
p.legend.title = "Obervations"

# change appearance of legend text
p.legend.label_text_font = "times"
p.legend.label_text_font_style = "italic"
p.legend.label_text_color = "navy"

# change border and background of legend
p.legend.border_line_width = 3
p.legend.border_line_color = "navy"
p.legend.border_line_alpha = 0.8
p.legend.background_fill_color = "navy"
p.legend.background_fill_alpha = 0.2

script, div = components(p)
print(script)
print(div)
#+end_src

** Test animation                                                 :noexport:
#+name: tikz-test-animation
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, engine.opts=list(template="tikzfig-anim.tex")
\begin{tikzpicture}
  \tikz \node :fill opacity = { 0s="1", 2s="0", begin on=click }
  [fill = blue!20, draw = blue, ultra thick, circle] {ooeoeu};
\end{tikzpicture}
#+end_src
* Recap
** Perceptron (single neuron)
::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-rnn-recap-perceptron-1
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=4
\begin{tikzpicture}[node distance=2*\nodesep cm, >=latex]
\node[ionode={32pt}{black}{{$x_0=1$}}] (x0) {};
\node[input={$x_1$}, below left of=x0] (x1) {};
\node[input={$x_2$}, below of=x1] (x2) {};
\node[below of=x2] (vdots) {$\vdots$};
\node[input={$x_n$}, below of=vdots] (xn) {};

\node[circle, draw=black, thick, minimum size=32pt, right of=x2] (sum) {$\mathrm{f(a)}$};

\node[output={$y$}, right of=sum, node distance=4*\nodesep cm] (y) {};

\draw[->] (x0) -- (sum) node [midway, right] {$w_0$};
\draw[->] (x1) -- (sum) node [midway, above] {$w_1$};
\draw[->] (x2) -- (sum) node [midway, above] {$w_2$};
\draw[->] (xn) -- (sum) node [midway, right] {$w_n$};
\draw[->] (sum) -- (y);
\end{tikzpicture}
#+end_src

:::: 

:::: {}

<h3>Architecture</h3>

A single neuron has $n$ /inputs/ $x_i$ and an /output/ $y$. To each
input is associated a /weight/ $w_i$.

<h3>Activity rule</h3>
The *activity rule* is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=1,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

:::: 

:::::::: 
*** Notes                                                          :ignore:
::: {.notes}

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 [cite:@mackay_information_2003 p.471]

[cite:@alexander_amini_mit_2021 5:43] point out inputs $x_i$ represent
*one* time point

Draw bottom-up; then dimensions are in "correct" order

:::
** Feed forward network

several inputs and outputs

*** Notes                                                          :ignore:
:::: {.notes}

Show multi-valued (vector) output and hidden layer

::::
** Simplified illustration
Condense hidden layers to a box.
*** Notes                                                          :ignore:
:::: {.notes}

Emphasize input/output dimensions? (R_m, R_n)

::::
** Simplified illustration and notation
#+name: tikz-rnn-recap-perceptron-2
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=10
\begin{tikzpicture}[node distance=4*\nodesep cm, >=latex]

\node[input={$\boldsymbol{x}$}] (x) {};
\node[ionode={16pt}{black}{$\sum$}, draw=black, thick, minimum size=16pt, right of=x] (sum) {};
\node[sigtan={16pt}{blue}{4pt}, right of=sum] (tanh) {};
\node[output={$y$}, right of=tanh] (y) {};

\draw[->] (x) -- (sum) node [midway, above] {$\boldsymbol{w}$};
\draw[->] (sum) -- (tanh) node [midway, above] {$\boldsymbol{wx}$};
\draw[->] (tanh) -- (y) node [midway, above] {$\mathrm{tanh(}\boldsymbol{wx}\mathrm{)}$};
\end{tikzpicture}
#+end_src


<h3>Architecture</h3>

Vectorized versions: input $\boldsymbol{x}$, weights $\boldsymbol{w}$,
output $\boldsymbol{y}$

<h3>Activity rule</h3>

$$a = \boldsymbol{wx}$$

:::: {.notes}

FIXME: inconsistent notation? Weights are depicted as attached to
first arrow, then the labels indicate what *value* is passed along

::::

* Sequential models
** Motivation

#+name: tikz-rnn-motivation-time-series
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=white] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::
** Motivation

#+name: tikz-rnn-motivation-time-series-1
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** Motivation

#+name: tikz-rnn-motivation-time-series-2
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** Motivation

#+name: tikz-rnn-motivation-time-series-3
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}[>=latex]
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\draw[->, thick, black!50, dotted] (x2) to[out=90, in=180] (x1) to[out=0, in=90] (x0);
\end{tikzpicture}
#+end_src


*** Notes                                                          :ignore:
::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** Concrete models
FIXME: add examples from
- genomics
- time series
- language processing
- ...

** Temporal aspects
Provide the alphabet example from [cite:@phi_illustrated_2020]

A -> Z: easy, given one letter the other follows

Z -> A: try do enumerate alphabet in reverse; non-trivial

** Types of models
*** one-to-one
*** one-to-many
*** many-to-many
* RNNs

:::{.element: class="fragment"}

#+name: tikz-rnn-folded-only
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=4
\begin{tikzpicture}[thick]
  \def\sep{3 * \basenodesep}
  \def\width{\sep}
  \def\height{\rnnouterheight}
  \pic (foldedrnn_) {rnniofolded};
\end{tikzpicture}
#+end_src

:::
** Why ffns don't work

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

Column 1

:::: 

:::: {}

Nullam eu ante vel est convallis dignissim. Fusce suscipit,
wisi nec facilisis facilisis, est dui fermentum leo, quis tempor
ligula erat quis odio. Nunc porta vulputate tellus. Nunc rutrum turpis
sed pede. Sed bibendum. Aliquam posuere. Nunc aliquet, augue nec
adipiscing interdum, lacus tellus malesuada massa, quis varius mi
purus non odio. Pellentesque condimentum, magna ut suscipit hendrerit,
ipsum augue ornare nulla, non luctus diam neque sit amet urna.
Curabitur vulputate vestibulum lorem. Fusce sagittis, libero non
molestie mollis, magna orci ultrices dolor, at vulputate neque nulla
lacinia eros. Sed id ligula quis est convallis tempor. Curabitur
lacinia pulvinar nibh. Nam a sapien.



:::: 

:::: {}

Column 3

:::: 

::::::::::::::::::: 
** Why we need them and what they are

#+name: tikz-rnn-folded
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=12
\begin{tikzpicture}[thick]
  \def\sep{3 * \basenodesep}
  \def\width{6.5 * \sep}
  \def\height{\rnnouterheight}
  \draw[white] (0,0) rectangle (\width, \height);
  \pic (foldedrnn_) {rnniofolded};
\end{tikzpicture}
#+end_src


** Why we need them and what they are

#+name: tikz-rnn-folded-unfolded
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=12
\begin{tikzpicture}[thick]
  \def\sep{3 * \basenodesep}
  \def\width{6.5 * \sep}
  \def\height{\rnnouterheight}
  \draw[white] (0,0) rectangle (\width, \height);
  \pic (foldedrnn_) {rnniofolded};
  \node[xshift=1.5*\sep, yshift=\height/2] (eq) {\Huge =};
  \pic[xshift=2*\sep] (x0_) {rnnio={A}{$X_0$}{$H_0$}};
  \pic[xshift=3*\sep] (x1_) {rnnio={A}{$X_1$}{$H_1$}};
  \pic[xshift=4*\sep] (x2_) {rnnio={A}{$X_2$}{$H_2$}};
  \node[xshift=5*\sep] (dots) {\Huge \dots};
  \pic[xshift=5.5*\sep] (xt_) {rnnio={A}{$X_t$}{$H_t$}};
  \draw[->] (x0_right) -- (x1_left);
  \draw[->] (x1_right) -- (x2_left);
  \draw[->] (x2_right) -- (xt_left);
\end{tikzpicture}
#+end_src



** Parameter sharing
contrast with FFNs
** RNN basic architecture
[cite:@olah_christopher_understanding_nodate]

#+name: tikz-rnn-basic-architecture-1
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=18
\begin{tikzpicture}
  \draw[white] (0,0) rectangle (3*\rnnfigwidth, \rnnfigheight);
  \pic[xshift=\rnnfigwidth]{vanillarnn};
\end{tikzpicture}        
#+end_src
** RNN basic architecture
:PROPERTIES:
:ID:       e374b4c9-8989-43f2-a696-b7b143e3124f
:END:
[cite:@olah_christopher_understanding_nodate]

#+name: tikz-rnn-basic-architecture-2
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=18
\begin{tikzpicture}
  \draw[white] (0,0) rectangle (3*\rnnfigwidth, \rnnfigheight);
  \pic[xshift=\rnnfigwidth]{vanillarnn};
  \pic[xshift=0, xtlabel=$X_{t-1}$, htlabel=$H_{t-1}$]{vanillarnnoverlay};
\end{tikzpicture}        
#+end_src
** RNN basic architecture
[cite:@olah_christopher_understanding_nodate]

#+name: tikz-rnn-basic-architecture-3
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=18
\begin{tikzpicture}
  \draw[white] (0,0) rectangle (3*\rnnfigwidth, \rnnfigheight);
  \pic[xshift=\rnnfigwidth]{vanillarnn};
  \pic[xshift=0, xtlabel=$X_{t-1}$, htlabel=$H_{t-1}$]{vanillarnnoverlay};
  \pic[xshift=2 * \rnnfigwidth, xtlabel=$X_{t+1}$, htlabel=$X_{t+1}$]{vanillarnnoverlay};
\end{tikzpicture}        
#+end_src
** Examples
Examples using vanilla RNNs

e.g. Box & Jenkins airline passenger data set
** Exercise
Segway into exercise with co2 data
* Training
** Backpropagation in time
** Exploding and vanishing gradients
** Problems with Vanilla RNNs
* LSTMs and GRUs
** Motivation behind LSTMs and GRUs

<div class="based-on">Based on [cite/t/b:@phi_illustrated_2020]</div>
::::::::::::::::::: {style="display: flex;"}

:::: {}

<h6 align="center">LSTM</h6>

#+name: tikz-lstm
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}[node distance=1cm]
\node (lstm) at (0, 0) {\lstm[$\boldsymbol{c_{t-1}}$][$\boldsymbol{c_{t}}$][$\boldsymbol{x_t}$][$\boldsymbol{h_{t-1}}$][$\boldsymbol{h_{t}}$]};
\end{tikzpicture}
#+end_src

::::

:::: {}

<h6 align="center">GRU</h6>

#+name: tikz-gru
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}[node distance=1cm]
\node (gru) at (0, 0) {\gru[$\boldsymbol{c_{t-1}}$][$\boldsymbol{c_{t}}$][$\boldsymbol{x_t}$]};
\end{tikzpicture}
#+end_src

::::

:::::::::::::::::::

#+name: tikz-gru-lstm-legend
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=6
\tikzset{legend/.style={
        font=\sffamily\bfseries\tiny,
        text width=1.4cm,
        align=center
        }
}
\begin{tikzpicture}[node distance=1cm]
\node[pwise=X, node distance=2cm, label={[legend]below:pointwise multiplication}] (pmult) {};
\node[tanh, left of=pmult, label={[legend]below:tanh}] (tanh) {};
\node[sigmoid, left of=tanh, label={[legend]below:sigmoid}] (sigmoid) {};
\node[pwise=+, right of=pmult, label={[legend]below:pointwise addition}] (padd) {};
\node[vcon=1cm, right of=padd, label={[legend]below:vector concatenation}] (vconcat) {};
\end{tikzpicture}
#+end_src

Long Short Term Memory (LSTM) [cite:@hochreiter_long_1997] and Gated
Recurrent Unit (GRU) [cite:@cho_learning_2014] architectures were
proposed to solve the vanishing gradient problem.


*** Notes                                                          :ignore:

::: {.notes}
Here are some notes...
- explain pseudo-targets
- point out the two common idioms for collecting targets:
  1. expand
  2. input functions

:::

** Intuition
Example on cereal ad really good (we remember the important parts):

https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

** Gating (forget / remember)
** A closer look at LSTM architecture - the forget gate


#+name: tikz-lstm-forget-gate
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node (lstm) at (0, 0) {\lstmforgetgate};
\end{tikzpicture}
#+end_src

*Purpose*: decide what information to keep or throw away

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where 0=forget, 1=keep.


** A closer look at LSTM architecture - the forget gate


#+name: tikz-lstm-forget-gate-1
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node (lstm) at (0, 0) {\lstmforgetgate};
%\node[packet, scale=.3] (ht) at (-2.7, -.53) {$h_{t-1}$};
%\node[packet, scale=.3] (xt) at (-2.2, -.7) {$h_{t-1}$};
\end{tikzpicture}
#+end_src

*Purpose*: decide what information to keep or throw away

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where 0=forget, 1=keep.

\[
f_t = \sigma()
\]

** The input gate

#+name: tikz-lstm-input-gate
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node (lstm) at (0, 0) {\lstminputgate};
\end{tikzpicture}
#+end_src

*Purpose*:
** The cell state

#+name: tikz-lstm-cell-state
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node (lstm) {\lstmcellstate};
\end{tikzpicture}
#+end_src
*Purpose*:
** The output gate

#+name: tikz-lstm-output-gate
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node (lstm) {\lstmoutputgate};
\end{tikzpicture}
#+end_src

*Purpose*:
** Exercise
Exercise that compares vanilla RNN to LSTMs
* Applications
** Google translate
feels like one of the more obvious language applications that people
use in everyday life
** Time series
** Attention networks
Mention attention networks as a next step generalisation?
** Recombination rate estimation in genomics
segway to practical

* Bibliography                                                       :ignore:

** Bibliography {.allowframebreaks}
<div id="refs" class="references hanging-indent" role="doc-bibliography" style="font-size: 70%;">
