#+STARTUP: indent
#+OPTIONS: toc:nil num:t  \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:nil H:4 rmd_yaml:nil
#+EXPORT_FILE_NAME: lecture.Rmd
#+CITE_EXPORT: csl /home/peru/opt/styles/apa.csl
#+PROPERTY: header-args:jupyter-python :kernel nn_dl_python :results silent

* Header                                                             :ignore:
#+begin_export markdown
---
title: "Recurrent neural networks"
author:
  - Per Unneberg
date: "20 January, 2022"
output:
  revealjs::revealjs_presentation:
    css: revealjs.css
    includes:
      in_header: footer.html
    self_contained: true
    reveal_plugins: []
    highlight: breezedark
    fig_caption: false
    toc: false
    toc_depth: 2
    slide_level: 2
    transition: none
    reveal_options:
      slideNumber: true
      previewLinks: true
      minScale: 1
      maxScale: 1
      height: 800
      width: 1200
csl: /home/peru/opt/styles/apa.csl
mainfont: Liberation Serif
monofont: Liberation Mono
bibliography: references.bib
nocite: |
  @hochreiter_long_1997
---
#+end_export
* Knitr setup                                                        :ignore:
#+name: knitr-setup
#+begin_src R :ravel echo=FALSE, include=FALSE
library(knitr)
library(tidyverse)
library(kableExtra)
library(reticulate)
options(browser="firefox")
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      autodep=TRUE, echo=FALSE,
                      cache=FALSE, include=TRUE, eval=TRUE, tidy=FALSE, error=TRUE
                      )
#class.source = "numberLines lineAnchors", comment="",
#                      class.output = c("numberLines lineAnchors chunkout"))
knitr::knit_hooks$set(inline = function(x) {
                      prettyNum(x, big.mark=" ")
})
knitr::opts_chunk$set(engine.opts=list(template="tikzfig.tex", dvisvgm.opts = "--font-format=woff"))
#+end_src

#+name: load-python-libraries
#+begin_src jupyter-python :ravel
import os
import sys
import pandas as pd
import rnnutils
import bokeh
from bokeh.models import ColumnDataSource, HoverTool
from bokeh import plotting
from bokeh.io import output_notebook
from bokeh.embed import components
#+end_src


#+name: python-load-bokeh-scripts
#+begin_src python :ravel results="asis"
print("""
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js"
        crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js"
        crossorigin="anonymous"></script>
""")
#+end_src


* Overview

<h3>Recap</h3>
<h3>Sequential models</h3>
<h3>Recurrent neural networks (RNNs)</h3>
<h3>LSTMs and GRUs</h3>
<h3>Practical applications</h3>
*** Notes                         :ignore:
:::: {.notes}

1. Recap perceptron
   - Even if it has been done before recap perceptron with my notation
   - want to show what it looks like with a perceptron in a sequential
     model
2. Sequential models
   - begin with simple model, e.g. sinus time series
   - DNA sequence characteristics, language processing, time series (maybe intuitively simplest)
   - solve with perceptron
   - highlight problems with perceptron
3. RNNs
   -
4. LSTMs and GRUs
   - solution to vanishing gradients
   - need to explain *what* they do and *how* they solve the issue:
     - gated inputs / outputs
     - ReLUs (indep from above or part of?)
5. Practical applications
   - look in literature to focus on life sciences; possibly also languages as this is interesting in itself (e.g. google translate)
::::

* Sandbox slides                                                   :noexport:
FIXME: Remove this section; for testing purposes only

** Motivation
Show images of some sequential models

$$f(x) = \sum_{i=1}^n x_iw_i$$

#+name: tikz-perceptron
#+begin_src tikz :ravel cache=FALSE, fig.width=3, fig.ext="svg"
\begin{tikzpicture}[node distance=2cm, auto, >=latex', thick]
\draw[draw=white,use as bounding box](0,0) rectangle (4, 7);
                     % \path (0, 0)  rectangle (4, 7);
\node[ionode={16pt}{black}{$\boldsymbol{x_{0}}$}] (x0) at (1, 6) {};
\node[input={$\boldsymbol{x_1}$}] (x1) [below of=x0] {};
\node[input={}, draw=none, fill=none] (x2) [below of=x1] {};
\node[output={}, draw=none, fill=none] (y) [right of=x1] {};
\end{tikzpicture}
#+end_src


** Motivation
Show images of some sequential models

$$f(x) = \sum_{i=1}^n x_iw_i$$

#+name: tikz-perceptron-2
#+begin_src tikz :ravel cache=FALSE, fig.width=3, fig.ext="svg"
\begin{tikzpicture}[node distance=2cm, auto, >=latex', thick]
\draw[draw=white,use as bounding box](0,0) rectangle (4, 7);
                     % \path (0, 0)  rectangle (4, 7);
\node[ionode={16pt}{black}{$\boldsymbol{x_{0}}$}] (x0) at (1, 6) {};
\node[input={$\boldsymbol{x_1}$}] (x1) [below of=x0] {};
\node[input={$\boldsymbol{x_1}$}] (x2) [below of=x1] {};
\node[output={$\boldsymbol{y}$}] (y) [right of=x1] {};
\end{tikzpicture}
#+end_src
** Alternative motivation
Show simple example, e.g. of a time series with 1, 2, or 3 points
#+name: tensorflow-block
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
import tensorflow as tf

rnn = tf.layer.SimpleRNN()
#+end_src
** Bokeh plot                                                     :noexport:
#+name: bokeh-test-plot
#+begin_src jupyter-python :ravel results="asis", fig.align="right", out.width="800px"
# prepare some data
x = [1, 2, 3, 4, 5]
y1 = [4, 5, 5, 7, 2]
y2 = [2, 3, 4, 5, 6]

# create a new plot
p = plotting.figure(title="Legend example")

# add circle renderer with legend_label arguments
line = p.line(x, y1, legend_label="Temp.", line_color="blue", line_width=2)
circle = p.circle(
    x,
    y2,
    legend_label="Objects",
    fill_color="red",
    fill_alpha=0.5,
    line_color="blue",
    size=80,
)

# display legend in top left corner (default is top right corner)
p.legend.location = "top_left"

# add a title to your legend
p.legend.title = "Obervations"

# change appearance of legend text
p.legend.label_text_font = "times"
p.legend.label_text_font_style = "italic"
p.legend.label_text_color = "navy"

# change border and background of legend
p.legend.border_line_width = 3
p.legend.border_line_color = "navy"
p.legend.border_line_alpha = 0.8
p.legend.background_fill_color = "navy"
p.legend.background_fill_alpha = 0.2

script, div = components(p)
print(script)
print(div)
#+end_src
** rnn

#+name: tikz-test-rnn
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
  \pic {RNN};
\end{tikzpicture}
#+end_src

** lstm empty

#+name: tikz-test-lstm-empty
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
  \pic {lstmempty};
\end{tikzpicture}
#+end_src

** lstm

#+name: tikz-test-lstm
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=updatecellstate, add labels=true] {lstm};
\end{tikzpicture}
#+end_src

** Test animation                                                 :noexport:
#+name: tikz-test-animation
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, engine.opts=list(template="tikzfig-anim.tex")
\begin{tikzpicture}
  \tikz \node :fill opacity = { 0s="1", 2s="0", begin on=click }
  [fill = blue!20, draw = blue, ultra thick, circle] {ooeoeu};
\end{tikzpicture}
#+end_src

* Recap
** Perceptron (single neuron)
::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-rnn-recap-perceptron-simple
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {mackayperceptron};
\end{tikzpicture}
#+end_src

::::

:::: {}


<h3>Architecture</h3>

A single neuron has $n$ /inputs/ $x_i$ and an /output/ $y$. To each
input is associated a /weight/ $w_i$.

<h3>Activity rule</h3>
The *activity rule* is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=1,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

cite:mackay_information_2003

::::

::::::::
*** Notes                                                          :ignore:
::: {.notes}

Beware of notation here. Points to make:

- $w_0=1$ -> bias
- activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 [cite:@mackay_information_2003 p.471]

[cite:@alexander_amini_mit_2021_rnn 5:43] point out inputs $x_i$ represent
*one* time point



:::
** Perceptron (single neuron)
::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-rnn-recap-perceptron-activity
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture}
#+end_src

::::

:::: {}

<h3>Architecture</h3>

A single neuron has $n$ /inputs/ $x_i$ and an /output/ $y$. To each
input is associated a /weight/ $w_i$.

<h3>Activity rule</h3>
The *activity rule* is given by two steps:

$$a = \sum_{i} w_ix_i, \quad i=1,...,n$$

$$\begin{array}{ccc}
\mathrm{activation} & & \mathrm{activity}\\
a & \rightarrow & y(a)
\end{array}$$

cite:mackay_information_2003
::::

::::::::
*** Notes                                                          :ignore:
::: {.notes}

Beware of notation here. Points to make:

- $w_0=1$ -> bias
- activation -> activity can be separated (next slide)

Alternative view of bias: an additional weight $w_0$ with input
permanently set to 1 [cite:@mackay_information_2003 p.471]

[cite:@alexander_amini_mit_2021 5:43] point out inputs $x_i$ represent
*one* time point



:::
** Perceptron (single neuron)
::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1.5fr; grid-column-gap: 10px;"}


:::: {}

#+name: tikz-rnn-recap-perceptron-vectorized
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {mackayperceptronactivity};
\end{tikzpicture}
#+end_src

::::

:::: {}

$$a = w_0 + \sum_{i} w_ix_i, \quad i=1,...,m$$

$$y = y(a) = g\left( w_0 + \sum_{i=1}^{m} w_ix_i \right)$$


:::: {.element: class="fragment"}

or in vector notation

$$y = g\left(w_0 + \mathbf{X^T} \mathbf{W} \right)$$

where:

$$\quad\mathbf{X}=
\begin{bmatrix}x_1\\ \vdots \\ x_m\end{bmatrix},
\quad \mathbf{W}=\begin{bmatrix}w_1\\ \vdots \\ w_m\end{bmatrix}$$

::::
cite:alexander_amini_mit_2021_rnn
::::

::::::::
*** Notes                                                          :ignore:
::: {.notes}

Follow MIT notation: g() is the non-linear activation (function)

:::
** Simplified illustration and notation
#+name: tikz-rnn-recap-perceptron-simplified
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=10
\begin{tikzpicture}[node distance=4*\basenodesep, >=latex]

\node[input={$\boldsymbol{x}$}] (x) {};
\node[ionode={16pt}{black}{$\sum$}, draw=black, thick, minimum size=16pt, right of=x] (sum) {};
\node[sigtan={16pt}{blue}{4pt}, right of=sum] (tanh) {};
\node[output={$y$}, right of=tanh] (y) {};

\draw[->] (x) -- (sum) node [midway, above] {$\boldsymbol{w}$};
\draw[->] (sum) -- (tanh) node [midway, above] {$\boldsymbol{wx}$};
\draw[->] (tanh) -- (y) node [midway, above] {$\mathrm{tanh(}\boldsymbol{wx}\mathrm{)}$};
\end{tikzpicture}
#+end_src


<h3>Architecture</h3>

Vectorized versions: input $\boldsymbol{x}$, weights $\boldsymbol{w}$,
output $\boldsymbol{y}$

<h3>Activity rule</h3>

$$a = \boldsymbol{wx}$$
*** Notes                                                          :ignore:
:::: {.notes}

FIXME: inconsistent notation? Weights are depicted as attached to
first arrow, then the labels indicate what *value* is passed along

::::



** Feed forward network
:PROPERTIES:
:ID:       aead39b9-26e8-4672-939c-25c569fcebc4
:END:

#+name: tikz-rnn-recap-perceptron-multiout
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=0},
    nnconnection/.append style={->},
  }
  \rnntikzset{
    dotted=true
  }
  \pic (i_) {nnlayer={5}{input}{X}{m}{}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/boxed=true] (h1_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=5*\basenodesep, \rnntikzbasekey/boxed=true] (h_) {nnlayer={3}{hidden}{}{}{green}};
  \pic[xshift=7*\basenodesep] (o_) {nnlayer={3}{output}{\widehat{Y}}{n}{}};
  \pic {connectlayers={i_}{h1_}{5}{3}};
  \pic {connectlayers={h1_}{h_}{3}{3}};
  \pic {connectlayers={h_}{o_}{3}{3}};

  \node[xshift=3.5*\basenodesep, yshift=-2.5*\basenodesep] {$\dots$};
  \node[xshift=2*\basenodesep, yshift=2*\basenodesep] {$1$};
  \node[xshift=5*\basenodesep, yshift=2*\basenodesep] {$k$};

  \node[yshift=-4*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=7*\basenodesep, yshift=-4*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture}
#+end_src

*** Notes                                                          :ignore:
:::: {.notes}

Show multi-valued (vector) output and hidden layer

::::
** Simplified illustration

#+name: tikz-rnn-recap-perceptron-multiout-simple
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
  \tikzset{
    iolabel/.append style={rotate=-270},
    nnconnection/.append style={->},
  }
  \begin{scope}[rotate=270, transform shape]
    \pic {rnnio={}{$X$}{$Y$}};
  \end{scope}
  \node[yshift=-2*\basenodesep] {$\mathbf{X} \in \mathbb{R}^m$};
  \node[xshift=3*\basenodesep, yshift=-2*\basenodesep] {$\mathbf{Y} \in \mathbb{R}^n$};
\end{tikzpicture}
#+end_src

*** Notes                                                          :ignore:
:::: {.notes}

Condense hidden layers to a box.

::::
* Sequential models
** Motivation

#+name: tikz-rnn-motivation-time-series
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=white] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::
** Motivation

#+name: tikz-rnn-motivation-time-series-1
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=white] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** Motivation

#+name: tikz-rnn-motivation-time-series-2
#+begin_src tikz :ravel cache=TRUE, fig.ext="svg", fig.width=8
\begin{tikzpicture}
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\end{tikzpicture}
#+end_src

::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** Motivation

#+name: tikz-rnn-motivation-time-series-3
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=8
\begin{tikzpicture}[>=latex]
\node[obs, fill=black!50] (x0) at (100pt, 0pt) {};
\node[obs, fill=black!20] (x1) at (50pt, 50pt) {};
\node[obs, fill=black!20] (x2) at (0pt, 0pt) {};
\draw[->, thick, black!50, dotted] (x2) to[out=90, in=180] (x1) to[out=0, in=90] (x0);
\end{tikzpicture}
#+end_src


*** Notes                                                          :ignore:
::: {.notes}

incremental figure showing time series (e.g. sinus) that highlights
- dependency on previous time point
- (weaker) dependency on more distant time points

:::

** A few concrete examples of real-world sequences


<h5>Language translation</h5>

#+name: rnn-example-language-translation
#+begin_src tikz :ravel fig.ext="svg", fig.width=6
\begin{tikzpicture}[node distance=2cm, >=latex]
  \node[align=left, font=\ttfamily, text width=22pt, rectangle, draw=black, thick] (vec) {+0.5 +0.2 -0.1 -0.3 +0.4 +1.2};
  \node[left of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=blue!20, draw=blue!80, anchor=east, rounded corners, thick, label={center:Encoder}] (encoder) {};
  \node[left of=encoder, anchor=east] (swedish) {jag är en student};
  \node[right of=vec, rectangle, minimum width=1.5cm, text height=1cm, align=center, fill=violet!20, draw=violet!80, anchor=west, rounded corners, thick, label={center:Decoder}] (decoder) {};
  \node[right of=decoder, anchor=west] (english) {I am a student};
  \draw[->] (swedish) -- (encoder);
  \draw[->] (encoder) -- (vec);
  \draw[->] (vec) -- (decoder);
  \draw[->] (decoder) -- (english);
\end{tikzpicture}
#+end_src


::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2px;"}

:::: {}

<h5>Time series</h5>

#+name: sequential-model-time-series-example
#+begin_src R :ravel out.width="500px"
knitr::include_graphics("https://github.com/unit8co/darts/raw/master/static/images/example.png")
#+end_src

# Box & Jenkins airline passenger data set

cite:herzen2021darts

::::


:::: {}

<h5>Genomics</h5>

#+name: sequential-model-genomics-example
#+begin_src R :ravel out.width="300px"
knitr::include_graphics("https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-33321-1/MediaObjects/41598_2018_33321_Fig5_HTML.png?as=webp")
#+end_src


cite:shen_recurrent_2018

::::

:::::::::::::::::::

** Types of models

::::::::::::::::::: {style="display: grid; grid-template-columns: 10fr 1fr 10fr 10fr 10fr; grid-gap: 0px 0px; grid-template-rows: 1fr 1fr;" }

:::: {}

<h5>one to one</h5>

#+name: sequential-models-one-to-one
#+begin_src tikz :ravel fig.ext="svg", fig.width=1, cache=FALSE
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic{rnnio={}{}{}};
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {.element: class="fragment" data-fragment-index="2"}

<h5>many to one</h5>


#+name: sequential-models-many-to-one
#+begin_src tikz :ravel fig.ext="svg", fig.width=3, cache=FALSE
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnioin};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioin};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture}
#+end_src


::::

:::: {.element: class="fragment" data-fragment-index="3"}

<h5>one to many</h5>

#+name: sequential-models-one-to-many
#+begin_src tikz :ravel fig.ext="svg", fig.width=3, cache=FALSE
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnioout};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnioout};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture}
#+end_src


::::

:::: {.element: class="fragment"  data-fragment-index="4"}

<h5>many to many</h5>

#+name: sequential-models-many-to-many
#+begin_src tikz :ravel fig.ext="svg", fig.width=3, cache=FALSE
\begin{tikzpicture}
  \tikzset{rnnw=0.75\rnninnerwidth}
  \tikzset{rnnh=1.5\rnninnerheight}
  \pic (r1) {rnnio={}{}{}};
  \pic[xshift=1*\rnnouterwidth] (r2) {rnnio={}{}{}};
  \pic[xshift=2*\rnnouterwidth] (r3) {rnnio={}{}{}};
  \draw[->] (r1_center) -- (r2_center);
  \draw[->] (r2_center) -- (r3_center);
\end{tikzpicture}
#+end_src


::::

# r2c1
:::: {}

<h6>Image classification</h6>

#+name: fashion-mnist-image-classification
#+begin_src python :ravel cache=TRUE
from tensorflow import keras
import matplotlib.pyplot as plt
fashion_mnist = keras.datasets.fashion_mnist
img = fashion_mnist.load_data()[0][0:2][0][0:2]
label = fashion_mnist.load_data()[0][0:2][1][0:2]
class_names = [ "T-shirt/top" , "Trouser" , "Pullover" , "Dress" , "Coat" , "Sandal" , "Shirt" , "Sneaker" , "Bag" , "Ankle boot" ]
plt.figure(figsize=(10,5))
plt.rc('axes', labelsize=40)
for i in range(2):
    plt.subplot(1,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(img[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[label[i]])
plt.show()
#+end_src

::::

# r2c2
:::: {}

::::

# r2c3
:::: {.element: class="fragment" data-fragment-index="2"}

<h6>Sentiment analysis</h6>

#+name: sequential-model-sentiment-analysis
#+begin_src R :ravel out.width="300px"
knitr::include_graphics("https://d1sjtleuqoc1be.cloudfront.net/wp-content/uploads/2019/04/25112909/shutterstock_1073953772.jpg")
#+end_src

::::

# r2c4
:::: {.element: class="fragment" data-fragment-index="3"}

<h6>Image captioning</h6>

#+name: sequential-model-image-captioning
#+begin_src R :ravel out.width="350px"
knitr::include_graphics("https://cocodataset.org/images/captions-splash.jpg")
#+end_src

::::
# r2c5
:::: {.element: class="fragment" data-fragment-index="4"}

<h6>Machine translation</h6>

#+name: sequential-model-google-translate
#+begin_src R :ravel out.width="200px"
knitr::include_graphics("https://img.icons8.com/plasticine/344/google-translate-new-logo.png")
#+end_src

::::

:::::::::::::::::::

cite:karpathy_unreasonable_effectiveness_of_RNNs



*** Notes                                                          :ignore:
:::: {.notes}

Important point here: each input/output/hidden are *vectors*

[cite:@karpathy_unreasonable_effectiveness_of_RNNs]

Issues with Vanilla NNs and CNNs:
- dependency on *fixed* size input
- fixed amount of computational steps


Models:

- one to one :: Vanilla processing without RNN, from fixed input to
  fixed output e.g. image classification (aka vanilla neural network)
- one to many :: sequence output, e.g. image captioning
- many to one :: sequence input, e.g. sentiment analysis (classify
  sequence as happy/sad/...)
- many to many :: sequence input and sequence output, e.g. machine
  translation


Data:

- [cite:@xiao_fashion-mnist_2017]

- https://cocodataset.org/#captions-2015

::::
* Recurrent Neural Networks (RNNs)
** Intro                                                            :ignore:
<br/>

#+name: tikz-rnn-folded-only
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=4
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={A}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

*** Notes                                                          :ignore:
:::: {.notes}

We will now look at the essentials of RNNs. As the figure implies, the
output of the network


[cite:@alexander_amini_mit_2021 5:43] point out inputs $x_i$ represent
*one* time point

input, output, green box: contains vectors of data, arrows represent
operations/functions
[cite:@karpathy_unreasonable_effectiveness_of_RNNs]

Key feature: the recurrence (green) can be applied as many times as we
want, i.e. no constraint on input size

Why recurrent networks?
https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn

FFNs

- Cannot handle sequential data
- Considers only the current input
- Cannot memorize previous inputs

and information only flows forward (i.e. no memory)

::::

** Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-1
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {.element: class="fragment" data-fragment-index="2" style="border-left: 2px black solid;"}

#+name: ffn-x0-xt-1
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \end{scope}
  \end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {.element class="fragment" data-fragment-index="1"}

Assume multiple time points.

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

Rotated FFN: take a moment to recap the ffn. Input $X_t \in
\mathbb{R}^{m}$ is mapped to output $\widehat{Y}_t \in \mathbb{R}^n$ via
the network ($f(\cdot)$

Now assume we have several time steps, starting at e.g. time 0. Also we predict the outputs individually.

::::

** Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-2
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid;"}

#+name: ffn-x0-xt-2
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \end{scope}
\end{tikzpicture}
#+end_src


::::

:::: {}

::::

:::: {}

Assume multiple time points.

::::

:::::::::::::::::::


*** Notes                         :ignore:
:::: {.notes}

Add another time step...

::::

** Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-3
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-3
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {}

Assume multiple time points.

> - Dependency of inputs not modelled such that ambiguous sequences
  cannot be be distinguished:


:::: fragment

"dog bites man" vs "man bites dog"

::::

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

Use an ambiguous example to point out that ffns can't distinguish
order of words; we explicitly want to model sequential dependencies

Example: "the boat is in the water" vs "the water is in the boat"

Alt example: "man bites dog" vs "dog bites man" [cite:@zhang2021dive, 8.1]

Emphasize fact that any prediction is based only on the current input

::::

** Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-4
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-4
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=both] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {}

Assume multiple time points.

- Time points are modelled *individually* ( $\hat{Y}_t = f(X_t)$ )

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::

** Feed forward network implementation to sequential data

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-5
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-5
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {}

Assume multiple time points.

- Time points are modelled *individually* ( $\hat{Y}_t = f(X_t)$ )
- Also want dependency on *previous* inputs ( $\hat{Y}_t = f(..., X_2, X_1)$ )

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

Emphasize fact that any prediction is based only on the current input

Also: the dependency on many previous variables motivates the
introduction of a latent variable model that depends on the previous
state via a hidden (latent) variable

::::

** Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-arr-1
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-arr-1
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
\end{tikzpicture}
#+end_src

::::

:::: {}



::::

:::: {}

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

We want to model dependencies over time. Solution is to model the cell
state (a hidden state) and pass this information on to the next 

::::

** Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-arr-2
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-arr-2
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::

** Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-arr-3
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px black solid; "}

#+name: ffn-x0-xt-arr-3
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \end{scope}
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture}
#+end_src

::::

:::: {}

::::

:::: {}

::::

:::::::::::::::::::

** Adding recurrence relations

::::::::::::::::::: {style="display: grid; grid-template-columns: 250px auto; grid-column-gap: 0px; grid-template-rows: 350px auto; grid-row-gap: 0px; justify-items: stretch;"}

:::: {}

#+name: ffn-xt-arr-4
#+begin_src tikz :ravel fig.ext="svg", out.height="300px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (\rnnouterwidth, \rnnouterheight + \ionodesize);
  \pic[\rnntikzbasekey/.cd, add labels=true, folded=true] {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
\end{tikzpicture}
#+end_src

::::

:::: {style="border-left: 2px white solid; "}

#+name: ffn-x0-xt-arr-4
#+begin_src tikz :ravel fig.ext="svg", out.height="300px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (6*\rnnouterwidth, \rnnouterheight + \ionodesize);
  \begin{scope}[xshift=0.2*\rnnioxshiftsmall]
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \end{scope}

  \node[font=\Huge, left of=x0_left, node distance=0.7*\ionodesize] {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  \draw[nnconarr] (x0_right) -- node[above] {$h_0$} (x1_left);
  \draw[nnconarr] (x1_right) -- node[above] {$h_1$} (x2_left);
  \draw[nnconarr] (x2_right) -- node[above] {$h_2, ..., h_{t-1}$} (xt_left);
\end{tikzpicture}
#+end_src

::::

:::: {}

Folded representation

::::

:::: {}

Unfolded representation

:::::: fragment

Add a /hidden state/ $h$ that introduces a dependency on the previous
step:
 
\[
\hat{Y}_t = f(X_t, h_{t-1})
\]

::::::

::::

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

$h_t$ is a summary of the inputs we've seen sofar

[cite:@zhang2021dive chapter 8.4]:

#+begin_quote
If we want to incorporate the possible effect of words earlier than
time step t−(n−1) on xt, we need to increase n. However, the number of
model parameters would also increase exponentially with it, as we need
to store |V|n numbers for a vocabulary set V. Hence, rather than
modeling P(xt∣xt−1,…,xt−n+1) it is preferable to use a latent variable
model:

P(xt∣xt−1,...,x1) ~ P(xt∣ht−1),
#+end_quote

IOW, with ht the recurrence becomes a latent variable model.

::::
** Why we need them and what they are                    :obsolete:noexport:

#+name: tikz-rnn-folded
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=16
\begin{tikzpicture}[thick]
  \def\sep{3 * \basenodesep}
  \def\width{6.5 * \sep}
  \def\height{\rnnouterheight}
  % \draw[white] (0,0) rectangle (\width, \height);
  \useasboundingbox (0, 0) rectangle (\width, height);
  \pic[\rnntikzbasekey/folded=true] (foldedrnn_) {rnnio};
\end{tikzpicture}
#+end_src

** Why we need them and what they are                    :obsolete:noexport:

#+name: tikz-rnn-folded-unfolded
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, fig.width=12
\begin{tikzpicture}[thick]
  \def\sep{3 * \basenodesep}
  \def\width{6.5 * \sep}
  \def\height{\rnnouterheight}
  \useasboundingbox (0, 0) rectangle (\width, height);
  \pic[\rnntikzbasekey/folded] (foldedrnn) {rnnio};
  \node[xshift=1.5*\sep, yshift=\height/2] (eq) {\Huge =};
  \pic[xshift=2*\sep] (x0) {rnnio={A}{$X_0$}{$H_0$}};
  \pic[xshift=3*\sep] (x1) {rnnio={A}{$X_1$}{$H_1$}};
  \pic[xshift=4*\sep] (x2) {rnnio={A}{$X_2$}{$H_2$}};
  \node[xshift=5*\sep] (dots) {\Huge \dots};
  \pic[xshift=5.5*\sep] (xt) {rnnio={A}{$X_t$}{$H_t$}};
  \draw[->] (x0_right) -- (x1_left);
  \draw[->] (x1_right) -- (x2_left);
  \draw[->] (x2_right) -- (xt_left);
\end{tikzpicture}
#+end_src

** Sequential memory of RNNs

RNNs have what one could call "sequential memory" cite:phi_illustrated_2020_RNN

*** Alphabet

Exercise: say alphabet in your head

#+begin_example
A B C ... X Y Z
#+end_example

:::: {.element: class="fragment"}

Modification: start from e.g. letter F

May take time to get started, but from there on it's easy

::::

:::: {.element: class="fragment"}

Now read the alphabet in reverse:


#+begin_example
Z Y X ... C B A
#+end_example


::::

:::: {.element: class="fragment"}

Memory access is associative and context-dependent

::::

*** Notes                                                          :ignore:
:::: {.notes}

Provide the alphabet example from cite:@phi_illustrated_2020_RNN

cf [cite:@haykin_neural_2010, p203]:
#+begin_quote
For a neural network to be dynamic, it must be given /short-term
memory/ in one form or the other
#+end_quote


::::
** Recurrent Neural Networks

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-rnn-folded-hidden-eq-1
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="400px"
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture}
#+end_src

:::: 

:::: {}

:::: {.element: class="fragment"}

Add recurrence relation where current hidden cell state $h_t$ depends
on input $x_t$ and previous hidden state $h_{t-1}$ via a function
$f_W$ that defines the network parameters (weights):

\[
h_t = f_\mathbf{W}(x_t, h_{t-1})
\]

::::

:::: {.element: class="fragment"}

Note that the same function and weights are used across all time
steps!

::::


:::: 

:::::::::::::::::::

** Recurrent Neural Networks - pseudocode

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-rnn-folded-hidden-eq-2
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="400px"
\begin{tikzpicture}[thick]
  \tikzset{nnlabel/.style={font=\bfseries\small\sffamily\sansmath}}
  \pic[\rnntikzbasekey/folded=true] {rnnio={RNN}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture}
#+end_src

:::: 

:::: {}

:::: {style="font-size: 0.8em"}

#+name: rnn-simple-pseudocode
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
class RNN:
    def __init__(self):
        # Initialize weights and cell state
        self._h = [...]
        self._Whh = [...]
        self._Wxh = [...]
        self._Why = [...]

    def update_cell_state(self, x):
        # function is some function that updates cell state
        self._h = function(self._h * self._Whh + x * self.Wxh)
    
    def predict(self):
        return self._h * self._Why

    def update_weights(self, y):
        # Calculate error via some loss function
        error = loss(self.predict() - y)
        # update weights via back propagation...

rnn = RNN()

for x, y in input_data:
    rnn.update_cell_state(x)
    rnn.update_weights(y)

# Retrieve next prediction
yhat = rnn.predict()
#+end_src

::::

:::: 

:::::::::::::::::::

*** Notes                         :ignore:
:::: {.notes}

Pseudocode examples:

[cite:@karpathy_unreasonable_effectiveness_of_RNNs]

#+begin_src jupyter-python
rnn = RNN()
y = rnn.step(x) # x is an input vector, y is the RNN's output vector

class RNN:
  # ...
  # Description of forward pass
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y

#+end_src
A two-layer network would look as follows:
#+begin_src jupyter-python
y1 = rnn.step(x)
y2 = rnn.step(y1)
#+end_src

Keras version (https://keras.io/api/layers/recurrent_layers/rnn/#rnn-class):
#+begin_src jupyter-python
class MinimalRNNCell(keras.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = backend.dot(inputs, self.kernel)
        output = h + backend.dot(prev_output, self.recurrent_kernel)
        return output, [output]
#+end_src

::::

** Vanilla RNNs

<br/>

::::::::::::::::::: {style="display: grid; grid-template-columns: 400px auto; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-vanilla-rnn-folded-hidden-eq-1
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="400px"
\begin{tikzpicture}[thick]
  \pic[\rnntikzbasekey/.cd, folded=true, add labels=true] {rnnio={tanh}{$X_t$}{$\hat{Y}_t$}};
\end{tikzpicture}
#+end_src

:::: 

:::: {}


:::: {.element: class="fragment" data-fragment-index="3"}

<h3 style="color: red;">Output vector</h3>

\[
\hat{Y}_t = \mathbf{W_{hy}^T}h_t
\]



::::

:::: {.element: class="fragment" data-fragment-index="2"}

<h3 style="color: green;">Update hidden state</h3>

\[
h_t = \mathsf{tanh}(\mathbf{W_{xh}^T}X_t + \mathbf{W_{hh}^T}h_{t-1})
\]

::::

:::: {.element: class="fragment" data-fragment-index="1"}

<h3 style="color: blue;">Input vector</h3>

\[
X_t
\]

::::


:::: 

::::::::::::::::::: 
** Vanilla RNNs

<br/>

cite:@olah_christopher_understanding_nodate

#+name: tikz-vanilla-rnn-unfolded-weights-1
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="1200px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3.1*\xd, yshift=0.6*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
\end{tikzpicture}
#+end_src


*** Notes                         :ignore:
:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

\[
h_t = f_W(x_t, h_{t-1})
\]

and point out that f, W are *shared* across all units

Add pseudocode to exemplify

::::

** Vanilla RNNs
<br/>

#+name: tikz-vanilla-rnn-unfolded-weights-2
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="1200px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
\end{tikzpicture}
#+end_src


*** Notes                         :ignore:
:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

\[
h_t = f_W(x_t, h_{t-1})
\]

and point out that f, W are *shared* across all units

Add pseudocode to exemplify

::::

** Vanilla RNNs
<br/>

#+name: tikz-vanilla-rnn-unfolded-weights-3
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="1200px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
\end{tikzpicture}
#+end_src


*** Notes                         :ignore:
:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

\[
h_t = f_W(x_t, h_{t-1})
\]

and point out that f, W are *shared* across all units
[cite:@lendave_vijaysinh_lstm_2021]

Add pseudocode to exemplify

::::


** Vanilla RNNs
<br/>
#+name: tikz-vanilla-rnn-unfolded-weights-4
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="1200px"
\begin{tikzpicture}
  \useasboundingbox (0, -\ionodesize) rectangle (3.2*\RNNouterwidth, \RNNioouterheight);
  \def\xd{0.98*\RNNouterwidth}
  \pic[xshift=\xd] (r) {RNNio};
  \pic[xshift=0] (rl) {RNNio={A}{$X_{t-1}$}{$\widehat{Y}_{t-1}$}};
  \pic[xshift=2*\xd] (rr) {RNNio={A}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=3*\xd, yshift=-0.5*\xd, anchor=west, scale=1.5, transform shape, \rnntikzbasekey/rotate=true] {rnnlegend};
  \node[nncon, iolabel, anchor=west] at ($(r_xt) !.4! (r_xt |- r_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rl_xt) !.4! (rl_xt |- rl_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=west] at ($(rr_xt) !.4! (rr_xt |- rr_hin) $) {$\mathbf{W_{xh}}$};
  \node[nncon, iolabel, anchor=south] at (r_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rl_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=south] at (rr_cout) {$\mathbf{W_{hh}}$};
  \node[nncon, iolabel, anchor=north west] at (r_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rl_yt.south) {$\mathbf{W_{hy}}$};
  \node[nncon, iolabel, anchor=north west] at (rr_yt.south) {$\mathbf{W_{hy}}$};
\end{tikzpicture}
#+end_src

:::: {.element: class="fragment"}

Note: $\mathbf{W_{xh}}$, $\mathbf{W_{hh}}$, and $\mathbf{W_{hy}}$ are
shared across all cells!

::::
*** Notes                                                          :ignore:
:::: {.notes}

From MIT lecture: use the folded version and incrementally reveal the equation

\[
h_t = f_W(x_t, h_{t-1})
\]

and point out that f, W are *shared* across all units

Add pseudocode to exemplify

::::

** Desired features of RNN

<div class="fragment">
<h3>1. Variable sequence lengths</h3>

Not all inputs are of equal length
<br/>
</div>

<div class="fragment">
<h3>2. Long-term memory</h3>

"I grew up in England, and ... I speak fluent English"
<br/>
</div>

<div class="fragment">
<h3>3. Preserve order</h3>
"dog bites man" != "man bites dog"
<br/>
</div>

<div class="fragment">
<h3>4. Share parameters</h3>

Adresses points 2 and 3.
<br/>
</div>


*** Notes                         :ignore:
:::: {.notes}

- variable sequence lengths


From [cite:@cho_learning_2014]:
#+begin_quote
architectur that learns to /encode/ a variable-length sequence into a
fixed-length vector representation and to /decode/ a given
fixed-length representation back into a variable-length sequence
#+end_quote

::::

** Example: Box & Jenkins airline passenger data set

#+name: box-jenkins-airline-1
#+begin_src jupyter-python :ravel echo=FALSE, fig.ext="png", out.width="500px"
df = rnnutils.airlines()
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
rnnutils.plt.show()
#+end_src
cite:onnen_temporal_2021
*** Notes                         :ignore:
:::: {.notes}

[cite:@onnen_temporal_2021]

See also
https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/
for rnn example on sunspots

Important: need to explicitly show how data is partitioned as this can
be difficult to understand

https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/

Herzen article on darts:
https://medium.com/unit8-machine-learning-publication/training-forecasting-models-on-multiple-time-series-with-darts-dc4be70b1844
::::
** Example: generate test and training data
#+name: box-jenkins-airline-2
#+begin_src jupyter-python :ravel echo=FALSE, fig.ext="png", out.width="500px"
fig, ax = rnnutils.plt.subplots()
p = ax.plot(df.time, df.passengers)
p = ax.plot(df.time[100:144], df.passengers[100:144], color="red")
p = ax.legend(["train", "test"])
rnnutils.plt.show()
#+end_src
Partition time series into training and test data sets at an e.g. 2:1
ratio:

#+name: box-jenkins-airline-partition-data
#+begin_src jupyter-python :ravel echo=TRUE, eval=TRUE
import rnnutils
import numpy as np
df = rnnutils.airlines()
data = np.array(df['passengers'].values.astype('float32')).reshape(-1, 1)
train, test, scaler = rnnutils.make_train_test(data)
#+end_src
** Example: prepare data for keras
#+name: tikz-prepare-airline-data-for-keras
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.width="800px"
\begin{tikzpicture}[node distance=2cm, align=center, >=latex]
  \node (data) {data = [0, 10, 20, 30, 40, 50, 60, 70]};
  \node[below left of=data, text width=1cm, rectangle, draw=black, node distance=4cm] (t) {t=0,1 t=2,3 t=4,5};
  \node[right of=t, text width=1cm, rectangle, draw=black] (x) {0, 10  20, 30 40, 50};
  \node[right of=x, text width=0.8cm, rectangle, draw=black] (t2) {t=2 t=4 t=6};
  \node[right of=t2, text width=0.5cm, rectangle, draw=black] (y) {20 40 60};
  \node[above of=x, node distance=0.9cm] (xlab) {X};
  \node[above of=y, node distance=0.9cm] (ylab) {Y};
  \draw (xlab.north) edge["predict Y from X by row", ->, bend left] (ylab.north);
\end{tikzpicture}
#+end_src

:::: {.element: class="fragment"}

#+name: airline-example-makexy
#+begin_src jupyter-python :ravel echo=TRUE, eval=TRUE
time_steps = 12
trainX, trainY, trainX_indices, trainY_indices = rnnutils.make_xy(train, time_steps)
testX, testY, testX_indices, testY_indices = rnnutils.make_xy(test, time_steps)
#+end_src

::::
** Example: create vanilla RNN model
#+name: airline-rnn-model
#+begin_src jupyter-python :ravel echo=TRUE, eval=TRUE
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN

model = Sequential()
model.add(SimpleRNN(units=3, input_shape=(time_steps, 1),
                    activation="tanh"))
model.add(Dense(units=1, activation="tanh"))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()
#+end_src

*** Notes                         :ignore:
:::: {.notes}

On RNN layers and time steps (https://keras.io/guides/working_with_rnns/):

- the units correspond to the output size (yhat)
- an RNN layer processes batches of input sequences
- an RNN layer loops RNN cells that process one input at a time (e.g.
  one word, one time point)

Also from the source code (LSTMCell):

#+begin_quote
  This class processes one step within the whole time sequence input, whereas
  `tf.keras.layer.LSTM` processes the whole sequence.
#+end_quote

The number of LSTMCells is defined by the units parameter, e.g.:

#+begin_src jupyter-python
# Stacked layer
rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
#+end_src
::::

** Example: fit the model and evaluate

:::: {style="font-size: 0.8em"}

#+name: airline-model-fit
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
history = model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)
Ytrainpred = model.predict(trainX)
Ytestpred = model.predict(testX)
#+end_src

::::

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% 50%; grid-column-gap: 10px; grid-row-gap: 0px"}

:::: {style="font-size: 0.8em"}

#+name: airline-plot-history-command
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
rnnutils.plot_history(history)
#+end_src


:::: 

:::: {style="font-size: 0.8em"}

#+name: airline-plot-model-fit-command
#+begin_src jupyter-python :ravel echo=TRUE, eval=FALSE
data = {'train': (model.predict(trainX), train, trainY_indices),
        'test': (model.predict(testX), test, testY_indices)}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20), labels=df.year[range(0, 144, 20)])
#+end_src

:::: 

:::: {}

#+name: airline-plot-history
#+begin_src jupyter-python :ravel echo=FALSE, eval=TRUE, fig.ext="png", out.width="80%"
d = {'loss': [0.15468591451644897, 0.04875592887401581,
              0.026622500270605087, 0.02037993259727955, 0.01717367395758629,
              0.01483415998518467, 0.013102399185299873, 0.011407987214624882,
              0.01016605831682682, 0.009000968188047409, 0.008080212399363518,
              0.007306812796741724, 0.006582655943930149, 0.005831228103488684,
              0.0052399300038814545, 0.004740677773952484, 0.004358494654297829,
              0.003976745996624231, 0.0037244476843625307, 0.003332830499857664]}
rnnutils.plot_history(d)
#+end_src

::::

:::: {}

#+name: airline-plot-model-fit
#+begin_src jupyter-python :ravel echo=FALSE, eval=TRUE, fig.ext="png", out.width="80%"
data = {"train": [np.array([[0.07300922274589539], [0.049322694540023804], [0.12263555824756622], [0.10771659016609192], [0.08678102493286133], [0.0807582437992096], [0.16047000885009766], [0.13826081156730652], [0.12276159226894379], [0.09296061843633652], [0.005158169195055962], [0.07526839524507523], [0.058306947350502014], [0.050476402044296265], [0.1301475316286087], [0.11820383369922638], [0.05991018936038017], [0.08855298906564713], [0.18303704261779785], [0.2147040069103241], [0.12630178034305573], [0.18395473062992096], [-0.05559101700782776], [0.12674248218536377], [0.13126012682914734], [0.07171593606472015], [0.2608124911785126], [0.1329176276922226], [0.22195808589458466], [0.16103020310401917], [0.2596116065979004], [0.2631970942020416], [0.222457155585289], [0.19196732342243195], [0.08305624127388], [0.13445650041103363], [0.23295335471630096], [0.12711040675640106], [0.2997269332408905], [0.19363194704055786], [0.16322757303714752], [0.29337525367736816], [0.2702861726284027], [0.3810942769050598], [0.24219587445259094], [0.29155629873275757], [0.1209971085190773], [0.22091759741306305], [0.31695908308029175], [0.14803078770637512], [0.37239915132522583], [0.33496564626693726], [0.3264579474925995], [0.3433589041233063], [0.32992351055145264], [0.41659021377563477], [0.2793201804161072], [0.3563149571418762], [0.11158497631549835], [0.229257732629776], [0.33076655864715576], [0.10553482174873352], [0.3575865626335144], [0.30193302035331726], [0.3180869221687317], [0.33221328258514404], [0.4005788266658783], [0.406070739030838], [0.3288029730319977], [0.34573444724082947], [0.18024373054504395], [0.2741532623767853], [0.3648494482040405], [0.25930526852607727], [0.37611520290374756], [0.36360082030296326], [0.3635270297527313], [0.3676840662956238], [0.43250808119773865], [0.41396385431289673], [0.37228304147720337], [0.38717105984687805], [0.23085232079029083], [0.3464904725551605]]), np.array([0.015444010496139526, 0.027027025818824768, 0.054054051637649536, 0.04826255142688751, 0.03281852602958679, 0.059845566749572754, 0.08494207262992859, 0.08494207262992859, 0.06177607178688049, 0.028957530856132507, 0.0, 0.027027025818824768, 0.021235525608062744, 0.042471036314964294, 0.0714285671710968, 0.059845566749572754, 0.04054054617881775, 0.08687257766723633, 0.12741312384605408, 0.12741312384605408, 0.1042470932006836, 0.055984556674957275, 0.019305020570755005, 0.06949806213378906, 0.07915058732032776, 0.08880308270454407, 0.1428571343421936, 0.11389961838722229, 0.13127413392066956, 0.1428571343421936, 0.18339768052101135, 0.18339768052101135, 0.15444016456604004, 0.11196911334991455, 0.0810810923576355, 0.11969110369682312, 0.12934362888336182, 0.14671814441680908, 0.1718146800994873, 0.14864864945411682, 0.1525096595287323, 0.22007721662521362, 0.2432432472705841, 0.2664092481136322, 0.20270270109176636, 0.16795367002487183, 0.13127413392066956, 0.17374518513679504, 0.17760616540908813, 0.17760616540908813, 0.25482624769210815, 0.2528957426548004, 0.24131274223327637, 0.26833975315093994, 0.3088802993297577, 0.3243243396282196, 0.2567567527294159, 0.20656371116638184, 0.14671814441680908, 0.18725869059562683, 0.19305017590522766, 0.1621621549129486, 0.2528957426548004, 0.2374517321586609, 0.2509652376174927, 0.3088802993297577, 0.38223937153816223, 0.36486485600471497, 0.2992278039455414, 0.24131274223327637, 0.1911197006702423, 0.24131274223327637, 0.2664092481136322, 0.24903473258018494, 0.3146717846393585, 0.318532794713974, 0.3204633295536041, 0.40733590722084045, 0.5019304752349854, 0.46911194920539856, 0.4015444219112396, 0.3281853497028351, 0.2567567527294159, 0.33590731024742126, 0.3474903404712677, 0.3339768350124359, 0.41119691729545593, 0.403474897146225, 0.4131273925304413, 0.521235466003418, 0.5965250730514526, 0.5810810327529907, 0.4845559895038605, 0.3899613916873932, 0.3223938047885895, 0.3899613916873932]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])], "test": [np.array([[0.41985487937927246], [0.2897593379020691], [0.43206876516342163], [0.3984266519546509], [0.39088255167007446], [0.43195009231567383], [0.4271005094051361], [0.42042967677116394], [0.37909582257270813], [0.3912075459957123], [0.33704182505607605], [0.35475632548332214], [0.43653494119644165], [0.28377869725227356], [0.4360058009624481], [0.35397568345069885], [0.3680872619152069], [0.4023270905017853], [0.3970481753349304], [0.40373513102531433], [0.3252890706062317], [0.37355899810791016], [0.2793232798576355], [0.31504255533218384], [0.38788625597953796], [0.25943249464035034], [0.414558470249176], [0.3525553047657013], [0.3922974467277527], [0.3646024167537689], [0.3950953483581543], [0.34701961278915405], [0.3233894407749176], [0.32918328046798706], [0.2957551181316376], [0.3380872905254364]]), np.array([0.40733590722084045, 0.3803088963031769, 0.4864864647388458, 0.4710424840450287, 0.4845559895038605, 0.6138995885848999, 0.6969112157821655, 0.7007721662521362, 0.5791505575180054, 0.46911194920539856, 0.38803085684776306, 0.4478764235973358, 0.4555984437465668, 0.4131273925304413, 0.49806949496269226, 0.4710424840450287, 0.4999999701976776, 0.6389961242675781, 0.747104287147522, 0.7741312980651855, 0.5791505575180054, 0.49227800965309143, 0.39768341183662415, 0.44980695843696594, 0.4942084848880768, 0.45945945382118225, 0.5830116271972656, 0.5637065172195435, 0.6100386381149292, 0.7104246616363525, 0.8571429252624512, 0.8783783912658691, 0.6930501461029053, 0.584942102432251, 0.49806949496269226, 0.5810810327529907, 0.6042470932006836, 0.5540540218353271, 0.6081080436706543, 0.6891891956329346, 0.7104246616363525, 0.832046389579773, 1.0, 0.9691120386123657, 0.7799227237701416, 0.6891891956329346, 0.5521235466003418, 0.6332045793533325]), np.array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])]}
rnnutils.plot_pred(data, scaler=scaler, ticks=range(0, 144, 20), labels=df.year[range(0, 144, 20)])
#+end_src


::::

::::::::::::::::::: 
** Exercise
See if you can improve the airline passenger model. Some things to
try:

- change the number of units
- change time_steps
- change the number of epochs

* Training
** Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-ffn-1
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
\end{tikzpicture}
#+end_src

:::: 

:::: {}


:::: 

::::::::::::::::::: 

*** Notes                                                          :ignore:
:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.


::::



** Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-ffn-2
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[->, thick] (start) -- (end);
\end{tikzpicture}
#+end_src

:::: 

:::: {}
<br/>
1. perform forward pass and generate prediction

:::: 

::::::::::::::::::: 

*** Notes                                                          :ignore:
:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.


::::


** Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-ffn-3
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
\end{tikzpicture}
#+end_src

:::: 

:::: {}
<br/>
1. perform forward pass and generate prediction
2. calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
   \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$

:::: 

::::::::::::::::::: 




*** Notes                                                          :ignore:
:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.


::::



** Recap: backpropagation algorithm in ffns

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-column-gap: 50px;"}

:::: {}

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-ffn-4
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}[rotate=90, transform shape]
  \useasboundingbox (-\basenodesep, -6.5*\basenodesep) rectangle (5*\basenodesep, 3*\basenodesep);
  \tikzset{
    iolabel/.append style={rotate=-90},
    nncon/.append style={->},
  }
  \pic[\rnntikzbasekey/.cd, boxed=true, dotted=true] (i) {nnlayer={6}{input}{}{}{blue}};
  \pic[xshift=2*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (h) {nnlayer={4}{hidden}{}{}{green}};
  \pic[xshift=4*\basenodesep, \rnntikzbasekey/.cd, boxed=true, dotted=true] (o) {nnlayer={3}{output}{}{}{red}};
  \pic {connectlayers={i}{h}{6}{4}};
  \pic {connectlayers={h}{o}{4}{3}};
  \node[below of=i_n6] (start) {};
  \node[right of=start, node distance=4*\basenodesep] (end) {};
  \draw[thick, ->] (start) -- (end);
  \node[below of=end] (bptt_start) {};
  \node[below of=start] (bptt_end) {};
  \draw[thick, ->, color=red] (bptt_start) -- (bptt_end);
\end{tikzpicture}
#+end_src

:::: 

:::: {}
<br/>

1. perform forward pass and generate prediction
2. calculate prediction error $\epsilon_i$ wrt (known) output: $\epsilon_i =
   \mathcal{L}(\hat{y}_i, y_i)$, loss function $\mathcal{L}$
3. back propagate errors and update weights to minimize loss

:::: 

::::::::::::::::::: 

*** Notes                                                          :ignore:
:::: {.notes}

Revise basic steps of training with incremental figure. Base on RNN
since that is what we are looking at but point out that this review is
general and applies also to ffns.


::::


** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-unfolded-1
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);
\end{tikzpicture}
#+end_src

** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-unfolded-2
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
\end{tikzpicture}
#+end_src



** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn


#+name: tikz-backpropagation-unfolded-3
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
  \end{scope}
\end{tikzpicture}
#+end_src

** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-unfolded-5
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\end{tikzpicture}
#+end_src

** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-unfolded-6
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
\end{scope}  

\end{tikzpicture}
#+end_src


** Backpropagation through time (BPTT)

cite:alexander_amini_mit_2021_rnn

#+name: tikz-backpropagation-unfolded-7
#+begin_src tikz :ravel fig.ext="svg", out.height="500px", out.width="100%"
\begin{tikzpicture}
  \useasboundingbox (-\ionodesize, -\ionodesize) rectangle (6*\rnnioxshiftsmall, 7*\basenodesep);  
  \tikzset{nnlabel/.style={font=\bfseries\normalsize\sffamily\sansmath}}
  \rnntikzset{folded=true}
  \pic (xfolded)  {rnnio={RNN}{$X_t$}{$Y_t$}};
  \rnntikzset{add weights=all, folded=false}
  \pic[xshift=1.5*\rnnioxshiftsmall] (x0) {rnnio={RNN}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=2.5*\rnnioxshiftsmall] (x1) {rnnio={RNN}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (x2) {rnnio={RNN}{$X_2$}{$\widehat{Y}_2$}};
  \rnntikzset{add weights=none, add weights/wxh=true, add weights/why=true}
  \pic[xshift=5*\rnnioxshiftsmall] (xt) {rnnio={RNN}{$X_t$}{$\widehat{Y}_t$}};

  \node[font=\Huge] at ($ (xfolded_output) !.5! (x0_input) $) {=};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};

  % rnn connections
  \tikzset{nnlabel/.style={font=\bfseries\scriptsize\sffamily\sansmath}}
  \draw[nnconarr] (x0_right) -- (x1_left);
  \draw[nnconarr] (x1_right) -- (x2_left);
  \draw[nnconarr] (x2_right) -- (xt_left);

  \node[loss, above of=x0_output] (l0) {$l_0$};
  \node[loss, above of=x1_output] (l1) {$l_1$};
  \node[loss, above of=x2_output] (l2) {$l_2$};
  \node[loss, above of=xt_output] (lt) {$l_t$};

  \draw[nnconarr] (x0_output) -- (l0);
  \draw[nnconarr] (x1_output) -- (l1);
  \draw[nnconarr] (x2_output) -- (l2);
  \draw[nnconarr] (xt_output) -- (lt);
  
  \node[loss, node distance=2*\basenodesep, above of=l2, minimum height=30pt, minimum width=30pt, font=\Large] (L) {$\mathcal{L}$};

  \begin{scope}[bend angle=5]
  \draw (l0) edge[->, bend left] (L);
  \draw (l1) edge[->, bend left] (L);
  \draw (l2) edge[->, bend left] (L);
  \draw (lt) edge [->, bend right] (L);
\end{scope}
  %% Reverse connections
  \begin{scope}[draw=red, bend angle=5]
  \draw (L) edge[->, bend left] (l0);
  \draw (L) edge[->, bend left] (l1);
  \draw (L) edge[->, bend left] (l2);
  \draw (L) edge [->, bend right] (lt);
\end{scope}
\begin{scope}[draw=red, bend angle=-10]
  \draw[nnconarr] (l0) edge[->, bend left] (x0_output);
  \draw[nnconarr] (l1) edge[->, bend left] (x1_output);
  \draw[nnconarr] (l2) edge[->, bend left] (x2_output);
  \draw[nnconarr] (lt) edge[->, bend left] (xt_output);
\end{scope}  
\begin{scope}[draw=red, bend angle=-10]
  \draw (x0_output) edge[->, bend left] (x0_center);
  \draw (x1_output) edge[->, bend left] (x1_center);
  \draw (x2_output) edge[->, bend left] (x2_center);
  \draw (xt_output) edge[->, bend left] (xt_center);
\end{scope}  
\begin{scope}[draw=red, bend angle=-20]
  \draw (x1_left) edge[->, bend left] (x0_right);
  \draw (x2_left) edge[->, bend left] (x1_right);
  \draw (xt_left) edge[->, bend left] (x2_right);
  \node[ultra thick, node distance=.7*\basenodesep, below right of=xt_input]  (gradstart) {};
  \node[ultra thick, node distance=.7*\basenodesep, below left of=x0_input]  (gradend) {};
  \draw (gradstart) edge [->] (gradend);
\end{scope}  

\end{tikzpicture}
#+end_src

Errors are propagated backwards in time from $t=t$ to $t=0$.

:::: {.element: class="fragment"}

Problem: calculating gradient may depend on large powers of
$\mathbf{W_{hh}}^{\mathsf{T}}$ (e.g. $\delta\mathcal{L} / \delta h_0
\sim f((\mathbf{W_{hh}}^{\mathsf{T}})^t)$


::::

*** Notes                         :ignore:
:::: {.notes}

Wording: gradient $(dL/dh_0) ~ f((W_hh^T)^t)$, i.e. gradient may depend on
large powers of $W^T_hh$. So gradient is $\propto a^t$, so if

(a > 1: exploding gradients; just mention here)

a < 1: vanishing gradients

This is problematic since the *size* of weight adjustments depend on
size of gradient

::::
** The effect of vanishing gradients on long-term memory



::::::::::::::::::: {style="display: grid; grid-template-columns: 30% auto; grid-column-gap: 0px; font-size: 0.8em;"}

:::: {}

:::: fragment

In layer $i$ gradient size ~ $(\mathbf{W_{hh}}^{\mathsf{T}})^{t-i}$

::::

:::: fragment

$\downarrow$

Weight adjustments depend on size of gradient

::::


:::: fragment

$\downarrow$

Early layers tend to "see" small gradients and do very little updating

::::

:::: fragment

$\downarrow$

Bias parameters to learn recent events 

::::

:::: fragment

$\downarrow$

RNN suffer short term memory

::::


:::: 

:::: {}

:::: {.element: class="fragment"}

cite:olah_christopher_understanding_nodate

"The clouds are in the ___"

::::

:::: {.element: class="fragment"}

#+name: tikz-clouds-are-in-the-sky
#+begin_src tikz :ravel fig.ext="svg", out.height="200px"
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3*\rnnioxshiftsmall] (x3) {rnnio={}{$X_3$}{$\widehat{Y}_3$}};
  \pic[xshift=4*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (x4) {rnnio={}{$X_4$}{$\widehat{Y}_4$}};
  \pic[xshift=5*\rnnioxshiftsmall] (x5) {rnnio={}{$X_5$}{$\widehat{Y}_5$}};
  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (x3_left)
  (x3_right) edge (x4_left)
  (x4_right) edge (x5_left);
\end{tikzpicture}
#+end_src

<br/>

::::

:::: {.element: class="fragment"}

"I grew up in England ... I speak fluent ___"

::::

:::: {.element: class="fragment"}

#+name: tikz-I-speak-fluent-english
#+begin_src tikz :ravel fig.ext="svg", out.height="200px"
\begin{tikzpicture}
  \pic (x0) {rnnio={}{$X_0$}{$\widehat{Y}_0$}};
  \pic[xshift=\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x1) {rnnio={}{$X_1$}{$\widehat{Y}_1$}};
  \pic[xshift=2*\rnnioxshiftsmall, \rnntikzbasekey/shade=input] (x2) {rnnio={}{$X_2$}{$\widehat{Y}_2$}};
  \pic[xshift=3.5*\rnnioxshiftsmall] (xt) {rnnio={}{$X_t$}{$\widehat{Y}_t$}};
  \node[font=\Huge] at ($ (x2_input) !.5! (xt_input) $) {$\dots$};
  
  \pic[xshift=4.5*\rnnioxshiftsmall, \rnntikzbasekey/shade=output] (xt1) {rnnio={}{$X_{t+1}$}{$\widehat{Y}_{t+1}$}};
  \pic[xshift=5.5*\rnnioxshiftsmall] (xt2) {rnnio={}{$X_{t+2}$}{$\widehat{Y}_{t+2}$}};

  \draw[->] (x0_right) edge (x1_left)
  (x1_right) edge (x2_left)
  (x2_right) edge (xt_left)
  (xt_right) edge (xt1_left)
  (xt1_right) edge (xt2_left);

\end{tikzpicture}
#+end_src

::::


:::: 

::::::::::::::::::: 

*** Notes                                                          :ignore:
:::: {.notes}

[cite:@aiml_vanishing_2018]

The bigger the gradient, the bigger the adjustment and *vice versa*.
Gradients are calculated wrt to effects of gradients in previous
layer. If those adjustments were small, gradients will be small, which
in time leads to exponentially declining values. Early layers fail to
do any learning.

In flowchart: *given* that W is smaller than one, the gradients tend
to vanish and be negligible for early layers

Examples: highlight the context dependency of the prediction. Sky is
easy to infer, but in the second example, if the intervening paragraph
is long, we need context from much farther back.

::::

** Solutions to vanishing gradient

::::::::::::::::::: {style="display: grid; grid-template-columns: 50% auto; grid-template-rows: auto auto auto; grid-row-gap: 0px; grid-column-gap: 0px;"}

# row 1
:::: {.element: class="fragment" data-fragment-index="1"}

1. Activation function

   ReLU (or leaky ReLU) instead of sigmoid or tanh
::::

:::: {.element: class="fragment" data-fragment-index="1"}

#+name: tikz-vanishing-gradient-trick-1
#+begin_src tikz :ravel fig.ext='svg', out.height="200px"
\begin{tikzpicture}
  \tikzset{declare function={
      sigma(\x)=1/(1+exp(-\x));
      sigmap(\x)=sigma(\x)*(1-sigma(\x));
      relu(\x)=x;
      tanhp(\x)=1-(exp(\x) - exp(-\x))^2/(exp(\x)+exp(-\x))^2;
    }
  }
  \pgfplotsset{every axis plot/.append style={thick}}
\begin{axis}%
  [
  grid=major,     
  xmin=-4,
  xmax=4,
  axis x line=bottom,
  ymax=1.1,
  ymin=-0.1,
  axis y line=middle,
  samples=100,
  domain=-4:4,
  legend style={at={(1,0.9)}},
  ytick=\empty
  ]
  \addplot[blue,mark=none]   (x,{sigmap(x)});
  \addplot[green, mark=none] (x, {tanhp(x)});
  \addplot[red,mark=none,domain=0:4]   (x,1);
  \addplot[red,mark=none,domain=-4:0]   (x, 0);
  \addplot +[red,mark=none] coordinates {(0, 0) (0, 1)};

  \legend{$\sigma'(x)$, $\mathsf{tanh}'(x)$, $\mathsf{ReLU}'$}
\end{axis}
\end{tikzpicture}
#+end_src

:::: 

# row 2
:::: {.element: class="fragment" data-fragment-index="2"}

1. [@2] Weight initialization

   Set bias=0, weights to identity matrix
   
::::

::::  {.element: class="fragment" data-fragment-index="2" }

#+name: tikz-vanishing-gradient-trick-2
#+begin_src tikz :ravel fig.ext='svg', out.height="200px"
\begin{tikzpicture}
  \matrix [matrix of math nodes,left delimiter=(,right delimiter=), anchor=west](W){ 
    1 & 0 & \dots  & 0\\
    0 & 0 & \dots  & 0\\
    \vdots  & \vdots  & \ddots & \vdots\\
    0 & 0 & \dots  & 1\\
  };
  \node [left of=W, anchor=east, node distance=1.5cm] {W =};

\end{tikzpicture}
#+end_src

:::: 


# row 3
:::: {.element: class="fragment" data-fragment-index="3"}

1. [@3] More complex cells using "gating"

   For example LSTM
::::


::::  {.element: class="fragment" data-fragment-index="3"}

#+name: tikz-vanishing-gradient-trick-3
#+begin_src tikz :ravel fig.ext='svg', out.height="200px"
\begin{tikzpicture}
  \pic {lstm};  
\end{tikzpicture}
#+end_src

:::: 


::::::::::::::::::: 

*** Notes                         :ignore:
:::: {.notes}

Note that ReLUs not used in LSTMs / GRU as ReLU is non-negative. The
tanh activation is needed so that values can be added *and*
subtracted. Sigmoid is in (0, 1).

::::

* LSTMs and GRUs
** Motivation behind LSTMs and GRUs

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

<h5 align="center">LSTM</h5>

#+name: tikz-lstm
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture}
#+end_src

::::

:::: {}

<h5 align="center">GRU</h5>

#+name: tikz-gru
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {gru};
\end{tikzpicture}
#+end_src

::::

:::::::::::::::::::


#+name: tikz-gru-lstm-legend
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\tikzset{legend/.style={
        font=\sffamily\bfseries\tiny,
        text width=1.4cm,
        align=center
        }
}
\begin{tikzpicture}[node distance=1cm]
\node[pwise=X, node distance=2cm, label={[legend]below:pointwise multiplication}] (pmult) {};
\node[tanh, left of=pmult, label={[legend]below:tanh}] (tanh) {};
\node[sigmoid, left of=tanh, label={[legend]below:sigmoid}] (sigmoid) {};
\node[pwise=+, right of=pmult, label={[legend]below:pointwise addition}] (padd) {};
\node[pwise={1-}, right of=padd, label={[legend]below:pointwise inversion}] (pinvert) {};
\node[vcon=1cm, right of=pinvert, label={[legend]below:vector concatenation}] (vconcat) {};
\node[vcopy=1cm, right of=vconcat, label={[legend]below:vector copy}] (vcopy) {};
\end{tikzpicture}
#+end_src

Long Short Term Memory (LSTM)  [cite:@hochreiter_long_1997] and Gated
Recurrent Unit (GRU) [cite:@cho_learning_2014] architectures were
proposed to solve the vanishing gradient problem.


*** Notes                                                          :ignore:

::: {.notes}

Based on [cite:@phi_illustrated_2020_lstm]

- solution to short-term memory
- gates *regulate* the flow of information, concentrating on the important parts

:::

** Intuition

:::: {.element: style="font-family: Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter, monospace; font-size: 0.8em;"}

In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN).
One RNN encodes a sequence of symbols into a fixed-length vector
representation, and the other decodes the representation into another
sequence of symbols. The encoder and decoder of the proposed model are
jointly trained to maximize the conditional probability of a target
sequence given a source sequence. The performance of a statistical
machine translation system is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
semantically and syntactically meaningful representation of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  [cite:@cho_learning_2014]

::::

*** Notes                         :ignore:
:::: {.notes}

[cite:@phi_illustrated_2020_lstm]

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

- solution to vanishing gradient problem
- gates regulate flow of information, focusing on the important parts

::::

** Intuition

:::: {.highlight}

In this paper, we propose a *novel neural network* model called *RNN*
*Encoder-Decoder* that consists of *two recurrent neural networks* (RNN).
One RNN *encodes* a *sequence of symbols* into a fixed-length vector
representation, and the other *decodes the representation* into another
sequence of symbols. The encoder and decoder of the proposed model are
*jointly trained* to maximize the conditional probability of a target
sequence given a source sequence. The performance of a *statistical*
*machine translation system* is empirically found to improve by using
the conditional probabilities of phrase pairs computed by the RNN
Encoder-Decoder as an additional feature in the existing log-linear
model. Qualitatively, we show that the proposed model learns a
*semantically and syntactically meaningful representation* of linguistic
phrases.

::::

:::: {.element: style="font-size: 0.6em;"}

Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation  [cite:@cho_learning_2014]

::::

<br/>

:::: {.element: class="fragment"}

Remember the important parts, pay less attention to (forget) the rest.

::::

*** Notes                         :ignore:
:::: {.notes}

[cite:@phi_illustrated_2020_lstm]

Example: provide long text (e.g. customer) review and point out what
we most likely will remember the following day. Intuition on LSTM/GRU:
focus on relevant information.

Intuition:

- solution to vanishing gradient problem
- gates regulate flow of information, focusing on the important parts

::::

** LSTM: Cell state flow and gating

cite:olah_christopher_understanding_nodate

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-cellstateflow
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.height="300px"
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=cellstateflow] {lstm} ;
\end{tikzpicture}
#+end_src

Information flows in the cell state from $c_{t-1}$ to $c_t$.

::::

:::: {.element: class="fragment"}

#+name: tikz-lstmgate
#+begin_src tikz :ravel fig.ext="svg", cache=FALSE, out.height="300px"
\begin{tikzpicture}
  \pic {lstmgate} ;
\end{tikzpicture}
#+end_src

Gates affect the amount of information let through. The sigmoid layer
outputs anything from 0 (nothing) to 1 (everything).

::::

:::::::::::::::::::

:::: {.element: class="fragment" style="font-size: 0.8em"}

cite:cho_learning_2014
#+begin_quote
In our preliminary experiments, we found that it is crucial to use
this new unit with gating units. We were not able to get meaningful
result with an oft-used tanh unit without any gating.
#+end_quote

::::

*** Notes                         :ignore:
:::: {.notes}

[cite:@olah_christopher_understanding_nodate]

[cite:@cho_learning_2014, p1726] on the hidden unit:
#+begin_quote
In our preliminary experiments, we found that it is crucial to use
this new unit with gating units. We were not able to get meaningful
result with an oft-used tanh unit without any gating.
#+end_quote


::::
** Forget, input, and output gates

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

<h5>forget gate</h5>

#+name: tikz-lstm-forget-gate-only
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=forgetgate] {lstm};
\end{tikzpicture}
#+end_src

*Purpose:* reset content of cell

::::

:::: {}

<h5>input gate</h5>

#+name: tikz-lstm-input-gate-only
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=inputgate] {lstm};
\end{tikzpicture}
#+end_src

*Purpose:* decide when to read data into cell
::::

:::: {}

<h5>output gate</h5>

#+name: tikz-lstm-output-gate-only
#+begin_src tikz :ravel fig.ext="svg"
\begin{tikzpicture}
  \pic[\rnntikzbasekey/highlight=outputgate] {lstm};
\end{tikzpicture}
#+end_src

*Purpose:* read entries from cell

::::

:::::::::::::::::::

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where anything from no information (0) to all information (1) passes through the gate.

** The forget gate
#+name: tikz-lstm-forget-gate
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
\pic[\rnntikzbasekey/.cd, highlight=forgetgate] {lstm};
\end{tikzpicture}
#+end_src

*Purpose*: decide what information to keep or throw away

Sigmoid squishes vector $[\boldsymbol{h_{t-1}}, \boldsymbol{x_t}]$
(previous hidden state + input) to $(0, 1)$, where 0=forget, 1=keep.

:::: {.element: class="fragment"}

\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\]

::::

** Add new information - the input gate

#+name: tikz-lstm-candidatecellstate-input
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=inputgate] {lstm};
\end{tikzpicture}
#+end_src

Two steps to adding new information:

1. sigmoid layer decides which values to update


** Add new information - get candidate values

#+name: tikz-lstm-candidatecellstate-input-1
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=candidatecellstate] {lstm};
\end{tikzpicture}
#+end_src

Two steps to adding new information:

1. sigmoid layer decides which values to update
2. tanh layer creates vector of new candidate values $\tilde{c}_t$

:::: {.element: class="fragment"}

\[
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)
\]

::::


** Updating the cell state
#+name: tikz-lstm-update-cell-state
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=updatecellstate] {lstm};
\end{tikzpicture}
#+end_src

1. multiply old cell state by $f_t$ to forget what was decided to
   forget
2. add $i_t * \tilde{c}_t$


:::: {.element: class="fragment"}

\[
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t
\]

::::


** Cell output

#+name: tikz-lstm-output-gate
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic[\rnntikzbasekey/.cd, highlight=celloutput] {lstm};
\end{tikzpicture}
#+end_src
Output is filtered version of cell state.

1. sigmoid output gate decides what parts of cell state to output
2. push cell state through tanh and multiply by sigmoid output


:::: {.element: class="fragment"}

\[
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
\]

::::


** LSTM: putting it together

#+name: tikz-lstm-intuition
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture}
#+end_src

*** Intuition
- if forget ~ 1, input ~ 0, $c_{t-1}$ will be saved over time

*** Notes                                                          :ignore:
:::: {.notes}

From [cite:@zhang2021dive, 9.2.1.3]:

- if forget ~ 1, input ~ 0, C_{t-1} will be saved over time

::::
** LSTM: putting it together

cite:zhang2021dive

::::::::::::::::::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

:::: {}

#+name: tikz-lstm-2
#+begin_src tikz :ravel cache=FALSE, fig.ext="svg", fig.width=5
\begin{tikzpicture}
  \pic {lstm};
\end{tikzpicture}
#+end_src


::::

:::: {}

\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\\
i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{c}_t = \mathsf{tanh}(W_c \cdot [h_{t-1}, x_t] + b_c)\\
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t\\
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\\
h_t = o_t * \mathsf{tanh}(c_t)
\]


::::

:::::::::::::::::::

\[
x_t \in \mathbb{R}^{n\times d}, h_{t-1} \in \mathbb{n \times h},
i_t \in \mathbb{R}^{n\times h}, f_t \in \mathbb{R}^{n\times h}, o_t \in \mathbb{R}^{n\times h},
\]

and

\[
W_f \in \mathbb{R}^{n \times (h+d)}, W_i \in \mathbb{R}^{n \times (h+d)}, W_o \in \mathbb{R}^{n \times (h+d)}, W_c \in \mathbb{R}^{n \times (h+d)}
\]

** GRU
#+name: tikz-big-gru
#+begin_src tikz :ravel fig.ext="svg", out.width="80%"
\begin{tikzpicture}
\pic {gru};  
\end{tikzpicture}
#+end_src

- forget and input states combined to single /update/ gate
- merge cell and hidden state
- simpler model than LSTM

*** Notes                         :ignore:
:::: {.notes}

[cite:@olah_christopher_understanding_nodate]

Notes that GRU has been gaining traction lately (where lately=2015!)

Comparison [cite:@lendave_vijaysinh_lstm_2021]:

- GRU has fewer parameters so uses less memory and executes faster
- LSTM more accurate on larger datasets



::::

** Exercise
*** 1. Analyse airline passengers with LSTM
Modify the airline passenger model to use an LSTM and compare the
results. Try out different parameters to improve test predictions.

*** 2. Time-series forecasting with LSTM, discrete state space
LSTM with Variable Length Input Sequences to One Character Output

*** 3. Estimation of recombination rate using GRU units {.element class="fragment" data-fragment-index="1"} :noexport:

:::: {.element: class="fragment" data-fragment-index="1"}

If time permits look at estimation of recombination rates using
simulated data.

::::

*** Notes                         :ignore:
:::: {.notes}

Good example at [cite:@koehrsen_recurrent_2018]

::::

** Time-series forecasting with LSTM, discrete state space

*** Objective
Predict next character in sequence of strings

*** Comments
- you may have to use several LSTM layers, in which the last layer
  should not return sequences (set =return_sequences=False=)
  cite:brownlee_stacked_2017

* Applications                                                     :noexport:
** Google translate
feels like one of the more obvious language applications that people
use in everyday life
** Time series
** Attention networks
Mention attention networks as a next step generalisation?
** Recombination rate estimation in genomics
segway to practical

* Bibliography                                                       :ignore:

** Bibliography
<div id="refs" class="references hanging-indent" role="doc-bibliography" style="font-size: 60%;"></div>
