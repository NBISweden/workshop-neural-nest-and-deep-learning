{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<!--<a href=\"https://colab.research.google.com/github/mkierczak/autoencoders_workshop/blob/main/PCA2VAE_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBE-XinUP2SY"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "First, we need to import appropriate libraries that we will use throughout the workshop.\n",
        "\n",
        "Note! Some of the libraries below are not directly used in the code below but will be necessary once you start experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fLVD-Iuaojq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "from keras import backend\n",
        "from sklearn.decomposition import PCA\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNH1c7NSwCT7"
      },
      "source": [
        "# Data\n",
        "\n",
        "We will be working on a dataset that I have created from HapMap phase 3 project. In HapMap, several individuals have been genotyped that come from different parts of the world and from different ethnic backgrounds. For the sake of simplicity, I have **randomly selected 5000 autosomal** markers that we will be working with and saved them along with phenotype data (biological sex, ethnicity etc.) and a pre-computed genomic kinship matrix in the *hdf5* file format.\n",
        "\n",
        "If you want to try another dataset, you can go for e.g., data from Lazaridis et. al. [*Genomic insights into the origin of farming in the ancient Near East*](https://www.nature.com/articles/nature19310) 2016. Nature **536**:419-424. I have pre-prepared chr1 markers at call rate of 0.99 and higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfrkMEPwIL6",
        "outputId": "41682d75-b16d-426b-d1a8-3757d5a527ac"
      },
      "outputs": [],
      "source": [
        "# HapMap3 randomly selected 5000 autosomal markers data\n",
        "!wget -O data.hd5 https://www.dropbox.com/scl/fi/2daedhwkdjweotnthxee3/HapMap3_5000.h5?rlkey=nz8f9df7tt9n0hrpqg7y19omc&dl=1\n",
        "\n",
        "# Data for chr1 from Lazaridis et al.\n",
        "#!wget -O data.hd5 https://www.dropbox.com/scl/fi/kck4puyi1qmuzr65bgdbn/HumanOriginsPublic2068_geno_chr1.h5?rlkey=xp4nfljz0c2za9ihriletxx3x&dl=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqmjLTgqaWTj"
      },
      "outputs": [],
      "source": [
        "# Extract genotypes, phenotypes and genomic kinship matrix from hdf5 file\n",
        "orig_geno = pd.read_hdf('data.hd5', key = 'geno')\n",
        "orig_pheno = pd.read_hdf('data.hd5', key = 'pheno')\n",
        "orig_gkin = pd.read_hdf('data.hd5', key = 'gkin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "bcrBRSgUbVU0",
        "outputId": "57f722c7-8a21-4e70-b88c-c6bf07ab4faf"
      },
      "outputs": [],
      "source": [
        "# Let's have a look at the genotypes data\n",
        "orig_geno.info()\n",
        "orig_geno.iloc[0:4, 0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "_ZPIQ8LmFy_U",
        "outputId": "a7fe1e45-6f36-442c-d3a2-237995d5121f"
      },
      "outputs": [],
      "source": [
        "# Examine phenotypes\n",
        "orig_pheno.iloc[0:4, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "SzhJ02FKF1aM",
        "outputId": "cee5fea0-420e-48ed-b413-0489cad23f9e"
      },
      "outputs": [],
      "source": [
        "# Look at the genomic kinship matrix\n",
        "orig_gkin.iloc[0:4, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dkrQqTUc11Z",
        "outputId": "a35768c6-44f2-4378-f4d4-b1286d5d1646"
      },
      "outputs": [],
      "source": [
        "# Check whether we have missing genotypes in our data\n",
        "print(\"Missing genotypes per marker: \\n\", orig_geno.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Yd7KEcR4k6"
      },
      "source": [
        "Since neural networks require numeric values as input, we do not need to do anything -- our genotypes are already encoded as minor allele counts. However, we know that it is best for the networks to get input from the range [0, 1], thus we shall re-code our genotypes so that they are bound between zero and one. \n",
        "\n",
        "Now, one of the alleles is encoded as 0, meaning that for this particular genotype the bias term can take over the weight (y = 0*w_0 + b). While true in previous implementations of tensorflow, this \"bias hitchhiking\" is no longer a concern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "ZKVsSINldHiW",
        "outputId": "8dd5a52d-b1c1-4041-fe36-1ce249d3d856"
      },
      "outputs": [],
      "source": [
        "geno = orig_geno.replace([0, 1.0, 2.0], [0, 0.5, 1.0])\n",
        "#geno.fillna(0, inplace = True)\n",
        "#print(geno.isna().sum())\n",
        "geno\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzfcN2jCSdVe"
      },
      "source": [
        "# Training Phase\n",
        "\n",
        "Now, we will follow some best machine learning practises and randomly split our data into two sets: the training set and the test set. The training set will be used to train our neural network, i.e. to optimize network weights while the latter will be set aside so that it is NEVER used in the training phase. This set will be used to objectively assess performance of our model at the very end.\n",
        "\n",
        "## Question\n",
        "\n",
        "Do you think that this random split of the original dataset is good enough?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6-1i5-TdVSZ",
        "outputId": "9d189a91-64f0-4ce0-ccd4-002d02b3c19f"
      },
      "outputs": [],
      "source": [
        "train = geno.sample(frac = 0.8, random_state = 42) # 80% of our individuals will go to the training set\n",
        "test = geno.drop(train.index)\n",
        "pheno = orig_pheno.set_index(keys = 'id')\n",
        "train_pheno = pheno[pheno.index.isin(train.index)]\n",
        "test_pheno = pheno.drop(train.index)\n",
        "train.reset_index()\n",
        "test.reset_index()\n",
        "train_pheno.reset_index()\n",
        "test_pheno.reset_index()\n",
        "\n",
        "# NOTE! However, we start without proper test set and will entrust internal validation.\n",
        "# Why? Well, we do not have so many individuals to spare for the test set.\n",
        "train = geno\n",
        "\n",
        "# Print some info about the resulting split\n",
        "print(\"Original data:\", orig_geno.shape)\n",
        "print(\"\\t - training set:\", train.shape)\n",
        "print(\"\\t - test set:\", test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfkQQ8_zTodM"
      },
      "source": [
        "# PCA\n",
        "\n",
        "Before we even start training our autoencoder model, it'd be good to have some benchmark to compare to. Below, we will do a very simple PCA on raw genotypes, not even making use of the kinship matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "FASaLyJGfnNH",
        "outputId": "af045589-129b-4383-9b76-d60717f3f1df"
      },
      "outputs": [],
      "source": [
        "embedding = PCA(n_components=2)\n",
        "pca_embedding = embedding.fit_transform(geno)\n",
        "x = pca_embedding[:,0]\n",
        "y = pca_embedding[:,1]\n",
        "pop = pheno.iloc[:,5]\n",
        "data = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (9,9))\n",
        "sns.scatterplot(x='x', y='y', data=data, hue='pop', style='pop', s=100, legend=True)\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q1-9HLBT-TA"
      },
      "source": [
        "As you can see, simple PCA separates our individuals into a couple of clear clusters: Africa in the East part of the plot, China and Japa in the NW part and ethnicities of mostly European descent end up in the SW part. While it is easy for PCA to separate Maasai from Yoruba, people of Chinese descent cannot be easily told apart from Japanese using this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvd8IAqyoEmn",
        "outputId": "7c0cc8a9-18a2-4963-a6ac-e9d193c1cbef"
      },
      "outputs": [],
      "source": [
        "# For Keras framework, we need to convert our data types a bit\n",
        "train_tensor = train.to_numpy()\n",
        "test_tensor = test.to_numpy()\n",
        "original_dim = train_tensor.shape[1]\n",
        "latent_dim = 2 # dimentionality of our latent space\n",
        "print(type(train_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF0eTuYzVVWU"
      },
      "source": [
        "# Autoencoder Model\n",
        "\n",
        "Below, we will build our autoencoder model using functional interface provided by the Keras framework. Keras, in turn, will talk to Tensorflow framework that enables us to build, train and use neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CJGgCpAooiG"
      },
      "outputs": [],
      "source": [
        "input = keras.Input(shape = (original_dim,))\n",
        "output = input\n",
        "\n",
        "h = layers.Dense(units = 500, activation = 'relu')(input)\n",
        "h = layers.BatchNormalization()(h)\n",
        "h = layers.Dropout(rate = 0.05)(h)\n",
        "h = layers.Dense(units = 250, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 25, activation = 'relu')(h)\n",
        "latent = layers.Dense(units = latent_dim, name = 'latent')(h)\n",
        "encoder = keras.Model(input, latent, name='encoder')\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "h = layers.Dense(units = 25, activation = 'relu')(latent_inputs)\n",
        "h = layers.Dense(units = 250, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 500, activation = 'relu')(h)\n",
        "dec_output = layers.Dense(original_dim, activation='sigmoid')(h)\n",
        "decoder = keras.Model(latent_inputs, dec_output, name='decoder')\n",
        "\n",
        "output = decoder(encoder(input))\n",
        "ae_model = keras.Model(input, output, name='AE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kP0f_4ubUFz"
      },
      "source": [
        "Keras.utils provides a very convenient way of visualizing models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "lOnDhZF-spYl",
        "outputId": "f334ea2b-383d-445f-adae-64256954f9d8"
      },
      "outputs": [],
      "source": [
        "plot_model(ae_model, show_shapes=True, expand_nested = True, dpi=58)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YheGNtA5bcMx"
      },
      "source": [
        "Once we have checked that the model looks as we wanted it to look, we can compile it. We will change two hyperparameters of the model from their defaults:\n",
        "\n",
        "*   we set custom loss function. Here, we want our loss function to measure how far the decoded points are from where they are in input. Ideal autoencoder should perfectly reproduce the input on the output,\n",
        "*   we also use ADAM as our optimizer. Without going into details, this should be your off-the-shelf optimizer in most cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcKU4MicvCo6",
        "outputId": "1347bd52-06f4-4b29-fb14-45e66afe1f0e"
      },
      "outputs": [],
      "source": [
        "hp_loss_fn = keras.losses.MeanSquaredError()\n",
        "hp_optimizer = 'adam'\n",
        "ae_model.compile(\n",
        "  loss = hp_loss_fn,\n",
        "  optimizer = hp_optimizer\n",
        ")\n",
        "ae_model.summary(expand_nested = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJtsgi4cPoF"
      },
      "source": [
        "Finally, we can train our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnVnSrLvaBd",
        "outputId": "91a3048b-3de4-4169-dd6a-5448d7b1ad4a"
      },
      "outputs": [],
      "source": [
        "hp_epochs = 30\n",
        "hp_batch_size = 64\n",
        "hp_val_split = 0.2\n",
        "\n",
        "autoencoder = ae_model.fit(\n",
        "                      x = train_tensor,\n",
        "                      y = train_tensor,\n",
        "                      epochs = hp_epochs,\n",
        "                      batch_size = hp_batch_size,\n",
        "                      validation_split = hp_val_split,\n",
        "                      verbose = 2\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adilYTghcWYZ"
      },
      "source": [
        "Once the model is trained, we can have a look at its performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "JNH6TQbtzVy9",
        "outputId": "a0f17098-ca70-4018-d2cc-308857473cde"
      },
      "outputs": [],
      "source": [
        "loss = autoencoder.history['loss']\n",
        "val_loss = autoencoder.history['val_loss']\n",
        "epochs = range(hp_epochs)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'ro', label='Validation loss')\n",
        "plt.title('Training and validation loss (MSE)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gm5pK2WcyV8"
      },
      "source": [
        "All right, so now our model has been trained and we need to visualize our data in low dimension by using neurons in from the bottleneck layer. The easiest way is to extract the whole trained encoder and run our input through it once again to get embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKFV8beu0PZi",
        "outputId": "a9f40b12-d241-474f-f3c5-f0f64b7c19c7"
      },
      "outputs": [],
      "source": [
        "geno_tensor = geno.to_numpy()\n",
        "geno_dim = geno_tensor.shape[1]\n",
        "input = keras.Input(shape = (geno_dim,))\n",
        "\n",
        "trained_encoder = keras.Model(ae_model.input, ae_model.layers[1].get_layer(\"latent\").output)\n",
        "trained_encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "Xb8USwL91_V4",
        "outputId": "02cf41c9-70e1-42dc-89a3-ef23a7939c4b"
      },
      "outputs": [],
      "source": [
        "embedded_points = trained_encoder.predict(geno_tensor)\n",
        "print(embedded_points)\n",
        "\n",
        "x = embedded_points[:,0]\n",
        "y = embedded_points[:,1]\n",
        "pop = pheno.iloc[:,5]\n",
        "data = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.scatterplot(x='x', y='y', data=data, hue='pop', style='pop', s=100)\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2\n",
        "Try to see what happens if you add an extra layer with, say 1500 units to the model. What are pros and what are potential cons of such approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d-ASNR_dAoC"
      },
      "source": [
        "# VAE\n",
        "\n",
        "Now it is, perhaps, time to go generative and build a variational variant of our autoencoder.\n",
        "\n",
        "We will need a special layer here, the so-called sampling layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMpPWJmLMqT6"
      },
      "outputs": [],
      "source": [
        "def sampling(args):\n",
        "    z_mean, z_log_sigma = args\n",
        "    #epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
        "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
        "    return z_mean + tf.exp(z_log_sigma) * epsilon\n",
        "\n",
        "# Encoder\n",
        "input = keras.Input(shape = (original_dim,))\n",
        "\n",
        "h = layers.Dense(units = 1500, activation = 'relu')(input)\n",
        "h = layers.BatchNormalization()(h)\n",
        "h = layers.Dense(units = 500, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 250, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 25, activation = 'relu')(h)\n",
        "# Bottleneck\n",
        "z_mean = layers.Dense(latent_dim, name = 'z_mean')(h)\n",
        "z_log_sigma = layers.Dense(latent_dim, name = 'z_sigma')(h)\n",
        "# Lambda layer with specified output shape\n",
        "z_sampling = layers.Lambda(sampling, name='z_sampling', output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
        "\n",
        "encoder = keras.Model(input, [z_mean, z_log_sigma, z_sampling], name='encoder')\n",
        "\n",
        "# Decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
        "h = layers.Dense(units = 25, activation = 'relu')(latent_inputs)\n",
        "h = layers.Dense(units = 250, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 500, activation = 'relu')(h)\n",
        "h = layers.Dense(units = 1500, activation = 'relu')(h)\n",
        "dec_output = layers.Dense(original_dim, activation='sigmoid')(h)\n",
        "decoder = keras.Model(latent_inputs, dec_output, name='decoder')\n",
        "\n",
        "# instantiate VAE model\n",
        "output = decoder(encoder(input)[2])\n",
        "vae_model = keras.Model(input, output, name='VAE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vvkuTUnhUel"
      },
      "source": [
        "We also need a custom loss function that has two constraints:\n",
        "* reconstructed space have to be as close to input as possible (like in vanilla autoencoder),\n",
        "* points in our latent points have to be distributed as close to a given distribution (here random normal) as possible (this is what KL divergence measures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NxUFYgNOXtF"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss in a custom layer\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_sigma, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        \n",
        "        # Reconstruction loss\n",
        "        reconstruction_loss = keras.losses.MeanSquaredError()(inputs, reconstructed) * original_dim\n",
        "        \n",
        "        # KL divergence loss\n",
        "        kl_loss = 1 + z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
        "        \n",
        "        # Add losses to the model\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "vae_model = VAE(encoder, decoder)\n",
        "vae_model.compile(optimizer='adam')\n",
        "vae_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "Dh4lEzb5T2F7",
        "outputId": "d4b9f93c-3459-440c-90ad-e7d16ddd589f"
      },
      "outputs": [],
      "source": [
        "plot_model(encoder, show_shapes=True, show_layer_names = True, dpi = 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "n7ic0XYeUk2d",
        "outputId": "aa8c6657-d9cd-4fbf-d662-98dd9d5496c4"
      },
      "outputs": [],
      "source": [
        "plot_model(decoder, show_shapes=True, expand_nested = True, dpi = 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ0pJL5RU0tE",
        "outputId": "38656fb3-da74-4668-8e4a-52a61fba62a8"
      },
      "outputs": [],
      "source": [
        "hp_epochs = 30\n",
        "hp_batch_size = 64\n",
        "hp_val_split = 0.2\n",
        "\n",
        "vae_model.fit(train_tensor, train_tensor,\n",
        "        epochs = hp_epochs,\n",
        "        batch_size = hp_batch_size,\n",
        "        shuffle = True,\n",
        "        validation_split = hp_val_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "IY9G-LeLVUhW",
        "outputId": "5b0d7feb-4f9f-4fb0-e72e-71ff389f1091"
      },
      "outputs": [],
      "source": [
        "loss = vae_model.history.history['loss']\n",
        "val_loss = vae_model.history.history['val_loss']\n",
        "epochs = range(hp_epochs)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'ro', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IMdpLlmYVdhB",
        "outputId": "ed1fa033-fd44-42a7-8d31-a99246f8957b"
      },
      "outputs": [],
      "source": [
        "embedded_points = encoder.predict(geno_tensor)\n",
        "print(embedded_points)\n",
        "\n",
        "x = embedded_points[0][:,0]\n",
        "y = embedded_points[0][:,1]\n",
        "pop = pheno.iloc[:,5]\n",
        "data = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.scatterplot(x='x', y='y', data=data, hue='pop', style='pop', s=100)\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us create some 10 imaginary individuals around a given point in the latent space, here [0.4, -0.5]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSHnynsm93m",
        "outputId": "f716fb22-2e1c-410f-af85-b93fbcd17f7f"
      },
      "outputs": [],
      "source": [
        "N_ind = 10\n",
        "center = [0, 0.0]\n",
        "cx = tf.random.normal(shape = [N_ind], mean = 0.4, stddev = 0.1)\n",
        "cy = tf.random.normal(shape = [N_ind], mean = -0.5, stddev = 0.1)\n",
        "z_sample = np.column_stack((cx, cy))\n",
        "z_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuZ6gM-Islnn",
        "outputId": "477e59d0-2eac-45ce-fb04-12194ee8bb86"
      },
      "outputs": [],
      "source": [
        "# Let's thread those artificial individuals through the decoder part of the model\n",
        "decoded = decoder.predict(z_sample,)\n",
        "decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xNcWUDvoqir",
        "outputId": "bb8d0982-ab29-4861-e399-b22a22979920"
      },
      "outputs": [],
      "source": [
        "# And see what are their genotypes\n",
        "new_geno = np.zeros(shape = decoded.shape)\n",
        "new_geno[decoded <= 0.33] = 0\n",
        "new_geno[np.logical_and(decoded > 0.33, decoded < 0.66)] = 1\n",
        "new_geno[decoded > 0.66] = 2\n",
        "print(new_geno)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "xoFLN_k1n10U",
        "outputId": "39f38d56-64af-4d9b-933c-fc5fce593c48"
      },
      "outputs": [],
      "source": [
        "# And now, we can see where our individuals ended up in the embedded space. \n",
        "x_encoded = encoder.predict(np.row_stack((geno_tensor, decoded)), batch_size=32)\n",
        "x = x_encoded[0][:, 0]\n",
        "y = x_encoded[0][:, 1]\n",
        "pop_list = [pheno['population'], pd.Series(np.repeat('TST', new_geno.shape[0]))]\n",
        "pop = pd.concat(pop_list)\n",
        "data_tmp = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (7, 7))\n",
        "sns.scatterplot(x='x', y='y', data=data_tmp, hue='pop', style='pop', s=100)\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3\n",
        "Remove the layer with 1500 units from the VAE model and see how does it affect the result. What do you think is happening here?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMh8f2xK/cCsTLHl1OhBLu/",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
