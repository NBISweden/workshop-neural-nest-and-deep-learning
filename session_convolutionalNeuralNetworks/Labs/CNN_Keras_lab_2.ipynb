{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Lab 2: Segmentation of Human Blood Cells using Convolutional Neural Networks\n",
    "\n",
    "For this lab, we use the [Human White Blood Cell images](https://github.com/zxaoyou/segmentation_WBC) from [Jiangxi Tecom Science Corporation, China](http://en.tecom-cn.com/).\n",
    "\n",
    "<img src=\"illustrations/WhiteBloodCells.png\" width=\"500px\" title=\"Blood cell illustration (Wikipedia)\" align=\"center\"/>\n",
    "<center><i>(Illustration from <a href=\"https://en.wikipedia.org/wiki/White_blood_cell\">Wikipédia</a>)</i></center>\n",
    "\n",
    "<br/>\n",
    "The dataset contains three hundred 120x120 RGB images with one blood cell per image, and corresponding segmentation masks. The segmentation mask was manually sketched by domain experts, with the background, cytoplasms and nuclei pixels labelled as 0, 1 and 2 respectively.\n",
    "\n",
    "<img src=\"illustrations/WBC_Dataset1.png\" title=\"Blood cells dataset\" align=\"center\"/>\n",
    "\n",
    "These images and masks are in the **data/bloodcells_seg/** folder:\n",
    "```\n",
    "└── data\n",
    "    └── bloodcells_seg\n",
    "        ├── masks\n",
    "        │   ├── all\n",
    "        └── images\n",
    "            ├── all\n",
    "```\n",
    "\n",
    "We want to use convolutional neural networks to do pixel-wise classification of these blood cells images into background / cytoplasm / nuclei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run this notebook in Google Colab, follow [this link](https://colab.research.google.com/github/cavenel/workshop-neural-nets-and-deep-learning/blob/master/session_convolutionalNeuralNetworks/Labs/CNN_Keras_lab_2.ipynb), select GPU as execution type, then uncomment and run the following code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content\n",
    "# !git clone https://github.com/cavenel/workshop-neural-nets-and-deep-learning\n",
    "# %cd /content/workshop-neural-nets-and-deep-learning/session_convolutionalNeuralNetworks/Labs/\n",
    "# %ls\n",
    "# !unzip data.zip\n",
    "# !pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import useful modules like numpy and Keras layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import Conv2DTranspose, concatenate, UpSampling2D\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# cnn_helper contains some useful functions for this lab\n",
    "import cnn_helper\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load both the images and the masks as generators, and combine both generators into a zip of generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 3 # Background, cytoplasm, nuclei\n",
    "N_CHANNELS = 3  # R,G,B\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "seed = 909 # (IMPORTANT) to transform image and corresponding mask with same augmentation parameter.\n",
    "image_datagen = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255 #We need to rescale png images from integers to floats\n",
    ")\n",
    "mask_datagen = ImageDataGenerator(\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load image and mask generators for training\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    directory='data/bloodcells_seg/images/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='training',\n",
    "    color_mode='rgb',\n",
    "    shuffle=False)\n",
    "\n",
    "train_mask_generator = mask_datagen.flow_from_directory(\n",
    "    directory='data/bloodcells_seg/masks/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)\n",
    "\n",
    "train_generator = zip(train_image_generator, train_mask_generator)\n",
    "\n",
    "# Load image and mask  generators for validation\n",
    "val_image_generator = image_datagen.flow_from_directory(\n",
    "    directory='data/bloodcells_seg/images/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation',\n",
    "    color_mode='rgb',\n",
    "    shuffle=False)\n",
    "\n",
    "val_mask_generator = mask_datagen.flow_from_directory(\n",
    "    directory='data/bloodcells_seg/masks/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)\n",
    "\n",
    "val_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check that the generators deliver our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cMap = ListedColormap(['red', 'lime', 'blue'])\n",
    "\n",
    "images,masks = next(train_generator)\n",
    "for i in range(0,2):\n",
    "    plt.figure()\n",
    "    plt.imshow(images[i], cmap=cMap)\n",
    "    plt.figure()\n",
    "    plt.imshow(masks[i,:,:,0], cmap=cMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a basic UNet Model\n",
    "\n",
    "<img src=\"illustrations/unet.png\" title=\"Unet model\"/>\n",
    "\n",
    "This time we will use the Keras functional API to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((IMG_SIZE, IMG_SIZE, N_CHANNELS))\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.2) (c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = Dropout(0.1) (c8)\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = Dropout(0.1) (c9)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(NUM_CLASSES, (1, 1), activation='softmax') (c9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our ground truth is represented by integers on a mask (0 for background, 1 for cytoplasm, 2 for nuclei pixels) and not hot-one encoded, we will use the Keras SparseCategoricalCrossentropy loss function (see https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we add a callback function that will plot `num_plot` results from validation after each epoch, to follow in real time the training of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=240/BATCH_SIZE, \n",
    "    epochs=20,\n",
    "    validation_data=(val_generator),\n",
    "    validation_steps=64/BATCH_SIZE, \n",
    "    callbacks=[\n",
    "        cnn_helper.PlottingKerasCallback(\n",
    "            test_batch=next(val_generator),\n",
    "            num_plot=3 # how many validation examples to plot at each epoch (maximum = BATCH_SIZE)\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot some other examples from validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_helper.plot_prediction (model, next(val_generator), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning of the model\n",
    "You can now try to tune your model by changing the learning rate of the Adam optimizer, the dropout, the batch size, or any other parameter you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models\n",
    "\n",
    "To simplify the construction of our networks, we will now use the `segmentation_model` python library (install using `pip install segmentation_model`).\n",
    "\n",
    "The main features of this library are:\n",
    " - High level API (just two lines of code to create model for segmentation)\n",
    " - 4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n",
    " - 25 available backbones for each architecture\n",
    " \n",
    "You can read more about the `segmentation_model` library at https://github.com/qubvel/segmentation_models.\n",
    "\n",
    "## List of models:\n",
    "\n",
    "| Unet | Linknet |\n",
    "| --- | --- |\n",
    "| <img width=\"500px\" src=\"https://raw.githubusercontent.com/qubvel/segmentation_models/master/images/unet.png\" title=\"UNet\"/> | <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/linknet.png\" title=\"Linknet\"/> |\n",
    "\n",
    "| PSPNet | FPN |\n",
    "| --- | --- |\n",
    "| <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/pspnet.png\" title=\"UNet\"/> | <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/fpn.png\" title=\"Linknet\"/> |\n",
    "\n",
    "## List of backbones:\n",
    "\n",
    "| Type | Names |\n",
    "| --- | --- |\n",
    "| VGG | `vgg16` `vgg19` |\n",
    "| ResNet | `resnet18` `resnet34` `resnet50` `resnet101` `resnet152` |\n",
    "| SE-ResNet | `seresnet18` `seresnet34` `seresnet50` `seresnet101` `seresnet152` |\n",
    "| ResNeXt | `resnext50` `resnext101` |\n",
    "| SE-ResNeXt | `seresnext50` `seresnext101` |\n",
    "| SENet154 | `senet154` |\n",
    "| DenseNet | `densenet121` `densenet169` `densenet201` |\n",
    "| Inception | `inceptionv3` `inceptionresnetv2` |\n",
    "| MobileNet | `mobilenet` `mobilenetv2` |\n",
    "| EfficientNet | `efficientnetb0` `efficientnetb1` `efficientnetb2` `efficientnetb3` `efficientnetb4` `efficientnetb5` efficientnetb6` efficientnetb7` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for the segmentation_model library\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"]=\"tf.keras\"\n",
    "\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different models and backbone from the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the Unet model, using the VGG16 backbone\n",
    "model = sm.Unet(\n",
    "    'vgg16',\n",
    "    #encoder_weights=None,\n",
    "    encoder_weights='imagenet',\n",
    "    classes=NUM_CLASSES,\n",
    "    activation='softmax',\n",
    "    input_shape=(IMG_SIZE,IMG_SIZE,3)\n",
    ")\n",
    "\n",
    "#print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can now compile and train with the same parameters as before:\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=240/BATCH_SIZE, \n",
    "    epochs=30,\n",
    "    validation_data=(val_generator),\n",
    "    validation_steps=64/BATCH_SIZE, \n",
    "    verbose=1,\n",
    "    callbacks=[cnn_helper.PlottingKerasCallback(\n",
    "        test_batch=next(val_generator),\n",
    "        num_plot=2 # maximum num_plot is BATCH_SIZE\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_helper.plot_prediction (model, next(val_generator), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
