{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".column {\n",
       "  float: left;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "/* Clear floats after the columns */\n",
       ".row:after {\n",
       "  content: \"\";\n",
       "  display: table;\n",
       "  clear: both;\n",
       "}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import Conv2DTranspose, concatenate, UpSampling2D\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".column {\n",
    "  float: left;\n",
    "  width: 50%;\n",
    "}\n",
    "\n",
    "/* Clear floats after the columns */\n",
    ".row:after {\n",
    "  content: \"\";\n",
    "  display: table;\n",
    "  clear: both;\n",
    "}</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNNs are used everywhere\n",
    "<center>\n",
    "    <img src=\"illustrations/vision.png\" style=\"max-height:700px;width:auto;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN for image classification\n",
    "\n",
    "### CNN = Convolutional Neural Networks (or ConvNets)\n",
    "\n",
    "<img src=\"illustrations/LeNet.png\"/>\n",
    "\n",
    "<small>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). LeNet: gradient-based learning applied to document recognition.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline of the lecture\n",
    "\n",
    " - Convolutions\n",
    " - Convolutions in Neural Networks\n",
    "  - Motivations\n",
    "  - Layers\n",
    " - Architectures\n",
    "  - Classic CNN Architecture\n",
    "  - AlexNet\n",
    "  - VGG16\n",
    "  - ResNet\n",
    " - Data augmentation\n",
    " - Grad-CAM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    " - A mathematical operation that combines two functions to form a third function.\n",
    " - The feature map (or input data) and the kernel are combined to form a transformed feature map.\n",
    " - Often interpreted as a filter: the kernel filters the feature map for certain information (edges, etc.)\n",
    " \n",
    "<center>\n",
    "    <img src=\"illustrations/convolution-1.png\" style=\"width:450px;\"/>\n",
    "    <small>Figure 1: Convolving an image with an edge detector kernel.</small>\n",
    "</center>\n",
    "\n",
    "The mathematical definition of convolution of two functions f and x over a range t is:\n",
    "<center>\n",
    "    $y(t) = f \\otimes x = \\int_{-\\inf}^{\\inf}f(k) \\cdot x(t-k) \\mathrm{d}k$\n",
    "</center>\n",
    "\n",
    "where the symbol ⊗ denotes convolution.\n",
    "\n",
    "<small>https://developer.nvidia.com/discover/convolution</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "Convolutional filters can be interpreted as feature detectors:\n",
    " - The input (feature map) is filtered for a certain feature (the kernel).\n",
    " - The output is large if the feature is detected in the image.\n",
    "\n",
    "<center>\n",
    "    <img src=\"illustrations/convolution-3.png\" style=\"width:450px;\"/>\n",
    "    <small>The kernel can be interpreted as a feature detector where a detected feature results in large outputs (white) and small outputs if no feature is present (black).</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution in a neural network\n",
    "<center>\n",
    "<img src=\"illustrations/numerical_no_padding_no_strides.gif\" style=\"width:450px;\"/>\n",
    "</center>\n",
    "\n",
    "- $x$ is a $3 \\times 3$ chunk (yellow area) of the image (green array)\n",
    "\n",
    "- Each output neuron is parametrized with the $3 \\times 3$ weight matrix $w$ (small numbers)\n",
    " \n",
    "The activation obtained by sliding the $3 \\times 3$ window and computing:\n",
    "<center>\n",
    "$z(x) = relu(\\mathbf{w}^T x + b)$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivations\n",
    "\n",
    "Standard Dense Layer for an image input:\n",
    "```Python\n",
    "x = Input((640, 480, 3), dtype='float32')\n",
    "# shape of x is: (None, 640, 480, 3)\n",
    "x = Flatten()(x)\n",
    "# shape of x is: (None, 640 x 480 x 3)\n",
    "z = Dense(1000)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$640 \\times 480 \\times 3 \\times 1000 + 1000 = 922M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No spatial organization of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dense layers are never used directly on large images. Most standard solution is to use <b>convolution layers</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivations\n",
    "### Local connectivity\n",
    " - A neuron depends only on a few local input neurons\n",
    " - Translation invariance\n",
    "\n",
    "### Comparison to Fully connected\n",
    " - Parameter sharing: reduce overfitting\n",
    " - Make use of spatial structure: strong prior for vision!\n",
    "\n",
    "### Animal Vision Analogy\n",
    " - <i>Hubel & Wiesel, RECEPTIVE FIELDS OF SINGLE NEURONES IN THE CAT'S STRIATE CORTEX (1959)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Channels\n",
    "\n",
    "Colored image = tensor of shape (height, width, channels)\n",
    "\n",
    "Convolutions are usually computed for each channel, and summed:\n",
    "\n",
    "<center>\n",
    "<img src=\"illustrations/convmap1_dims.svg\" width=\"500px\"/>\n",
    "</center>\n",
    "\n",
    "<center>$(k \\star im^{color}) = \\sum\\limits_{c=0}^2 k^c \\star im^c\n",
    "$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple convolutions\n",
    "\n",
    "<center>\n",
    "<img src=\"illustrations/convmap_dims.svg\" width=\"500px\"/>\n",
    "</center>\n",
    "\n",
    " - Kernel size aka receptive field (usually 1, 3, 5, 7, 11)\n",
    " - Output dimension: length - kernel_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strides\n",
    "\n",
    "- Strides: increment step size for the convolution operator\n",
    "- Reduces the size of the output map\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/no_padding_strides.gif\" style=\"width: 260px;\" />\n",
    "</center>\n",
    "\n",
    "<center><small>\n",
    "Example with kernel size $3 \\times 3$ and a stride of $2$ (image in blue)\n",
    "</small></center>\n",
    "<br/><br/>\n",
    "<small><i>Convolution visualization by V. Dumoulin https://github.com/vdumoulin/conv_arithmetic</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding\n",
    "\n",
    "- Padding: artificially fill borders of image\n",
    "- Useful to keep spatial dimension constant across filters\n",
    "- Useful with strides and large receptive fields\n",
    "- Usually: fill with 0s\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/same_padding_no_strides.gif\" style=\"width: 260px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shapes of convolution layers\n",
    "\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"column\">\n",
    "        <br/><b>Kernel</b> or <b>Filter</b> shape $(F, F, C^i, C^o)$</div>\n",
    "    <div class=\"column\">\n",
    "        <img src=\"illustrations/kernel.svg\" style=\"width: 100px;\"/>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    " - $F \\times F$ kernel size,\n",
    " - $C^i$ input channels\n",
    " - $C^o$ output channels\n",
    "\n",
    "Number of parameters: $(F \\times F \\times C^i + 1) \\times C^o$\n",
    "\n",
    "**Activations** or **Feature maps** shape:\n",
    "- Input $(W^i, H^i, C^i)$\n",
    "- Output $(W^o, H^o, C^o)$\n",
    "\n",
    "$W^o = (W^i - F + 2P) / S + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"4d0bba7197528b94\"></div>\n",
       "    <script type=\"text/javascript\">\n",
       "        $(function(){\n",
       "            var p = $(\"#4d0bba7197528b94\");\n",
       "            if (p.length==0) return;\n",
       "            while (!p.hasClass(\"cell\")) {\n",
       "                p=p.parent();\n",
       "                if (p.prop(\"tagName\") ==\"body\") return;\n",
       "            }\n",
       "            var cell = p;\n",
       "            cell.find(\".input\").addClass(\"hide-in-slideshow\")\n",
       "        });\n",
       "    </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700\"\n",
       "            src=\"https://cs231n.github.io/assets/conv-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1dccc4266a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_code_in_slideshow()\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://cs231n.github.io/assets/conv-demo/index.html', width=\"100%\", height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$W^o = (W^i - F + 2P) / S + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    " - Spatial dimension reduction\n",
    " - Local invariance\n",
    " - No parameters: max or average of 2x2 units\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/max-pooling.png\" style=\"width: 560px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    "- Spatial dimension reduction\n",
    "- Local invariance\n",
    "- No parameters: max or average of 2x2 units\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maxpool.svg\" style=\"width: 380px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Keras\n",
    "\n",
    "#### Fully Connected Network: Multilayer Perceptron\n",
    "\n",
    "```Python\n",
    "input_image = Input(shape=(28, 28, 1))\n",
    "x = Flatten()(input_image)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "mlp = Model(inputs=input_image, outputs=x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Keras\n",
    "\n",
    "#### Convolutional Network\n",
    "\n",
    "```Python\n",
    "input_image = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(filters=32, kernel_size=5, padding='same', activation='relu')(input_image)\n",
    "x = MaxPooling2D(2, strides=2)(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D(2, strides=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "convnet = Model(inputs=input_image, outputs=x)\n",
    "```\n",
    "\n",
    "**2D spatial organization of features preserved untill Flatten.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "<center>\n",
    "    <h1>Architectures</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classic ConvNet Architecture\n",
    "\n",
    "### Input\n",
    "\n",
    "### Conv blocks\n",
    "\n",
    "- Convolution + activation (relu)\n",
    "- Convolution + activation (relu)\n",
    "- ...\n",
    "- Maxpooling 2x2\n",
    "\n",
    "### Output\n",
    "\n",
    "- Fully connected layers\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/alexNet.jpg\" style=\"width: 800px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Simplified version of Krizhevsky, Alex, Sutskever, and Hinton. \"Imagenet classification with deep convolutional neural networks.\" NIPS 2012\n",
    "</small>\n",
    "\n",
    "Input: 227x227x3 image\n",
    "First conv layer: kernel 11x11x3x96 stride 4\n",
    "\n",
    "- Kernel shape: `(11,11,3,96)`\n",
    "- Output shape: `(55,55,96)`\n",
    "- Number of parameters: `34,944`\n",
    "- Equivalent MLP parameters: `43.7 x 1e9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add other networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benchmarks\n",
    "<center>\n",
    "          <img src=\"illustrations/accuracy_vs_speed.png\" style=\"width: 50%;\" />\n",
    "</center>\n",
    "<small>\n",
    "    Top-1 accuracy vs. number of images processed per second (with batch size 1) using the Titan Xp <i>(S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark analysis of representative deep neural network architectures,” IEEE Access, vol. 6, 2018.)</i>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "autolaunch": false,
   "center": false,
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
