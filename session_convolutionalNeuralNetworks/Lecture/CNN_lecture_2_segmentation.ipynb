{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks for Image Segmentation\n",
    "#### *(Start recording)*\n",
    "<small class=\"bottom\">Some of the material from this lecture comes from online courses of Charles Ollion and Olivier Grisel - Master Datascience Paris Saclay.<br/>\n",
    "[CC-By 4.0 license](https://creativecommons.org/licenses/by/4.0/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNNs for computer vision\n",
    "\n",
    "<center>\n",
    "    <img src=\"illustrations/vision.png\" style=\"max-height:700px;width:auto;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "### CNNs\n",
    "- Previous lecture: image classification\n",
    "\n",
    "### Limitations\n",
    "- Mostly on centered images\n",
    "- Only a single object per image\n",
    "- Not enough for many real world vision tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_1.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_2.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_3.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_4_2.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_4_3.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "\n",
    "### Simple Localisation as regression\n",
    "\n",
    "### Detection Algorithms\n",
    "\n",
    "### Fully convolutional Networks\n",
    "\n",
    "### Semantic & Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Localisation\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/dog.jpg\" style=\"width: 400px;\" />\n",
    "</center>\n",
    "\n",
    "- Single object per image\n",
    "- Predict coordinates of a bounding box `(x, y, w, h)`\n",
    "\n",
    "- Evaluate via Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Localisation as regression\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/regression_dog.svg\" style=\"width: 700px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification + Localisation\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/doublehead.svg\" style=\"width: 900px;\" />\n",
    "</center>\n",
    "\n",
    "- Use a pre-trained CNN on ImageNet (ex. ResNet)\n",
    "- The \"localisation head\" is trained seperately with regression\n",
    "- At test time, use both heads\n",
    "\n",
    "$C$ classes, $4$ output dimensions ($1$ box)\n",
    "\n",
    "**Predict exactly $N$ objects:** predict $(N \\times 4)$ coordinates and $(N \\times K)$ class scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Object detection\n",
    "\n",
    "We don't know in advance the number of objects in the image. Object detection relies on *object proposal* and *object classification*\n",
    "\n",
    "**Object proposal:** find regions of interest (RoIs) in the image\n",
    "\n",
    "**Object classification:** classify the object in these regions\n",
    "\n",
    "### Two main families:\n",
    "\n",
    "- Single-Stage: A grid in the image where each cell is a proposal (SSD, YOLO, RetinaNet)\n",
    "- Two-Stage: Region proposal then classification (Faster-RCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO  (You Only Look Once)\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo1.png\" style=\"width: 750px;\" />\n",
    "</center>\n",
    "\n",
    "For each cell of the $S \\times S$ predict:\n",
    "- $B$ **boxes** and **confidence scores** $C$ ($5 \\times B$ values) + **classes** $c$\n",
    "- Final detections: $C_j * prob(c) > \\text{threshold}$\n",
    "<br/>\n",
    "<small class=\"bottom\">\n",
    "    Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO  (You Only Look Once)\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo1.png\" style=\"width: 750px;\" />\n",
    "</center>\n",
    "\n",
    "YOLO features:\n",
    "- Computationally very fast, can be used in real time\n",
    "- Globally processing the entire image once only with a single CNN\n",
    "<br/>\n",
    "<small class=\"bottom\">\n",
    "    Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RetinaNet\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/retinanet.png\" style=\"width: 1400px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" ICCV 2017.\n",
    "</small>\n",
    "\n",
    "Single stage detector with:\n",
    "- Multiple scales through a *Feature Pyramid Network*\n",
    "- More than 100K boxes proposed\n",
    "- Focal loss to manage imbalance between background and real objects\n",
    "\n",
    "See this [post](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Box Proposals\n",
    "\n",
    "Instead of having a predefined set of box proposals, find them on the image:\n",
    "- **Selective Search** - from pixels (not learnt)\n",
    "- **Faster - RCNN** - Region Proposal Network (RPN)\n",
    "\n",
    "**Crop-and-resize** operator (**RoI-Pooling**):\n",
    "- Input: convolutional map + $N$ regions of interest\n",
    "- Output: tensor of $N \\times 7 \\times 7 \\times \\text{depth}$ boxes\n",
    "- Allows to propagate gradient only on interesting regions, and efficient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/fastRCNN.png\" style=\"width: 1100px;\" />\n",
    "</center>\n",
    "\n",
    "\n",
    "<small>\n",
    "Girshick, Ross, et al. \"Fast r-cnn.\" ICCV 2015\n",
    "</small>\n",
    "\n",
    "- **Selective Search** + Crop-and-resize (RoI pooling)\n",
    "\n",
    "- Output: Softmax over $(K + 1)$ classes, and $4$ box offsets\n",
    "\n",
    "- Positive box are the ones with largest Intersection over Union **IoU** with ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Faster-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/fastRCNN_2.png\" style=\"width: 1000px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" NIPS 2015\n",
    "</small>\n",
    "\n",
    "- Replace **Selective Search** with **RPN**, train jointly\n",
    "\n",
    "- Region proposal is translation invariant, compared to YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation\n",
    "\n",
    "Output a class map for each pixel (here: dog vs background)\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/dog_segment.jpg\" style=\"width: 600px;\" />\n",
    "</center>\n",
    "\n",
    "- **Instance segmentation**: specify each object instance as well (two dogs have different instances)\n",
    "\n",
    "- This can be done through **object detection** + **segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutionize\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/convolutionalization.png\" style=\"width: 800px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Long, Jonathan, et al. \"Fully convolutional networks for semantic segmentation.\" CVPR 2015\n",
    "</small>\n",
    "\n",
    "- Slide the network with an input of `(224, 224)` over a larger image. Output of varying spatial size\n",
    "\n",
    "- **Convolutionize**: change Dense `(4096, 1000)` to $1 \\times 1$ Convolution, with `4096, 1000` input and output channels\n",
    "\n",
    "- Gives a coarse **segmentation** (no extra supervision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully Convolutional Network\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/densefc.png\" style=\"width: 800px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Long, Jonathan, et al. \"Fully convolutional networks for semantic segmentation.\" CVPR 2015\n",
    "</small>\n",
    "\n",
    "- Predict / backpropagate for every output pixel\n",
    "\n",
    "- Aggregate maps from several convolutions at different scales for more robust results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deconvolution\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/deconv.png\" style=\"width: 1200px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Noh, Hyeonwoo, et al. \"Learning deconvolution network for semantic segmentation.\" ICCV 2015\n",
    "</small>\n",
    "<br/>\n",
    "<br/>\n",
    "- \"Deconvolution\": transposed convolutions\n",
    "<center>\n",
    "          <img src=\"illustrations/conv_deconv.png\" style=\"width: 600px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deconvolution\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/deconv.png\" style=\"width: 1200px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Noh, Hyeonwoo, et al. \"Learning deconvolution network for semantic segmentation.\" ICCV 2015\n",
    "</small>\n",
    "\n",
    "- **skip connections** between corresponding convolution and deconvolution layers\n",
    "\n",
    "- **sharper masks** by using precise spatial information (early layers)\n",
    "\n",
    "- **better object detection** by using semantic information (late layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hourglass network\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/hourglass.png\" style=\"width: 1200px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Newell, Alejandro, et al. \"Stacked Hourglass Networks for Human Pose Estimation.\" ECCV 2016\n",
    "</small>\n",
    "\n",
    "- U-Net like architectures repeated sequentially\n",
    "\n",
    "- Each block refines the segmentation for the following\n",
    "\n",
    "- Each block has a segmentation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mask-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnn.png\" style=\"width: 760px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "K. He and al. Mask Region-based Convolutional Network (Mask R-CNN) NIPS 2017\n",
    "</small>\n",
    "\n",
    "Faster-RCNN architecture with a third, binary mask head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnnresults.png\" style=\"width: 1400px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "K. He and al. Mask Region-based Convolutional Network (Mask R-CNN) NIPS 2017\n",
    "</small>\n",
    "<br/><br/>\n",
    "- Mask results are still coarse (low mask resolution)\n",
    "- Excellent instance generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnnresults2.png\" style=\"width: 1200px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "He, Kaiming, et al. \"Mask r-cnn.\" Internal Conference on Computer Vision (ICCV), 2017.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".reveal .bottom {\n",
       "    position: fixed;\n",
       "    bottom: 100px;\n",
       "}\n",
       ".reveal h1 {\n",
       "    position: fixed;\n",
       "    top: 40%;\n",
       "    text-align: center;\n",
       "    width: 100%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just tuning some CSS for better display of columns and small code\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal .bottom {\n",
    "    position: fixed;\n",
    "    bottom: 100px;\n",
    "}\n",
    ".reveal h1 {\n",
    "    position: fixed;\n",
    "    top: 40%;\n",
    "    text-align: center;\n",
    "    width: 100%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "resources": {
   "reveal": {
    "center": false,
    "scroll": true,
    "transition": "none"
   }
  },
  "rise": {
   "autolaunch": false,
   "center": false,
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
