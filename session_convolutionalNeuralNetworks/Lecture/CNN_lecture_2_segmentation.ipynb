{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks for Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNNs for computer Vision\n",
    "\n",
    "<center>\n",
    "    <img src=\"illustrations/vision.png\" style=\"max-height:700px;width:auto;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "### CNNs\n",
    "- Previous lecture: image classification\n",
    "\n",
    "### Limitations\n",
    "- Mostly on centered images\n",
    "- Only a single object per image\n",
    "- Not enough for many real world vision tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_1.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_2.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_3.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_4_2.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Image Classification\n",
    "\n",
    "<center>\n",
    "          <br/>\n",
    "          <img src=\"illustrations/cls_4_3.png\" style=\"width: 1200px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "\n",
    "### Simple Localisation as regression\n",
    "\n",
    "### Detection Algorithms\n",
    "\n",
    "### Fully convolutional Networks\n",
    "\n",
    "### Semantic & Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Localisation\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/dog.jpg\" style=\"width: 400px;\" />\n",
    "</center>\n",
    "\n",
    "- Single object per image\n",
    "- Predict coordinates of a bounding box `(x, y, w, h)`\n",
    "\n",
    "- Evaluate via Interection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Localisation as regression\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/regression_dog_1.svg\" style=\"width: 700px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Localisation as regression\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/regression_dog.svg\" style=\"width: 700px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification + Localisation\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/doublehead.svg\" style=\"width: 900px;\" />\n",
    "</center>\n",
    "\n",
    "- Use a pre-trained CNN on ImageNet (ex. ResNet)\n",
    "- The \"localisation head\" is trained seperately with regression\n",
    "- At test time, use both heads\n",
    "\n",
    "$C$ classes, $4$ output dimensions ($1$ box)\n",
    "\n",
    "**Predict exactly $N$ objects:** predict $(N \\times 4)$ coordinates and $(N \\times K)$ class scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Object detection\n",
    "\n",
    "We don't know in advance the number of objects in the image. Object detection relies on *object proposal* and *object classification*\n",
    "\n",
    "**Object proposal:** find regions of interest (RoIs) in the image\n",
    "\n",
    "**Object classification:** classify the object in these regions\n",
    "\n",
    "### Two main families:\n",
    "\n",
    "- Single-Stage: A grid in the image where each cell is a proposal (SSD, YOLO, RetinaNet)\n",
    "- Two-Stage: Region proposal then classification (Faster-RCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo0.png\" style=\"width: 500px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>\n",
    "\n",
    "For each cell of the $S \\times S$ predict:\n",
    "- $B$ **boxes** and **confidence scores** $C$ ($5 \\times B$ values) + **classes** $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo1.png\" style=\"width: 500px;\" />\n",
    "</center>\n",
    "\n",
    "For each cell of the $S \\times S$ predict:\n",
    "- $B$ **boxes** and **confidence scores** $C$ ($5 \\times B$ values) + **classes** $c$\n",
    "\n",
    "<small>\n",
    "Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo1.png\" style=\"width: 500px;\" />\n",
    "</center>\n",
    "\n",
    "Final detections: $C_j * prob(c) > \\text{threshold}$\n",
    "\n",
    "<small>\n",
    "Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO\n",
    "\n",
    "<small>\n",
    "Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" CVPR (2016)\n",
    "</small>\n",
    "\n",
    "- After ImageNet pretraining, the whole network is trained end-to-end\n",
    "\n",
    "- The loss is a weighted sum of different regressions\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/yolo_loss.png\" style=\"width: 400px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RetinaNet\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/retinanet.png\" style=\"width: 700px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" ICCV 2017.\n",
    "</small>\n",
    "\n",
    "Single stage detector with:\n",
    "- Multiple scales through a *Feature Pyramid Network*\n",
    "- Focal loss to manage imbalance between background and real objects\n",
    "\n",
    "See this [post](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Box Proposals\n",
    "\n",
    "Instead of having a predefined set of box proposals, find them on the image:\n",
    "- **Selective Search** - from pixels (not learnt)\n",
    "- **Faster - RCNN** - Region Proposal Network (RPN)\n",
    "\n",
    "**Crop-and-resize** operator (**RoI-Pooling**):\n",
    "- Input: convolutional map + $N$ regions of interest\n",
    "- Output: tensor of $N \\times 7 \\times 7 \\times \\text{depth}$ boxes\n",
    "- Allows to propagate gradient only on interesting regions, and efficient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/fastRCNN.png\" style=\"width: 650px;\" />\n",
    "</center>\n",
    "\n",
    "\n",
    "<small>\n",
    "Girshick, Ross, et al. \"Fast r-cnn.\" ICCV 2015\n",
    "</small>\n",
    "\n",
    "- **Selective Search** + Crop-and-resize (RoI pooling)\n",
    "\n",
    "- Output: Softmax over $(K + 1)$ classes, and $4$ box offsets\n",
    "\n",
    "- Positive box are the ones with largest Intersection over Union **IoU** with ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Faster-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/fasterrcnn.png\" style=\"width: 300px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" NIPS 2015\n",
    "</small>\n",
    "\n",
    "- Replace **Selective Search** with **RPN**, train jointly\n",
    "\n",
    "- Region proposal is translation invariant, compared to YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation\n",
    "\n",
    "Output a class map for each pixel (here: dog vs background)\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/dog_segment.jpg\" style=\"width: 400px;\" />\n",
    "</center>\n",
    "\n",
    "- **Instance segmentation**: specify each object instance as well (two dogs have different instances)\n",
    "\n",
    "- This can be done through **object detection** + **segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutionize\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/convolutionalization.png\" style=\"width: 400px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Long, Jonathan, et al. \"Fully convolutional networks for semantic segmentation.\" CVPR 2015\n",
    "</small>\n",
    "\n",
    "- Slide the network with an input of `(224, 224)` over a larger image. Output of varying spatial size\n",
    "\n",
    "- **Convolutionize**: change Dense `(4096, 1000)` to $1 \\times 1$ Convolution, with `4096, 1000` input and output channels\n",
    "\n",
    "- Gives a coarse **segmentation** (no extra supervision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully Convolutional Network\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/densefc.png\" style=\"width: 500px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Long, Jonathan, et al. \"Fully convolutional networks for semantic segmentation.\" CVPR 2015\n",
    "</small>\n",
    "\n",
    "- Predict / backpropagate for every output pixel\n",
    "\n",
    "- Aggregate maps from several convolutions at different scales for more robust results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deconvolution\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/deconv.png\" style=\"width: 650px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Noh, Hyeonwoo, et al. \"Learning deconvolution network for semantic segmentation.\" ICCV 2015\n",
    "</small>\n",
    "\n",
    "- \"Deconvolution\": transposed convolutions\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/conv_deconv.png\" style=\"width: 350px;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deconvolution\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/deconv.png\" style=\"width: 650px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Noh, Hyeonwoo, et al. \"Learning deconvolution network for semantic segmentation.\" ICCV 2015\n",
    "</small>\n",
    "\n",
    "- **skip connections** between corresponding convolution and deconvolution layers\n",
    "\n",
    "- **sharper masks** by using precise spatial information (early layers)\n",
    "\n",
    "- **better object detection** by using semantic information (late layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hourglass network\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/hourglass.png\" style=\"width: 650px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "Newell, Alejandro, et al. \"Stacked Hourglass Networks for Human Pose Estimation.\" ECCV 2016\n",
    "</small>\n",
    "\n",
    "- U-Net like architectures repeated sequentially\n",
    "\n",
    "- Each block refines the segmentation for the following\n",
    "\n",
    "- Each block has a segmentation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mask-RCNN\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnn.png\" style=\"width: 760px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "K. He and al. Mask Region-based Convolutional Network (Mask R-CNN) NIPS 2017\n",
    "</small>\n",
    "\n",
    "Faster-RCNN architecture with a third, binary mask head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnnresults.png\" style=\"width: 760px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "K. He and al. Mask Region-based Convolutional Network (Mask R-CNN) NIPS 2017\n",
    "</small>\n",
    "\n",
    "- Mask results are still coarse (low mask resolution)\n",
    "- Excellent instance generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<center>\n",
    "          <img src=\"illustrations/maskrcnnresults2.png\" style=\"width: 760px;\" />\n",
    "</center>\n",
    "\n",
    "<small>\n",
    "He, Kaiming, et al. \"Mask r-cnn.\" Internal Conference on Computer Vision (ICCV), 2017.\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "autolaunch": false,
   "center": false,
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
