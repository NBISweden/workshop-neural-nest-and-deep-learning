{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target leakage example: classifying text data (tweets)\n",
    "\n",
    "In this lab, we will load a new dataset from disk, clean it up and prepare it for training. The data here is of text type, as this is a dataset of 3000 tweets. So, we have to deal with short text inputs.\n",
    "\n",
    "Each tweet has been written by one of two well-known individuals from the world of US politics. Our task is simply to decide who wrote it. Donald or Hillary?\n",
    "\n",
    "<img src=\"figures/trump-clinton-split.jpg\">\n",
    "\n",
    "The first question here is: how do we deal with string inputs? We can't multiply a word by a weight, so we need to translate the text input in numbers before we proceed and feed it to our first layer. In this case, there are usually two options. The first is called \"one-hot\" encoding, where each word in a dictionary is translated to a vectors of ones and zeros. If there are 10 words in our dictionary (for example, the words are \"zero\", \"one\", \"two\" ... \"nine\"), each vector will contain ten elements, with nine elements set to zero and one set to one:\n",
    "\n",
    "* \"zero\" -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "* \"one\"  -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "* \"two\"  -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "* ...\n",
    "* \"nine\" -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "This is usually ok when dealing with text (or, more generically, \"categorical\") variables taken from a relatively small dictionary. In the case of tweets, we might be dealing with very a dictionary containing tens of thousands of different terms, so we would have huge inputs of sparse vectors of zeros and ones. This is not ideal.\n",
    "\n",
    "The second option is to use word embeddings, which translate each word to a vector of floating points with some nice properties, as we can see in the following image:\n",
    "\n",
    "<img src=\"figures/Word-Vectors.png\">\n",
    "\n",
    "Check out [this cool page](https://anvaka.github.io/pm/#/galaxy/word2vec-wiki?cx=-17&cy=-237&cz=-613&lx=-0.0575&ly=-0.9661&lz=-0.2401&lw=-0.0756&ml=300&s=1.75&l=1&v=d50_clean) visualizing embeddings calculated on the whole English dictionary for more examples.\n",
    "\n",
    "Embeddings are done with a special NN layer that in Keras is called simply \"Embedding\" (https://keras.io/layers/embeddings/). The Embedding layer is provided with a number of text inputs (in our case, tweets) and learns to map similar words into n-dimensional vectors that are close together in the corresponding n-dimensional space.\n",
    "\n",
    "In the following piece of code, we will start by loading the dataset with [pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html) and preparing it for training.\n",
    "\n",
    "Notice how before we can use the Embedding layer, we want to map each word to an integer. This is because the input to an Embedding layer is actually a set of integers, where each integer represents a word. The important thing here, is that a given word has to be mapped always to the same integer throughout the whole dataset, so that the Embedding layer can recognise it from different tweets.\n",
    "\n",
    "In this case, for example, the word \"the\" will always be mapped to the number 1, the word \"question\" to the number 2, etc.\n",
    "\n",
    "* \"The question is what to do\"      -> [1, 2, 32, 55, 87]\n",
    "* \"I don't understand the question\" -> [12, 4, 123, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a helper function to plot training and validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    try:\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "    except:\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset (uncomment and run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data/\n",
    "#!wget -P data https://github.com/NBISweden/workshop-neural-nets-and-deep-learning/blob/961c3a45c3fd3603e6c31bac1c09f468c8e196bc/session_goodPracticesDatasetDesign/lab_targetLeakage/data/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the dataset, which in this case is saved as CSV file, and let's print one tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweet_dataset = pd.read_csv(\"data/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply a few basic operations to handle it more easily, for example:\n",
    "\n",
    "* We add a space after every non-alphanumeric character so that we have, for example \"-Hello\" -> \"- Hello\"\n",
    "* We make all words lower case, so that \"Hello\" == \"hello\"\n",
    "* Lastly, we split each tweet by using space as delimiter, so that we have a list of words for each tweet\n",
    "\n",
    "Then we assign a unique integer to each unique word in the dataset, so that we can translate each tweet to a list of numbers. But equal numbers will always correspond to equal words across all tweets! Notice how we reserve the integer \"0\" for \"padding\". This means that since the longest tweet has 58 words (so a list of 58 integers), we will add a series of \"0\"s to shorter tweets until they are also represented by a list of 58 integers.\n",
    "\n",
    "Lastly, we assign to our labels (in this case the author of the tweets) one class between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Get rid of all retweets\n",
    "tweet_dataset = tweet_dataset[tweet_dataset[\"is_retweet\"] == False]\n",
    "\n",
    "#Remove URLs\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "#Now let's make sure that non-alphanumeric characters are taken as single words\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('\\s*([^a-zA-Z0-9 ])\\s*', ' \\\\1 ', case=False)\n",
    "\n",
    "#remove hashtags\n",
    "#tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('#[a-zA-Z0-9]*', '', case=False)\n",
    "\n",
    "#remove @ mentions\n",
    "#tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('@[a-zA-Z0-9]*', '', case=False)\n",
    "\n",
    "#make all words lower case?\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.lower()\n",
    "\n",
    "#split the tweets in list of words\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.strip()\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.split(\" \")\n",
    "\n",
    "#since the neural networks don't really like string inputs,\n",
    "#we have to convert each word to a unique integer.\n",
    "integer_dict = {}\n",
    "integer_dict[\"padding\"] = 0\n",
    "\n",
    "word_dict = {}\n",
    "word_dict[0] = \"padding\"\n",
    "\n",
    "#assign a unique integer to each unique word\n",
    "count = 1\n",
    "for index, row in tweet_dataset.iterrows():\n",
    "    for element in row[\"text\"]:\n",
    "        if element not in integer_dict.keys():\n",
    "\n",
    "            integer_dict[element] = count\n",
    "            word_dict[count] = element\n",
    "            count += 1\n",
    "    \n",
    "tweet_dataset[\"numbers\"] = tweet_dataset[\"text\"].apply(lambda x:[integer_dict[y] for y in x])\n",
    "\n",
    "#Let's also assign integer labels \n",
    "tweet_dataset.loc[tweet_dataset[\"handle\"] == \"realDonaldTrump\",\"label\"] = 1\n",
    "tweet_dataset.loc[tweet_dataset[\"handle\"] == \"HillaryClinton\",\"label\"] = 0\n",
    "\n",
    "#The longest tweet has 58 words, this will add padding to shorter tweets\n",
    "train_x = pd.DataFrame(tweet_dataset[\"numbers\"].values.tolist()).values\n",
    "train_x[np.where(np.isnan(train_x[:]))] = 0\n",
    "\n",
    "train_y = np.array(tweet_dataset[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the effect that this has had on the first tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset[\"text\"][0]\n",
    "#train_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how we can use Embeddings to transform our dictionary of words into a dictionary of float vectors.\n",
    "\n",
    "First, let's use Embeddings, followed by Dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=count, output_dim=16, input_length=train_x.shape[1], name='embeddings'))\n",
    "model.add(Flatten()) #Dense layers only accept 1D inputs, so we need to flatten the output from Embedding, which is 2D\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model has been trained, you can call the method `predict()` to get the predictions for all future samples. \n",
    "That is how you actually use your model when the job is done!\n",
    "\n",
    "Can you think of a good dataset that you could use to test your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"at least 17 States have joined texas in the extraordinary case against the greatest Election\"\n",
    "tweet = tweet.lower()\n",
    "words = tweet.split(\" \")\n",
    "\n",
    "tweet_integer = np.array([integer_dict[element] for element in words])\n",
    "\n",
    "tweet_padded = np.zeros(58)\n",
    "tweet_padded[:15] = tweet_integer\n",
    "\n",
    "tweet_padded = tweet_padded[np.newaxis, :]\n",
    "print(tweet_padded.shape)\n",
    "result = model.predict(tweet_padded)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "* Which type of network works best?\n",
    "* What could be a better network layer given the nature of this dataset?\n",
    "* What is the meaning of the \"count\" variable used in the Embedding layer?\n",
    "* Why does the last layer have 2 units? Why is the activation of the 'softmax' type?\n",
    "* Play around with the hyperparameters, is there a way to improve the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the outputs of the embedding layer. We extract the embedding layer from the trained model and we use it to calculate embeddings for every word in our dictionary. Then, we map the 32-dimensional output vector onto 2 dimensions with the help of principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.pyplot import scatter\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "#extract the embeddings layer from the previous model, then use it as output in a new model\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('embeddings').output)\n",
    "\n",
    "n_words = 1000\n",
    "inputs = np.zeros((n_words, train_x.shape[1]))\n",
    "for i in range(n_words):\n",
    "    word = word_dict[i]\n",
    "    for c in range(train_x.shape[1]):\n",
    "        inputs[i, c] = i\n",
    "    \n",
    "intermediate_output = intermediate_layer_model.predict(inputs)\n",
    "\n",
    "points = np.squeeze(intermediate_output[:,0,:])\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "points_pca = pca.fit_transform(points)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(points_pca[:,0], points_pca[:,1])\n",
    "\n",
    "for i in range(n_words):\n",
    "    ax.annotate(word_dict[i], (points_pca[i,0], points_pca[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: what is wrong (or right) with this dataset?\n",
    "\n",
    "* Take a few minutes to analyze the word cloud. Can you see what kind of words make the classification easier? If you wanted to make a less biased (and harder to classify) dataset, what would you change? \n",
    "\n",
    "* If possible, go back to the dataset generation step and remove words that make the classification task too easy. Then, try and train a new network. Are the results the same as before?\n",
    "\n",
    "* Why do you think that the \"Dense\" network (Feed-forward, fully connected) works as well as the LSTM recurrent network on the dataset as it is now?\n",
    "\n",
    "* Now that we have trained and validated our model, what would you suggest using as test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of how the dataset can be manipulated. Try these lines of code (and come up with some others!) by pasting them in the right place the code block where the dataset is loaded (second code cell):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Get rid of all retweets\n",
    "tweet_dataset = tweet_dataset[tweet_dataset[\"is_retweet\"] == False]\n",
    "\n",
    "#Next, let's remove all URLs, since they should not be of any help (unless we actually checked what they link to)\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "#remove hashtags\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('#[a-zA-Z0-9]*', '', case=False)\n",
    "\n",
    "#Remove a word\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('crooked', '', case=False)\n",
    "\n",
    "#Remove non-alphanumeric characters\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('\\s*([^a-zA-Z0-9 ])\\s*', '', case=False)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
