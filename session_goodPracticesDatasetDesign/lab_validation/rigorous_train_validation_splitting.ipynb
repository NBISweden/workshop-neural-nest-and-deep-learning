{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigorous splitting of datasets into train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will try our hand at protein structure prediction. Given a few thousands protein sequences, for each of the amino acids in the sequences we will try to predict if in the protein structure they will be part of one of three classes:\n",
    "\n",
    "* $\\alpha$-helix\n",
    "* $\\beta$-sheet\n",
    "* none of the above\n",
    "<img src=\"figures/secondary_structure.png?0\">\n",
    "\n",
    "So the input to our predictor is a protein sequence string such as this one:\n",
    "\n",
    "```\n",
    ">APF29063.1 spike protein [Human coronavirus NL63]\n",
    "MKLFLILLVLPLASCFFTCNSNANLSMLQLGVPDNSSTIVTGLLPTHWFCANQSTSVYSANGFFYIDVGN\n",
    "HRSAFALHTGYYDVNQYYIYVTNEIGLNASVTLKICKFGINTTFDFLSNSSSSFDCIVNLLFTEQLGAPL\n",
    "```\n",
    "\n",
    "for each letter in the sequence, we want to make a classification in the three classes mentioned above.\n",
    "\n",
    "I have prepared a dataset where all protein sequences have been pre-split into windows of 31 amino acids. We want to predict the class for the amino acid in the center of the window, like so:\n",
    "\n",
    "\n",
    "predict(\"MKLFLILLVLPLASCF<font color=\"red\">F</font>TCNSNANLSMLQLG\") -> [p(H), p(S), p(C)]\n",
    "\n",
    "Of course, a neural network will not accept a string input as it is, so we will have to deal with this by converting each letter in our alphabet into an integer. Then, we will use word embeddings to translate the integers into vectors of floating points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To work on google colab\n",
    "\n",
    "[Click on this link](https://colab.research.google.com/github/NBISweden/workshop-neural-nets-and-deep-learning/blob/master/session_goodPracticesDatasetDesign/lab_validation/rigorous_train_validation_splitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "First, let's setup the colab environment, download dataset and other relevant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Uncomment this code in google colab to checkout the course repository and unzip the data\n",
    "\n",
    "#%cd /content\n",
    "#!git clone https://github.com/NBISweden/workshop-neural-nets-and-deep-learning.git\n",
    "#%cd /content/workshop-neural-nets-and-deep-learning/session_goodPracticesDatasetDesign/lab_validation\n",
    "!wget -v -O data/dataset_sseq_singleseq.pic -L   https://liu.box.com/shared/static/egutdq7rb5q3gaiu0fbjy9csl28ogpo9\n",
    "!wget -v -O data/trainset_distance_matrix.tsv -L https://liu.box.com/shared/static/xbgslccvlbosodtd585n7nh7s6hpmjoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load libraries and plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On MacOsx, if you experience dying kernel when \n",
    "# running `model_sseq2.fit` you might want to uncomment\n",
    "# the following lines\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import h5py\n",
    "import sys\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, LSTM, Conv1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0.4, 1.0])\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that returns a keras model. Modify the code below to try different architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def get_model(convolutional = False, window=31):# This switches between a convolutional or a recurrent architecture \n",
    "    embed_size = 64\n",
    "    bidir_size = 32\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(21, embed_size, input_shape=(window,)))\n",
    "    \n",
    "    if convolutional:\n",
    "        model.add(Conv1D(32, 7, activation='relu'))\n",
    "        model.add(Conv1D(16, 5, activation='relu'))\n",
    "        model.add(Conv1D(8, 3, activation='relu'))\n",
    "    else:\n",
    "        model.add(Bidirectional(LSTM(bidir_size, return_sequences=True)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    #MODEL\n",
    "    print('Compiling the model...')\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the architecture above:\n",
    "* What does putting the variable \"convolutional\" to False mean? What happens to the bidirectional layers when we are using a convolutional architecture?\n",
    "* Which architecture would be best for this type of dataset in your opinion?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_steps(target_list_data, batch=None):\n",
    "    \n",
    "    n_targets = len(target_list_data)\n",
    "    \n",
    "    target_index = 0\n",
    "    n_steps = 0\n",
    "    for target in target_list_data:\n",
    "\n",
    "        try:\n",
    "            X_batch = X[target]\n",
    "            #print(X_batch.shape)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        length = X_batch.shape[0]\n",
    "                                             \n",
    "        n_steps += int(length / batch)\n",
    "    return n_steps\n",
    "        \n",
    "def generate_inputs_window(X, y, target_list_data, batch=None, shuffle=False):\n",
    "\n",
    "    n_targets = len(target_list_data)\n",
    "    if shuffle:\n",
    "        random.shuffle(target_list_data)\n",
    "    \n",
    "    target_index = 0\n",
    "    while 1:\n",
    "        \n",
    "        target = target_list_data[target_index]\n",
    "        target_index += 1\n",
    "        target_index = target_index % n_targets\n",
    "\n",
    "        # create numpy arrays of input data\n",
    "        # and labels, from each line in the file\n",
    "        try:\n",
    "            X_batch = X[target]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        length = X_batch.shape[0]\n",
    "                                        # 0...00length00...0, max_depth                    \n",
    "            \n",
    "        labels_batch_sseq = y[target]\n",
    "        \n",
    "       # print(np.array(X_batch_windows).shape)\n",
    "        \n",
    "        for i in range(0, length, batch):\n",
    "            # length x 1 (sparse, 3 class)\n",
    "            yield (X_batch[i:i+batch], labels_batch_sseq[i:i+batch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the dataset as a pickle object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "(X,y) = pickle.load(open(\"data/dataset_sseq_singleseq.pic\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's start by taking the classical approach of randomly splitting the data in a trainset and a validation set (95%/5% by default, but you can change the ratio as you prefer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_dataset = 'data/dataset_sseq_singleseq.hdf5'\n",
    "target_list_path = 'data/train_set'\n",
    "batch = 256\n",
    "\n",
    "target_list_file = open(target_list_path)\n",
    "target_list = target_list_file.readlines()\n",
    "random.shuffle(target_list)\n",
    "n_targets = len(target_list)\n",
    "train_list = target_list[int(n_targets/20):] #95% train\n",
    "validation_list = target_list[:int(n_targets/20)] #5% validation\n",
    "\n",
    "train_steps = count_steps(train_list, batch)\n",
    "validation_steps = count_steps(validation_list, batch)\n",
    "print(\"Validation batches:\", validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run the following hidden cell if you don't want to spend a long time waiting for the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "#DON'T RUN THIS DURING A LAB\n",
    "#from keras.utils.io_utils import HDF5Matrix\n",
    "#def load_inputs(target_list_data, shuffle=False):\n",
    "#\n",
    "#    X = {}\n",
    "#    y = {}\n",
    "#    \n",
    "#    n_targets = len(target_list_data)\n",
    "#    if shuffle:\n",
    "#        random.shuffle(target_list_data)\n",
    "#            \n",
    "#    target_index = 0\n",
    "#    \n",
    "#    for target_index in range(n_targets):\n",
    "#\n",
    "#        target = target_list_data[target_index]\n",
    "#\n",
    "#        try:\n",
    "#            X_batch = np.asarray(HDF5Matrix(input_dataset, 'inputs_windows/' + target))  # length x max_depth\n",
    "#        except:\n",
    "#            continue\n",
    "#\n",
    "#        length = X_batch.shape[0]\n",
    "#                                        # 0...00length00...0, max_depth                    \n",
    "#        \n",
    "#        X[target] = X_batch\n",
    "#        y[target] = np.squeeze(np.asarray(HDF5Matrix(input_dataset, 'labels_sseq/' + target)))\n",
    "#    return X, y\n",
    "#\n",
    "#train_test_data = open(\"data/train_test_set\", 'r').readlines()\n",
    "#X,y = load_inputs(train_test_data)\n",
    "#\n",
    "#import pickle\n",
    "#pickle.dump((X,y), open(\"data/dataset_sseq_singleseq.pic\", 'wb'))\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's train the model a first time (train and validation split randomly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sseq = get_model(convolutional=True) #get a fresh model\n",
    "hist = model_sseq.fit(generate_inputs_window(X,y,train_list, batch), \n",
    "               validation_data=generate_inputs_window(X,y,validation_list, batch), \n",
    "               epochs=40, steps_per_epoch=train_steps, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training curves below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the best validation performance that you can extract from your Model?\n",
    "* What would be the best naïve classifier for this dataset? How does the validation performance of your model compare to it?\n",
    "* What do you think of randomly splitting the dataset this way? Can you think of a better way of doing it? Can you think of a _worse_ day of doing it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset by sequence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used HHblits (a software to perform sequence alignments) to find out just how distant the proteins in the dataset are, evolutionarily speaking. This distance goes from 0 (sequences are identical) to 1 (no relationship between the proteins could be detected at all). The distance is basically an inverse measure of how similar the sequences are to each other.\n",
    "\n",
    "This information is stored in a distance matrix of size NxN, where N is the number of sequences in the dataset. In the code block below I load the distance matrix from the filesystem, then we use the data to perform [linkage clustering](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) and plot a [dendrogram](https://en.wikipedia.org/wiki/Dendrogram) to visualize the clusters.\n",
    "\n",
    "In the dendrogram below we can see how proteins group together at various distance thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(100000) #fixes issue with scipy and recursion limit\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "distance_matrix = pd.read_csv('data/trainset_distance_matrix.tsv', sep='\\t')\n",
    "dists = squareform(distance_matrix)\n",
    "linkage_matrix = linkage(dists, \"single\")\n",
    "dendrogram(linkage_matrix, color_threshold=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we choose a threshold to get our cluster based on the distance threshold t. So we \"cut\" the dendrogram above at the threshold t, and all the proteins that fall under the same branch at that threshold will be put in the same cluster. Feel free to get a feeling of how clusters are formed/split by varying the threshold below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = fcluster(linkage_matrix,criterion='distance', t=0.8)\n",
    "print(len(cluster_assignments), np.max(cluster_assignments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a training and a validation set based on these clusters in such a way that a cluster of protein is EITHER in train OR in validation. Depending on the threshold we have picked, this could make sure that no proteins in the validation set are too similar to those in the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list_file = open(target_list_path)\n",
    "target_list = target_list_file.readlines()\n",
    "\n",
    "train_list_cluster = []\n",
    "validation_list_cluster = []\n",
    "validation_size_limit = int(n_targets/20)\n",
    "\n",
    "for i in range(1,np.max(cluster_assignments)+1):\n",
    "    index_this_cluster = np.where(cluster_assignments == i)[0]\n",
    "    if len(validation_list_cluster) < validation_size_limit: #add all elements in this cluster to either validation or train set\n",
    "        validation_list_cluster += [target_list[element] for element in index_this_cluster]\n",
    "    else:\n",
    "        train_list_cluster += [target_list[element] for element in index_this_cluster]\n",
    "\n",
    "random.shuffle(train_list_cluster)\n",
    "validation_steps_cluster = count_steps(validation_list_cluster, batch)\n",
    "print(\"Validation batches:\", validation_steps_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a new model with the new datasets and see if we get different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_sseq2 = get_model(convolutional=False) #get a fresh model\n",
    "\n",
    "hist2 = model_sseq2.fit(generate_inputs_window(X,y,train_list_cluster, batch), \n",
    "               validation_data=generate_inputs_window(X,y,validation_list_cluster, batch), \n",
    "               epochs=40, steps_per_epoch=1000, validation_steps=validation_steps_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot again the training curves from the first model and compare them to those from the new model. \n",
    "\n",
    "What are the differences, if any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the two models on previously unseen data. Which performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = open(\"data/test_set\").readlines()\n",
    "test_steps = count_steps(test_list, batch)\n",
    "print(\"Test steps:\", test_steps)\n",
    "res1 = model_sseq.evaluate(generate_inputs_window(X,y,test_list, batch), verbose=1, steps=test_steps)\n",
    "res2 = model_sseq2.evaluate(generate_inputs_window(X,y,test_list, batch), verbose=1, steps=test_steps)\n",
    "print(f\"Model 1 test acc: {res1[1]}, Model 2 test accuracy: {res2[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have extra time and want to play more with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make things even worse on purpose: whenever a cluster contains more than one sample, let's put half in the training set and half in the validation set. Then let's not shuffle the trainset so that the network sees those samples first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list_file = open(target_list_path)\n",
    "target_list = target_list_file.readlines()\n",
    "\n",
    "train_list_bad = []\n",
    "validation_list_bad = []\n",
    "validation_size_limit = int(n_targets/20)\n",
    "\n",
    "for i in range(1,np.max(cluster_assignments)+1):\n",
    "    index_this_cluster = np.where(cluster_assignments == i)[0]\n",
    "\n",
    "    if len(index_this_cluster) > 1: #add all elements in this cluster to either validation or train set\n",
    "        half_elements = int(len(index_this_cluster)/2)\n",
    "        validation_list_bad += [target_list[element] for element in index_this_cluster[:half_elements]]\n",
    "        train_list_bad += [target_list[element] for element in index_this_cluster[half_elements:]]\n",
    "    \n",
    "validation_steps_bad = count_steps(validation_list_bad, batch)\n",
    "print(\"Validation batches:\", validation_steps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sseq3 = get_model(convolutional=True, window=window) #get a fresh model\n",
    "\n",
    "hist3 = model_sseq3.fit(generate_inputs_window(X,y,train_list_bad, batch), \n",
    "               validation_data=generate_inputs_window(X,y,validation_list_bad, batch), \n",
    "               epochs=40, steps_per_epoch=1000, validation_steps=validation_steps_bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(hist)\n",
    "plot_loss_acc(hist2)\n",
    "plot_loss_acc(hist3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
