<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Artificial neural networks (ANNs)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Bengt Sennblad" />
    <meta name="date" content="2019-12-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">

class: inverse, middle, center
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=800px&gt;&lt;/html&gt; 

# Artificial neural networks (ANNs)
## An introduction to of their basic underlying theory
### Bengt Sennblad, NBIS
### 2019-12-18


???
- frustrated of ANNs
- black box -- ? under the hood
- examination committe
- Many already knows -- apologies
- hopefully, some benefit



&lt;style&gt;

.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 6px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: #EB811B;
}

.orange {
  color: #EB811B;
}
&lt;/style&gt;



---
# Artificial Neural networks jargon terms

.pull-left[
#### Activation function/Neuron models
- Perceptron
- **Sigmoid**
- ReLu 
- Tanh
- SoftMax
- LSTM

#### Network structure
- **Feedforward**
- Recurrent
- Convolutional

&lt;hr&gt;
.font70[*Refs*
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) by Michael Nielsen 
- [Colah's blog](http://colah.github.io) by Cristopher Olah]
]
].pull-right[

#### Cost functions
- **Quadratic cost** (MSE)
- Cross-entropy cost
- Exponentional cost
- Kullback–Leibler divergence

#### Optimization
- **Gradient descent**
  + **Back-propagation**
- Adam optimization algorithm
- Regularization

#### Application
- Aim (prediction vs explanation)
- Training set size (dimensionality curse)
]
???
- lot of terms -- difference
- I'll go through some
- future JC -- Reading course

---
class: inverse, middle, center
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=800px&gt;&lt;/html&gt; 

# Neuron models and networks

---


# Background

Inspired by biological neural networks.
Mimic neurons (at the most basic level):

- Single output *state*
    + 0 (inactive) or 
    + 1 (activated)
- State is a function of 1 - several inputs
- State transmitted as input of 1 - many downstream neurons.


&lt;img src="1*vBIWWCFLLzZzGes1HcCJOw.jpg" width="500" height="280" style="display: block; margin: auto;" /&gt;

.footnote[.font70[(c) fig from `https://medium.com/predict/artificial-neural-networks-mapping-the-human-brain-2e0bd4a93160`.]]

---

# Perceptron

A simple neuron model is the perceptron
&lt;img src="ann1_files/figure-html/perceptron-1.png" width="500" height="250" style="display: block; margin: auto;" /&gt;


- *multivariate linear model* `\(z=\sum_j w_j a'_j + b\)` over the inputs `\(a'\)` 
- use `\(z\)` to determine the output `\(a\)`
`$$a = 
\begin{cases} 
1 &amp; \quad\textrm{if} \quad z &gt;0\\ 
0 &amp; \quad\textrm{otherwise} 
\end{cases}$$`
  + `\(w\)` weights (coefficients)
  + `\(b\)` bias, threshold for *activation*

???

Models biological neuron as ...


Learning benefits from doing small changes. non-active-activated flip is to sensitive to small changes.
---
# Sigmoid neuron

It turns out that the perceptorn is a suboptimal for ANN learning. the situation can be substantially improved by relaxing the on/off output of the perceptron -- Enter the *sigmoid neuron*:

&lt;img src="ann1_files/figure-html/sigmaNeuron-1.png" width="500" height="250" style="display: block; margin: auto;" /&gt;

The basis is still a linear model, but the  output is modified by an *activation function* `\(\sigma\)` 


---

# Sigmoid neuron
.pull-left[
To compute the output, `\(a\)`, 

- first let
`$$z = \sum_{j=1}^M w_j a'_j + b$$`
- then determine `\(a\)` from `\(z.\)`
`$$a = \sigma(z),$$`
    + `\(\sigma(z)\)` is a *sigmoid function*, more specifically the *logistic function*:
    `$$\sigma(z) = \frac{1}{1+e^{-z}}$$`
]
.pull-right[
&lt;img src="ann1_files/figure-html/logistic-1.png" height="300" /&gt;

]


The output is now continuous, but *tends* towards either `\(0\)` or `\(1.\)`
The *bias*, `\(b\)`, can still be viewed as a threshold for *activation*, as it moves the tendency to activation.

---
# Sigmoid neuron

.footnote[.font70[ [1] also the perceptron can be viewed as a GLM. The threshold forms the activation function.]]

In fact, the sigmoid neuron represents a *generalized linear model* (*GLM*)&lt;sup&gt;1&lt;/sup&gt;, specifically the logistic model

In logistic regression, it is more common to describe GLMs in terms a link function that transforms the outcome:

`$$logit(a) = \sum_{j=1}^M w_ja'_j + b,$$`

where `\(logit(a) = \sigma^{-1}(a)\)` is called the link function.

A sigmoid neuron can thus be viewed simply as a multivariate logistic model. 

--

However, connected in a network they can do more.

---

# Sigmoid Feedforward ANN


In a *feedforward* ANN, neurons are arranged into layers: single *input* and *output* layers and a number of *hidden* layers.

The output of one layer forms the input of the next layer. 
.pull-left[
&lt;img src="ann1_files/figure-html/sigmoidAnn-1.png" width="700" height="300" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="ann1_files/figure-html/sigmoidAnn2-1.png" width="900" height="300" style="display: block; margin: auto;" /&gt;
]

???
- DAG
- Each layers can have any number of neurons.
- Specially, 
    + size of input layer should = input size 
    + size of output layer should = output size 

--

Each layer can be viewed as transforming the original data to a new multi-dimensional space.

???
So while each neuron performs rather simple transformations, the sum can be very complex

---

# ANN example -- classify _red_ vs _blue_

.footnote[.font70[(c) Figs from [Colah's blog](`colah.github.io/DeepIntro`) ]]

.pull-left[
#### input

&lt;img src="simple2_data.png" width="150" /&gt;

#### linear model fails 

&lt;img src="simple2_linear.png" width="150" /&gt;
]
???
- `\(r/b = w_0+w_1x+w_2y+b\)` ?

--
.pull-right[
#### ANN: result as hidden transformation

&lt;img src="simple2_1.png" width="150" /&gt;

#### ANN: result in original view

&lt;img src="simple2_0.png" width="150" /&gt;
]


---

# Summary Neuron models and networks

#### Sigmoid neuron is a logistic model

`\begin{eqnarray}
z &amp;=&amp; \sum_j w_ja'_j+b\\
a &amp;=&amp; \sigma(z)
\end{eqnarray}`

#### Feedforward network 
- Neurons arranged into layers
    + Input layer (0)
    + Output layer (L)
    + Hidden layers `\((0&lt;\ell &lt;L)\)`
- Each hidden layer transforms the input into a new multi-dimensional space which is fed to the next layer

#### Parameters
- `\(\{w_j^{(\ell)}\}\)` and `\(b^{(\ell)}\)` for all layers `\(\ell \in (1,L)\)`

???
- Next: how to determine theparameters (learning)

---
class: inverse, middle, center
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=800px&gt;&lt;/html&gt; 

# Learning
## Cost function and Gradient descent

---

# Learning

## Task
Find optimal values for `\(w_i\)` and `\(b\)` over all neurons
.pull-left[
## Data
Cross-validation approach
* Training set
* .orange[Validation set]
* .orange[Test set]
].pull-right[
## Tools
* Cost function
* Gradient descent
  + Back-propagation
* .orange[Cross-validation]
]

???
- orange = not treated
- gradient descent to optimise fo cost function
- back-prop to do this efficiently
---

# Simplifying notation: relaxed indexing
### Training data sets
.pull-left[

&lt;img src="ann1_files/figure-html/sigmoidAnnrep0-1.png" width="100%" style="display: block; margin: auto;" /&gt;
].pull-right[
#### Training data:

`\(X=\{X_1,\ldots, X_k\}\)`


#### Relaxed indexing:

`\begin{eqnarray}
\textrm{Full}
&amp;:&amp;\begin{cases}
X_k &amp; \textrm{(input)}\\
Y(X_k) &amp; \textrm{(true output)}\\ 
\widehat{Y}(X_k) = a^{(L)}(X_k) &amp; \textrm{(estimated output)}
\end{cases}\\
\quad\\
\quad\\
\textrm{Simple}
&amp;:&amp;\begin{cases}
x &amp; \textrm{(input)}\\
y &amp;  \textrm{(true output)}\\
\hat{y} = a^{(L)}&amp;  \textrm{(estimated input)}
\end{cases}`
\end{eqnarray} 
]

---


# Simplifying notation: relaxed indexing
#### Variables
.pull-left[
&lt;img src="ann1_files/figure-html/sigmoidAnnrep2-1.png" width="80%" style="display: block; margin: auto auto auto 0;" /&gt;
#### Full
`\(\quad\begin{array}{lll}a_i^{(\ell-1)} \qquad&amp; a_i^{(\ell)} \qquad&amp; a_i^{(\ell+1)}\\z_i^{(\ell-1)} &amp; z_i^{(\ell)} &amp; z_i^{(\ell+1)}\\w_{ij}^{(\ell-1)} &amp; w_{ij}^{(\ell)} &amp; w_{ij}^{(\ell+1)}\\b_i^{(\ell-1)} &amp; b_i^{(\ell)} &amp; b_i^{(\ell+1)}\end{array}\)`
]

--

.pull-right[
&lt;img src="ann1_files/figure-html/sigmaNeuron10-1.png" width="500" height="250" style="display: block; margin: auto;" /&gt;
#### Simple

`\(\quad\quad\begin{array}{lll}a' \qquad\qquad&amp; a \qquad\qquad&amp; a''\\z' &amp; z &amp; z''\\w'_j &amp; w_j &amp; w''_j\\b' &amp; b &amp; b''\end{array}\)`

]

---

# Simplifying notation

**Vectors:** We can further remove annoying summing over indices by letting
`\begin{eqnarray}
w=\begin{pmatrix}w_1\\\vdots\\w_M\end{pmatrix} &amp;\qquad&amp; a'=\begin{pmatrix}a'_1\\\vdots\\a'_M\end{pmatrix}
\end{eqnarray}`
and use vector multiplication
$$ z=w^Ta' +b = \sum_i w_ia'_i +b$$
---

# Simplifying notation

**Vectors:** We can further remove annoying summing over indices by letting
`\begin{eqnarray}
w=\begin{pmatrix}w_1\\\vdots\\w_M\end{pmatrix} &amp;\qquad&amp; a'=\begin{pmatrix}a'_1\\\vdots\\a'_M\end{pmatrix}
\end{eqnarray}`
and use vector multiplication
$$ z=w^Ta' +b = \sum_i w_ia'_i +b$$
---

# Supervised learning: cost function

Suppose we have training samples `\(X=(X_1,\ldots,X_K)\)` with true classifications `\(Y=(Y_1,\ldots,Y_K)\)`. Then the *quadratic cost function* is defined as follows:

???
Borrow some established concepts
- RSS from regression
-MSE from cross-validation

--
.pull-left[

1 For each `\(X_k\)`, use the residual sum of squares, *RSS*, as an error measure

`$$C(w,b|X_k) = \sum_i\frac{1}{2} \left( Y_i(X_k)-a_i^{(L)}(X_k)\right) ^2$$`
 
]

.pull-right[

&lt;img src="ann1_files/figure-html/rss-1.png" height="200" /&gt;

]

???
* The factor 1/2 is included for convenience  (see later)

--

2 The full quadratic cost function is  simply the Mean Squared Error (MSE) used in cross-validation
`\begin{eqnarray}
C(w,b) &amp;=&amp;  \frac{1}{K} \sum_{k=1}^K C(w,b|X_k)\\
%&amp;=&amp; \frac{1}{2K}\sum_{k=1}^K \Vert Y(X_k)-a^{(L)}(X_k)\Vert ^2
\end{eqnarray}`

???
Now we got something to optimize for , let's optimize

---

# Gradient descent -- "clever hillclimbing"
Consider inverted hill-climbing in one dimension `\(v\)`, i.e., we want to find the minimum instead of the maximum.

.pull-left[
####*Hill-climbing* 
1. randomly choose direction and length to change `\(v\)`
2. stay if `\(C(v|x)\)` got better, else go back.

We want to be smarter!

]

.pull-right[
![](ann1_files/figure-html/descent1-1.png)&lt;!-- --&gt;
]



---

# Gradient descent -- "clever hillclimbing"
Consider inverted hill-climbing in one dimension `\(v\)`, i.e., we want to find the minimum instead of the maximum.

.pull-left[
####*Hill-climbing* 
1. randomly choose direction and length to change `\(v\)`
2. stay if `\(C(v|x)\)` got better, else go back.

We want to be smarter!

####*Gradient descent*
1. compute the derivative `\(\frac{dC(v|x)}{dv}\)` to see which way *down* is

]

.pull-right[
![](ann1_files/figure-html/descent3-1.png)&lt;!-- --&gt;
]

---

# Gradient descent -- "clever hillclimbing"
Consider inverted hill-climbing in one dimension `\(v\)`, i.e., we want to find the minimum instead of the maximum.

.pull-left[
####*Hill-climbing* 
1. randomly choose direction and length to change `\(v\)`
2. stay if `\(C(v|x)\)` got better, else go back.

We want to be smarter!

####*Gradient descent*
1. compute the derivative `\(\frac{dC(v|x)}{dv}\)` to see which way *down* is
2. Take a reasonably long step in that direction, `\(v' = v-\eta\frac{dC(v|x)}{dv}\)`

]

.pull-right[
![](ann1_files/figure-html/descent4-1.png)&lt;!-- --&gt;
]

???

Notice the minus sign: the derivative show which way is up and wewant to go down.

---

# Gradient descent in higher dimensions


.pull-left[
&lt;img src="ann1_files/figure-html/twodim-1.png" width="600" /&gt;
]
.pull-right[
Same thing really, but we have to have *partial derivatives* for each dimension, which makes it look more complicated. 

Consider a 2-dimensional case,

1 Find the (partial) derivatives

`$$\begin{pmatrix}
\frac{\partial C(v_1,v_2|x)}{\partial v_1}\\
\frac{\partial C(v_1,v_2|x)}{\partial v_2}
\end{pmatrix}$$`
]

???
- `\(\partial\)` indicates partial derivative on one of the free parameters.
--
.pull-right[
2 Take a resonably long step
`$$\begin{pmatrix} v'_1\\ v'_2\end{pmatrix} = \begin{pmatrix}v_1-\eta\frac{\partial  C(x,w)}{\partial v_1} \\ v_2-\eta\frac{\partial C(x,v)}{\partial v_2} \end{pmatrix}$$`

]

---


# Gradient descent in higher dimensions 
### Some notes

1. Defining the gradient `\(\nabla_w C(w,b|x) = \begin{pmatrix}\frac{\partial C(w,b|x)}{\partial w_1}\\\ldots\\\frac{\partial C(w,b|x)}{\partial w_M}\end{pmatrix}\)`
simplifies notation. 
`$$w'=w- \eta \nabla_w C(w,b|x)$$`
&lt;br&gt;
--

2. The bias `\(b\)` can be treated analogously
`$$b'= b-\eta \nabla_b C(w,b|x)$$`
&lt;br&gt;

---

# Summary Learning

#### Cost function
`\begin{eqnarray}
C(w,b) &amp;=&amp; \frac{1}{2K}\sum_{k=1}^K  C(w,b|X_k)\\
C(w,b|x) &amp;=&amp; \sum_i\left(y_i-a_i^{(L)}\right)^2
\end{eqnarray}`

- Mean squared error (MSE)
- Residual sum of squares (RSS)

#### Gradient descent

- "Clever hill-climbing" in several dimensions
- Change all variables `\(v\)` by taking a reasonable step in opposite direction to the gradient 
`\begin{equation}
v' = v-\eta \nabla_{v}C(w,b|x)
\end{equation}`

--
&lt;br&gt;&lt;br&gt;
For this to work, we need to be able to **compute `\(\nabla_v C(w,b|x)\)` efficiently**

???
- This is where back-propagation comes in
---
class: inverse, middle, center
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=800px&gt;&lt;/html&gt; 

# Back-propagation
## Re-inventing the *chain rule*

---

# Chain rule of derivation -- a reminder
The chain rule simplifies derivation of complex functions and states that
`$$\frac{d f(g(x))}{dx} = \frac{dg(x)}{d x} \times \frac{df(g(x))}{d g(x)}$$`

???

- continued fraction


--
This notation is for derivatives of scalar `\(x\)`, but for vector `\(x\)`, the rule works also for partial derivatives

$$\frac{\partial f(g(x))}{\partial x_i} =  \frac{\partial g(x)}{\partial x_i} \times \frac{\partial f(g(x))}{\partial g(x)} $$
--
I will use `\(\otimes\)` to denote the chain rule applied (independently) to a vector of partial derivatives (a gradient):

`$$\nabla_x f(g(x)) =  \nabla_x g(x) \otimes \nabla_{g(x)}f(g(x))$$`
--
BTW, elementwise multiplications will be indicated by, e.g., `\(\nabla_x g(x) \circ \nabla_{g(x)}f(g(x))\)`
---

# Chain rule of derivation -- an example
The derivative of the sigmoid function `\(\sigma(z)=\frac{1}{1+e^{-z}}\)`:
???

`$$\begin{eqnarray*}
\frac{d\sigma(z)}{d z}
%&amp;=&amp;\frac{d \frac{1}{1+e^{-z}}}{d z}\\ 
&amp;=&amp; \frac{d (1+e^{-z})^{-1}}{d z}\\
&amp;=&amp;  \frac{d (1+e^{-z})}{d z} \quad\times\quad \frac{d (1+e^{-z})^{-1}}{d (1+e^{-z})}\\
&amp;=&amp; (-e^{-z}) \quad\times\quad -(1+e^{-z})^{-2}\\
&amp;=&amp; \frac{e^{-z}}{(1+e^{-z})^{2}} \\
&amp;=&amp; \frac{1}{1+e^{-z}}\quad \left(1-\frac{1}{1+e^{-z}}\right)\\\\
&amp;=&amp; \sigma(z)\left(1-\sigma(z)\right)
\end{eqnarray*}$$`

--

`$$\begin{eqnarray*}
\frac{d\sigma(z)}{d z}
%&amp;=&amp;\frac{d \frac{1}{1+e^{-z}}}{d z}\\ 
&amp;=&amp; \frac{d (1+e^{-z})^{-1}}{d z}\\
&amp;=&amp;  \frac{d (1+e^{-z})}{d z} \quad\times\quad \frac{d (1+e^{-z})^{-1}}{d (1+e^{-z})}\\
&amp;=&amp; (-e^{-z}) \quad\times\quad -(1+e^{-z})^{-2}\\
&amp;=&amp; \frac{e^{-z}}{(1+e^{-z})^{2}} \\
&amp;=&amp; \frac{1}{1+e^{-z}}\quad \left(1-\frac{1}{1+e^{-z}}\right)\\\\
&amp;=&amp; \sigma(z)\left(1-\sigma(z)\right)
\end{eqnarray*}$$`


---

# Back-propagation -- Strategy

The chain-rule is used to compute `\(\frac{\partial C(w,b|x)}{\partial v}\)`, for any `\(v\in\{w,b\}\)` of any neuron in the network, by sequential
application to each layer `\(\ell\)`, starting from the output layer `\(L\)`.
.pull-left[
&lt;img src="ann1_files/figure-html/sigmoidAnn3-1.png" width="900" height="300" style="display: block; margin: auto;" /&gt;
].pull-right[
&lt;img src="ann1_files/figure-html/sigmaNeuron3-1.png" width="500" height="250" style="display: block; margin: auto;" /&gt;
`$$\pmatrix{\cdots\\ a''\\ \cdots}\overset{w, b}{ \Longrightarrow} z \quad \overset{\sigma}\Rightarrow\quad a'$$`

]

Notice that `\(z\)` is the breaking point between layer dimensions!

???
- several variables and complicated dependencies
- but very regular
- recursion

---

# Back-propagation -- Strategy

We use the chain rule selectively targeting `\(z\)` to get, for `\(v\in w \cup b\)` of neuron `\(i\)` of layer `\(\ell\)`,   
\begin{eqnarray}\frac{\partial C(w,b|x)}{\partial v} &amp;=&amp;\frac{\partial z_i^{(\ell)}}{\partial v} \times \underset{{\Bigg\Uparrow}}{\frac{\partial C(w,b|x)}{\partial z_i^{(\ell)}}}\end{eqnarray}

???
- Really point out that$v$ belongs to neuron `\(i\)`

--

This allows us to use recursion to design a *Dynamic Programming* algorithm:
--

- For each layer `\(\ell\in (L,L-1,\ldots, 1)\)`, i.e., starting with the output layer:
--

  + Compute `\(\delta^{(\ell)} = \nabla_{z^{(\ell)}} C(w,b|x)\)` recursively, i.e., 
--

      - Store computed `\(\delta^{(\ell)}\)` in a table `\(\delta\)`
--
      - Re-use any computed `\(\delta^{(\ell+1)}\)` for computation of  `\(\delta^{(\ell)}\)`
      
???
 `\(\nabla_{z^{(\ell)}} C(w,b|x)\)` vector of partial derivatives `\(\frac{\partial C(w,b|x)}{\partial z_i^{(\ell)}}\)`
      
--

- For any requested `\(v\in  w\cup b\)` of some neuron `\(i\)` in layer `\(\ell\)`
--

  + Look up `\(\delta^{(\ell)}\)` in the a table
--

      - `\(\frac{\partial C(w,b|x)}{\partial v} = \frac{\partial z_i^{(\ell)}}{\partial v} \times \delta_i^{(\ell)}\)`

---

# Back-propagation -- the output layer `\(L\)` 

&lt;img src="ann1_files/figure-html/sigmoidAnn4-1.png" width="650" height="400" style="display: block; margin: auto auto auto 0;" /&gt;
`\(\hspace{9.7cm}\begin{array}{rcccl}&amp;&amp;z_i &amp;\Rightarrow&amp; a_i\\\frac{\partial C(w,b|x)}{\partial z_i} &amp;=&amp;\frac{\partial a_i}{\partial z_i}&amp;\times&amp; \frac{\partial C(w,b|x)}{\partial a_i}\end{array}\)`

---
# Back-propagation -- the output layer `\(L\)` 

Recall 
`$$C(w,b|x) = \sum_i\frac{1}{2}\left(y_i-a_i\right)^2
\qquad \Rightarrow \qquad \frac{\partial C(w,b|x)}{\partial a_i} = y_i-a_i$$`

???

`\begin{eqnarray}
\frac{\partial C(w,b|x)}{\partial z_i}
&amp;=&amp; 
\frac{\partial a_i}{\partial z_i}
\quad\times\quad
\frac{\partial C(w,b|x)}{\partial a_i}
\\
&amp;=&amp; 
\frac{\partial \sigma(z_i)}{\partial z_i}
\quad\times\quad
\frac{\partial \frac{1}{2}(y_i-a_i)^2}{\partial a_i}
\\
&amp;=&amp;  
\sigma(z_i)(1-\sigma(z_i))
\quad\times\quad
(y_i-a_i)
\end{eqnarray}`
--

Then for neuron `\(i\)` in `\(L\)`
`\begin{eqnarray}
\frac{\partial C(w,b|x)}{\partial z_i}
&amp;=&amp; 
\frac{\partial a_i}{\partial z_i}
\quad\times\quad
\frac{\partial C(w,b|x)}{\partial a_i}
\\
&amp;=&amp; 
\frac{\partial \sigma(z_i)}{\partial z_i}
\quad\times\quad
\frac{\partial \frac{1}{2}(y_i-a_i)^2}{\partial a_i}
\\
&amp;=&amp;  
\sigma(z_i)(1-\sigma(z_i))
\quad\times\quad 
(y_i-a_i)
\end{eqnarray}`
--
As a gradient over all neurons
`$$\delta^{(L)} = \nabla_{z^{(L)}} C(w,b|x)
= \pmatrix{\cdots\\\frac{\partial C(w,b,|x)}{\partial z_i^{(L)}}\\\cdots} 
=  \nabla_{z^{(L)}}\sigma\left(z^{(L)}\right)\circ\left(y-a^{(L)}\right)$$`
---

# Back-propagation -- any hidden layer `\(\ell\)` 

&lt;img src="ann1_files/figure-html/sigmoidAnn5-1.png" width="650" height="400" style="display: block; margin: auto auto auto 0;" /&gt;
`\(\hspace{3.5cm}\begin{array}{rclcl}&amp;&amp;z_i\ \ \Rightarrow a_i &amp;\Rightarrow&amp; z'' \\\frac{\partial C(w,b|x)}{\partial z_i}&amp;=&amp;\frac{\partial a_i}{\partial z_i} \times\Big(\sum_j \frac{\partial z''_j}{\partial a_i} &amp;\times&amp; \frac{\partial C(w,b|x)}{\partial z''_j} \quad \Big) \end{array}\)`

---

# Back-propagation -- any hidden layer `\(\ell\)` 

Recall `$$\quad z''_j =\sum_i w''_{ij} a_i + b''_j \qquad \Rightarrow \qquad \frac{\partial z''_j }{\partial a_i} = w''_{ij}$$`

???

For each neuron `\(i\)` in layer `\(\ell\)`
`\begin{eqnarray}
\frac{\partial C(w,b|x)}{\partial z_i}
&amp;=&amp;\sum_j 
\frac{\partial C(w,b|x)}{\partial z''_j} \times
\frac{\partial z''_j}{\partial a_i}
\times \frac{\partial a_i}{\partial z_i}\\
&amp;=&amp;\sum_j 
\delta''_j
\times w''_{ij}
\times \frac{\partial \sigma(z_j)}{\partial z_i}\\
&amp;=&amp;
\left(w_{i\cdot}''\right)^T 
\times\delta''
\times \sigma(z_j)(1-\sigma(z_j))
\end{eqnarray}`
--

For each neuron `\(i\)` in layer `\(\ell\)`
`\begin{eqnarray}
\frac{\partial C(w,b|x)}{\partial z_i}
&amp;=&amp;
 \frac{\partial a_i}{\partial z_i}
\quad\times\quad
\left(\sum_j \frac{\partial z''_j}{\partial a_i}
\quad\times\quad
\frac{\partial C(w,b|x)}{\partial z''_j} \right) \\
&amp;=&amp;
\frac{\partial \sigma(z_i)}{\partial z_i}
\quad\times\quad 
\left(\sum_j w''_{ij}
\quad\times\quad  \delta''_j \right)
\\
&amp;=&amp;
\sigma(z_i)(1-\sigma(z_i))
\quad\times\quad 
\left({w_{i\cdot}''}^T 
\quad\times\quad \delta''\right)
\end{eqnarray}`
--

.pull-left[

If we define 
`$$W^{(\ell)} = \pmatrix{w_{ij}^{(\ell)}&amp;\cdots\\\vdots&amp;\ddots}$$`
].pull-right[

Then we can write
`\begin{eqnarray}
\delta^{(\ell)}&amp;=&amp;\nabla_{z} C(w,b,|x)\\
&amp;=&amp; \nabla_{z^{(\ell)}}\sigma(z^{(\ell)}) \circ W^{(\ell)}\times\delta^{(\ell+1)}
\end{eqnarray}`
]
---

# Back-propagation -- tying it up 

For any layer `\(\ell \in \{1,\cdots,L\}\)` and any neuron `\(i \in \ell\)`

`$$z = w^T a' +b = \sum_j w_ja'_j+b.$$` 

--

which gives the partial derivative `\(\frac{\partial z}{\partial v}\)`:

`\begin{equation}
\frac{\partial z}{\partial v}  
= \sum_j\frac{  \partial w_{j}a'_j+b}{\partial v}
= \begin{cases} 
a'_j &amp; \textrm{if } v =  w_j \\
1 &amp; \textrm{if }v= b\\
\end{cases} 
\end{equation}`

--
and hence, for `\(v\in w\cup b\)` of neuron `\(i\)`

`\begin{eqnarray}
\frac{\partial C(w,b|x)}{\partial{v}} =  \frac{\partial z_i}{\partial v}\times \delta_i
&amp;=&amp; \begin{cases} 
a'_i\times \delta_i &amp; \textrm{if } v=w\\
\delta_i  &amp; \textrm{if } v=b
\end{cases} 
\end{eqnarray}`

---

# Summary -- Back-propagation

### Efficient computation of `\(\nabla_{v} C(w,b|x)\)`
#### Compute `\(\delta\)`
`\begin{equation}
\delta^{(\ell)} = \begin{cases}
\left(y-a^{(L)}\right) \circ \nabla_{z^{(L)}}\sigma\left(z^{(L)}\right)
&amp; \textrm{if }\ell=L\\
\\
W^{(\ell+1)}\times\delta^{(\ell+1)}\circ\nabla_{z^{(\ell)}}\sigma(z^{(\ell)}) 
&amp; \textrm{if } 0&lt;\ell&lt;L
&amp;\\
\end{cases} 
\end{equation}`

&lt;br&gt;&lt;br&gt;

#### Compute `\(\nabla_{v} C(w,b|x)\)` for any `\(v \in w\cup b\)` of any neuron `\(i\)` in any layer `\(\ell\)`
`\begin{equation}
\nabla_{v} C(w,b|x) 
= \begin{cases} 
\delta_i^{(\ell)} \times a' &amp; \textrm{if } v=w \\
\delta_i^{(\ell)}  &amp; \textrm{if } v=b 
\end{cases} 
\end{equation}`



---

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div> "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
