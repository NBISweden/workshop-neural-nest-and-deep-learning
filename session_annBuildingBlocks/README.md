# session_annBuildingBlocks

## This session focus on:

0. (intro: motivation, neurons, perceptrons, learning)
    - classification/regression
1. The basic building blocks of a sigmoid
feed-forward neural net,
    - sigmoid activation function
    - the mean square error loss function
    - gradient descent optimization
    - back-propagation
2. Performance measures for
    - cost-plots
    -
    - ...?
3. Selected additional activation functions and their
main properties (pros/cons):
    - reLu
    - tanh
    - SoftMax
    - ...?
    - Also cover linear and and step (perceptron) elaborate little 
    on why they don't work
4. Selected additional loss (cost) functions
    - Cross entropy
    - (log likelihood?)
    - ...?
5. Selected additional optimizations
    - (ADAM?)
    - ...?

**_Note!_** The exact "selections" above will be aligned with the needs
of following sessions

### Thoughts/Questions:
1. Are the following best covered here or in sessions on convolutional
and recurrent anns, respectively?
  - LSTMs
  - Convolutions
2. Anything vital missing?
