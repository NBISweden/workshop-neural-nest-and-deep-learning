{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras intro exercises\n",
    "\n",
    "## 1. Build a simple sequential model\n",
    "\n",
    "* Can you build a sequential model to reproduce the graph shown in the figure? \n",
    "* Assume that this is a classifier\n",
    "* Choose whatever activations you want, wherever possible\n",
    "* How many classes are we predicting?\n",
    "\n",
    "<center><img src=\"figures/sequence_api_exercise.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/NBISweden/workshop-neural-nets-and-deep-learning/master/session_annBuildingBlocks/lab_keras/exercises.ipynb\n",
    "!mkdir figures\n",
    "!wget -P figures/ https://raw.githubusercontent.com/NBISweden/workshop-neural-nets-and-deep-learning/master/session_annBuildingBlocks/lab_keras/figures/sequence_api_exercise.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Add your model here\n",
    "model = Sequential()\n",
    "...\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, \"figures/exercise_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a better XOR classifier\n",
    "\n",
    "Given the model seen at lecture, how do we make a better classifier (higher accuracy)?\n",
    "\n",
    "* More layers? More neurons?\n",
    "* Generate more data?\n",
    "* More epochs?\n",
    "* Different batch size?\n",
    "* Different optimizer?\n",
    "* It's up to you! Let's see who does best on validation\n",
    "\n",
    "Only for Tuesday's session:\n",
    "\n",
    "* Different activations?\n",
    "* Add Dropout? How large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training curve plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    try:\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "    except:\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data\n",
    "data = np.random.random((10000, 3)) - 0.5\n",
    "labels = np.zeros((10000, 1))\n",
    "\n",
    "labels[np.where(np.logical_xor(np.logical_xor(data[:,0] > 0, data[:,1] > 0), data[:,2] > 0))] = 1\n",
    "\n",
    "#let's print some data and the corresponding label to check that they match the table above\n",
    "for x in range(3):\n",
    "    print(\"{0: .2f} xor {1: .2f} xor {2: .2f} equals {3:}\".format(data[x,0], data[x,1], data[x,2], labels[x,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The baseline network to improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a regression model\n",
    "\n",
    "* Take the Boston housing dataset (http://lib.stat.cmu.edu/datasets/boston)\n",
    "* Records a set of variables for a set of houses in Boston, including among others:\n",
    "    * CRIM     per capita crime rate by town\n",
    "    * ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    * INDUS    proportion of non-retail business acres per town\n",
    "    * CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    * NOX      nitric oxides concentration (parts per 10 million)\n",
    "    * RM       average number of rooms per dwelling\n",
    "* Can we use these variables to predict the value of a house (in tens of thousands of dollars)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from sklearn.preprocessing import StandardScaler #hint\n",
    "from keras.layers import Dropout\n",
    "#This is how we load the dataset, pre-split in training/validation sets\n",
    "(X_train, y_train), (X_val, y_val) = tensorflow.keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113)\n",
    "\n",
    "#Let's have a look at the data\n",
    "print(X_train.shape)\n",
    "print(X_train[0], y_train[0])\n",
    "\n",
    "\n",
    "# Add your model here\n",
    "model = Sequential()\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_mae(history, metric):\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(history.history[metric])\n",
    "    ax.plot(history.history['val_' + metric])\n",
    "    ax.set_ylabel(metric)\n",
    "    ax2=ax.twinx()\n",
    "    ax2.plot(history.history['loss'], c=\"red\")\n",
    "    ax2.plot(history.history['val_loss'], c=\"green\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "    plt.title('model accuracy')\n",
    "    #plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train ' + metric, 'val ' + metric, 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss_mae(history, \"mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The IMDB movie review sentiment dataset\n",
    "\n",
    "Another pre-package toy dataset from Keras. Contains 25k reviews for a movies in IMDB, you want to predict whether the review is positive or negative.\n",
    "\n",
    "> each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "https://keras.io/api/datasets/imdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset, set a couple of important parameters (max_features, maxlen). Also pad all reviews with less than 200 words so that they have all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, padding=\"post\")\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is pre-processed so that each word is represented by an integer, we have to build a reverse dictionary if we want to actually read some of the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default parameters to keras.datasets.imdb.load_data\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "\n",
    "inverted_word_index[0] = \"\"\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in x_train[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[2])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we build a predictor for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
