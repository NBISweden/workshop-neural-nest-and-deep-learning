{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Keras, Tensorflow and advanced NN, part 2\n",
    "\n",
    "\n",
    "<center><img src=\"figures/keras-tensorflow-logo.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of first part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "* A dataset in supervised learning is made of a number of (features, label) pairs\n",
    "* Example, a dataset of diabetic patients is made of:\n",
    "    * Features: information describing each patient (weight, height, blood pressure...)\n",
    "    * Labels: whether each patient is diabetic or not (glucose levels higher or lower than...)\n",
    "* Each (features, label) pair is also called a _sample_ or _example_. Basically a data point\n",
    "* Features are also sometimes called _inputs_ when referred to something you feed to a NN\n",
    "* Labels are compared to the NN's _outputs_ to see how well the network is doing compared to the truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://keras.io\n",
    "\n",
    "* Keras is a high-level neural networks API (front-end), written in Python\n",
    "* Capable of running on top of TensorFlow, CNTK, or Theano (backends)\n",
    "* Built to simplify access to more complex backend libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://tensorflow.org\n",
    "\n",
    "Use *TensorFlow* if you want a finer level of control:\n",
    "\n",
    "* Build your own NN layers\n",
    "* Personalized cost function\n",
    "* More complex architectures than those available on Keras\n",
    "    \n",
    "We will be mostly writing python code using Keras libraries, but \"under the hood\" Keras is using tensorflow libraries.\n",
    "\n",
    "The documentation is at [keras.io](https://keras.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here's how a NN layer looks like in TensorFlow:\n",
    "\n",
    "* 7 samples in batch\n",
    "* 784 inputs\n",
    "* 500 outputs\n",
    "\n",
    "<center><img src=\"figures/run_metadata_graph.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A neural network in Keras is called a Model\n",
    "\n",
    "The simplest kind of model is of the Sequential kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is an \"empty\" model, with no layers, no inputs or outputs are defined either.\n",
    "\n",
    "Adding layer is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "A \"Dense\" layer is a fully connected layer as the ones we have seen in Multi-layer Perceptrons.\n",
    "The above is equal to having this network:\n",
    "\n",
    "<center><img src=\"figures/simplenet.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using \"model.add()\" keeps stacking layers on top of what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation=None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2, more Keras layers (https://keras.io/api/layers/)\n",
    "\n",
    "Common layers (we will cover most of these!)\n",
    "\n",
    "* Trainable\n",
    "    * <font color='red'>Dense (fully connected/MLP)</font>\n",
    "    * <font color='red'>Conv1D (2D/3D)</font>\n",
    "    * <font color='red'>Recurrent: LSTM/GRU/Bidirectional</font>\n",
    "    * <font color='red'>Embedding</font>\n",
    "    * <font color='red'>Lambda (apply your own function)</font>\n",
    "\n",
    "* Non-trainable\n",
    "    * <font color='red'>Dropout</font>\n",
    "    * <font color='red'>Flatten</font>\n",
    "    * BatchNormalization\n",
    "    * MaxPooling1D (2D/3D)\n",
    "    * Merge (add/subtract/concatenate)\n",
    "    * <font color='red'>Activation (Softmax/ReLU/Sigmoid/...)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout is a regularization layer\n",
    "\n",
    "* It's applied to a previous layer's output\n",
    "* Takes those outputs and randomly sets them to 0 with probability p\n",
    "* Other outputs are scaled up so that the sum of the inputs remains unchanged\n",
    "* if p = 0.5: `model.add(Dropout(0.5))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tf.Tensor(\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]], shape=(3, 4), dtype=float32)\n",
      "After:\n",
      "[[ 0.  4.  6.  0.]\n",
      " [ 0. 12. 14.  0.]\n",
      " [18. 20. 22. 24.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 07:58:31.056981: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-20 07:58:31.057209: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 07:58:31.058599: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "drop = Dropout(0.5, input_shape=(4,))\n",
    "data = tf.reshape(tf.range(1.0,13.0), (3, 4))\n",
    "\n",
    "print(\"Before:\", data, sep=\"\\n\")\n",
    "output = drop(data, training=True)\n",
    "print(\"After:\", K.eval(output), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout is a regularization layer\n",
    "\n",
    "* Applying the same input twice will give different results\n",
    "* Means that it is harder for the network to memorize patterns\n",
    "* Helps curb overfitting\n",
    "* Especially used with Dense() layers which are prone to overfitting\n",
    "* Active only at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tf.Tensor(\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]], shape=(3, 4), dtype=float32)\n",
      "After:\n",
      "[[ 2.  0.  0.  8.]\n",
      " [10.  0.  0. 16.]\n",
      " [ 0. 20.  0. 24.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#f.random.set_seed(0)\n",
    "drop = Dropout(0.5, input_shape=(4,))\n",
    "data = tf.reshape(tf.range(1.0,13.0), (3, 4))\n",
    "\n",
    "print(\"Before:\", data, sep=\"\\n\")\n",
    "output = drop(data, training=True)\n",
    "print(\"After:\", K.eval(output), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lambda layers\n",
    "\n",
    "* Work like regular lambda functions\n",
    "* Inputs and outputs are tensors, functions inside must be keras/tensorflow functions\n",
    "* Function has to be differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8], shape=(9,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  7,  9, 11, 13, 15, 17], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def sum_two_tensors(inputs):\n",
    "\n",
    "    x, y = inputs\n",
    "    sum_of_tensors = x + y\n",
    "\n",
    "    return sum_of_tensors\n",
    "\n",
    "input_tensor_1 = tf.range(0, 9)\n",
    "input_tensor_2 = tf.range(1, 10)\n",
    "print(input_tensor_1)\n",
    "print(input_tensor_2)\n",
    "#lambda_out = Lambda(sum_two_tensors)([input_tensor_1, input_tensor_2])\n",
    "lambda_layer = Lambda(sum_two_tensors)\n",
    "lambda_out = lambda_layer([input_tensor_1, input_tensor_2])\n",
    "K.eval(lambda_out)\n",
    "\n",
    "#model.add(Lambda(sum_two_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras activations (https://keras.io/api/layers/activations/)\n",
    "\n",
    "Activation functions for regression or inner layers:\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* LeakyReLU\n",
    "* Linear (None)\n",
    "\n",
    "THE activation function for classification (output layer only):\n",
    "* Softmax (ouputs probabilities for each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax\n",
    "\n",
    "It's an activation function applied to a output vector z with K elements (one per class) and outputs a probability distribution over the classes:\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"figures/simplenet.png\" width=200></td>\n",
    "<td><img src=\"figures/softmax.svg\"></td>\n",
    "</tr></table>\n",
    "\n",
    "What makes softmax your favorite activation:\n",
    "\n",
    "* K outputs sums to 1\n",
    "* K probabilities proportional to the exponentials of the input numbers\n",
    "* No negative outputs\n",
    "* Monotonically increasing output with increasing input\n",
    "\n",
    "Softmax is usually only used to activate the last layer of a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* Historically, sigmoid and tanh were the most used activation functions\n",
    "* Easy derivative\n",
    "* Bound outputs (ex: from 0 to 1)\n",
    "* They look like this:\n",
    "\n",
    "<img src=\"figures/logistic_curve.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* Problems arise when we are at large $|x|$\n",
    "* The derivative in that area becomes small (saturation)\n",
    "* Remember what the chain rule said?\n",
    "\n",
    "<img src=\"figures/logistic_curve.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* When we have $n$ layers, we go through $n$ activation functions\n",
    "* At layer $n$ the derivative is proportional to:\n",
    "$$\\begin{eqnarray} \n",
    "\\frac{\\partial L(w,b|x)}{\\partial w_{ln}} & \\propto &  \\frac{\\partial a_{ln}}{\\partial z_{ln}}\n",
    "\\end{eqnarray}$$\n",
    "* At layer 1 the derivative is proportional to:\n",
    "$$\\begin{eqnarray} \n",
    "\\frac{\\partial L(w,b|x)}{\\partial w_{l1}} & \\propto &  \\frac{\\partial a_{ln}}{\\partial z_{ln}} \\times \\frac{\\partial a_{n-1}}{\\partial z_{ln-1}} \\times \\frac{\\partial a_{ln-2}}{\\partial z_{ln-2}} \\ldots \\times \\frac{\\partial a_{l1}}{\\partial z_{l1}}\n",
    "\\end{eqnarray}$$\n",
    "* It is the product of many numbers $< 1$\n",
    "* Gradient becomes smaller and smaller for the initial layers\n",
    "* Gradient vanishing problem\n",
    "\n",
    "<!--- <img src=\"figures/large_net.png\" width=400> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU is the first activation to address the issue\n",
    "\n",
    "<center><img src=\"figures/relu.png\" width=400></center>\n",
    "\n",
    "Used in \"internal\" layers, usually not at last layer\n",
    "\n",
    "Pros:\n",
    "* Easy derivative (1 for x > 0, 0 elsewhere)\n",
    "* Derivative doesn't saturate for x > 0: alleviates gradient vanishing\n",
    "* Non-linear\n",
    "\n",
    "Cons:\n",
    "* Non-derivable at 0\n",
    "* Dead neurons if x << 0 for all data instances\n",
    "* Potential gradient explosion\n",
    "* Let's try this on Tensorflow playground: http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other ReLU-like activations\n",
    "\n",
    "LeakyReLU/PReLU\n",
    "* y = $\\alpha$x at x < 0\n",
    "* In PReLU $\\alpha$ is learned\n",
    "\n",
    "<center><img src=\"figures/leakyrelu.png?0\" width=400></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## <font color=\"white\">Other</font>\n",
    "\n",
    "ELU\n",
    "* Derivable at 0\n",
    "* Non-zero at x < 0\n",
    "\n",
    "<center><img src=\"figures/elu.png?0\" width=400></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"860\"\n",
       "            height=\"470\"\n",
       "            src=\"https://polarisation.github.io/tfjs-activation-functions/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fad4e653ed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame \n",
    "IFrame('https://polarisation.github.io/tfjs-activation-functions/', width=860, height=470) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting activations in Keras\n",
    "\n",
    "We can add activations as string parameters, or as functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.add(Dense(units=2, activation='relu'))\n",
    "model.add(Dense(units=2, activation=tf.keras.activations.relu))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But also as separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation(tf.keras.activations.relu))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Passing classes as parameters\n",
    "\n",
    "* Some parameters can be set by passing a string (optimizer='rmsprop')\n",
    "* we need to explicitly import the object if we want better control (optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'RMSprop',\n",
       " 'learning_rate': 0.001,\n",
       " 'decay': 0.0,\n",
       " 'rho': 0.9,\n",
       " 'momentum': 0.0,\n",
       " 'epsilon': 1e-07,\n",
       " 'centered': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(),                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Passing classes as parameters\n",
    "\n",
    "* Some parameters can be set by passing a string (optimizer='rmsprop')\n",
    "* we need to explicitly import the object if we want better control (optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'RMSprop',\n",
       " 'learning_rate': 1.0,\n",
       " 'decay': 0.0,\n",
       " 'rho': 0.9,\n",
       " 'momentum': 0.0,\n",
       " 'epsilon': 1e-07,\n",
       " 'centered': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(learning_rate=1.0),   #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There are multiple ways to pass data to fit()\n",
    "\n",
    "* You can load all of the data in memory, assign it to:\n",
    "    * numpy array or list of arrays (if you have multiple inputs/outputs)\n",
    "    * TensorFlow tensors\n",
    "    * A dictionary to map input names to arrays/tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "data = np.genfromtxt('path/to/dataset.csv',delimiter=',')\n",
    "\n",
    "X_train = data[:,0:10]\n",
    "y_train = data[:,10]\n",
    "\n",
    "model.fit(X_train, y_train,...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There are multiple ways to pass data to fit()\n",
    "\n",
    "\n",
    "* Or you can pass it an object/function that generates data for you:\n",
    "    * A generator() function\n",
    "    * A keras.utils.Sequence object\n",
    "    * A tensorflow.data.Dataset object\n",
    "\n",
    "Here a quick example on how a generator that loads loads data from a list of files (images, pickle objects, csv files...) on the filesystem:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def generator(input_list):\n",
    "    input_list_file = open(input_list, 'r')\n",
    "    while 1:\n",
    "        for next_file in input_list_file:\n",
    "        \n",
    "            data = open(next_file, 'r').readlines()\n",
    "            X = data[:,0:10]\n",
    "            y = data[:,10]\n",
    "        \n",
    "            yield X,y\n",
    "        input_list_file.seek(0)\n",
    "        \n",
    "model.fit(generator(train_data_list),...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Even more Keras layers\n",
    "\n",
    "* Dense is the classic FFNN where all nodes between layers are connected\n",
    "* Most of the other layers seen today are not trainable\n",
    "* What other layers are trainable then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional layers\n",
    "\n",
    "* Used where the _spatial_ relationship between inputs is significant\n",
    "* Classic example: imaging\n",
    "* Different types: 1D, 2D, 3D\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "model.add(Conv2D(filters, kernel_size, strides=(1, 1), padding=\"valid\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional layers\n",
    "\n",
    "<img src=\"figures/Typical_cnn.png\"></img>\n",
    "\n",
    "[(source)](https://en.wikipedia.org/wiki/Convolutional_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers\n",
    "\n",
    "* Used when the _temporal_ relationship between inputs is significant\n",
    "* Examples: audio, text\n",
    "* Different types: LSTM, GRU...\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LSTM\n",
    "model.add(LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers\n",
    "\n",
    "<img src=figures/rnn.png></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embedding layers\n",
    "\n",
    "* Used to transform a discrete input into a vector\n",
    "* Example: text input is made of words, how do we translate that into NN inputs?\n",
    "* \"cat\" -> `[0.1, 0.003, 1.2 ..., 0]`\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Embed\n",
    "model.add(Embedding(input_dim, output_dim))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embedding layers\n",
    "\n",
    "* Example: map amino acid names to 2D space\n",
    "* Which amino acids are most similar to tryptophan (W)?\n",
    "\n",
    "<img src=figures/aa_embed.png></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The functional API in Keras\n",
    "\n",
    "* https://keras.io/guides/functional_api\n",
    "* Sequential() is quite simple, but limited\n",
    "* What if we want to have multiple input/output layers?\n",
    "* What if we want a model that is not just a linear sequence of layers?\n",
    "\n",
    "<img src=\"figures/functional_api_40_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2/3 (reprise)\n",
    "\n",
    "* Remember the XOR classifier? Or the Boston housing dataset?\n",
    "* Can you apply some of the things we have learned today on the models from yesterday?\n",
    "* Do they help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 4 (optional)\n",
    "\n",
    "Classifying IMDB reviews into positive or negative.\n",
    "\n",
    "Check the exercises notebook!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
